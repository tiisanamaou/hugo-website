[{"content":"環境 Kubernetes 1.30.2 → 1.30.3 Control-Plane 3台 Woker-Node 2台 現状の構成 v1.30.2を使用している\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-control-plane-01 Ready control-plane 42d v1.30.2 192.168.10.41 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-36-generic containerd://1.7.18 k8s-control-plane-02 Ready control-plane 42d v1.30.2 192.168.10.44 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-36-generic containerd://1.7.18 k8s-control-plane-03 Ready control-plane 42d v1.30.2 192.168.10.46 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-39-generic containerd://1.7.18 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 192.168.10.42 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-39-generic containerd://1.7.18 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 192.168.10.43 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-39-generic containerd://1.7.18 mao@k8s-control-plane-01:~$ 手順 公式サイトの手順でアップグレードしていく\nhttps://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ 以下の順番でアップグレードしていく\nControl-Planeをアップグレード Control-Plane-01をアップグレード Control-Plane-02をアップグレード Control-Plane-03をアップグレード Woker-Nodeをアップグレード Woker-Node-01をアップグレード Woker-Node-02をアップグレード Control-Planeのアップグレード node上あるpodを別にnodeへ移動していく node名を確認する\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.2 k8s-control-plane-02 Ready control-plane 42d v1.30.2 k8s-control-plane-03 Ready control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-01:~$ 最初に\u0026quot;k8s-control-plane-01\u0026quot;をアップグレードする\n1 2 kubectl drain --ignore-daemonsets \u0026lt;node name\u0026gt; kubectl drain --ignore-daemonsets k8s-control-plane-01 1 2 3 4 5 6 7 8 9 10 11 mao@k8s-control-plane-01:~$ kubectl drain --ignore-daemonsets k8s-control-plane-01 node/k8s-control-plane-01 cordoned Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-dc87d, calico-system/csi-node-driver-8b6ts, kube-system/kube-proxy-9ng8c, metallb-system/speaker-zj5x9 evicting pod kube-system/coredns-7db6d8ff4d-w9f4s evicting pod calico-apiserver/calico-apiserver-5f78767767-hl7s7 evicting pod kube-system/coredns-7db6d8ff4d-vdlkw pod/calico-apiserver-5f78767767-hl7s7 evicted pod/coredns-7db6d8ff4d-vdlkw evicted pod/coredns-7db6d8ff4d-w9f4s evicted node/k8s-control-plane-01 drained mao@k8s-control-plane-01:~$ \u0026ldquo;k8s-control-plane-01\u0026quot;の\u0026quot;STATUS\u0026quot;が\u0026quot;SchedulingDisabled\u0026quot;になっているか確認する\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready,SchedulingDisabled control-plane 42d v1.30.2 k8s-control-plane-02 Ready control-plane 42d v1.30.2 k8s-control-plane-03 Ready control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-01:~$ kubeadm kubelet kubectl をアップデートする アップデートする対象のバージョンがあるか確認する\n1 2 sudo apt update sudo apt-cache madison kubeadm \u0026ldquo;1.30.3\u0026quot;があることを確認\n1 2 3 4 5 6 mao@k8s-control-plane-01:~$ sudo apt-cache madison kubeadm kubeadm | 1.30.3-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb Packages kubeadm | 1.30.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb Packages kubeadm | 1.30.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb Packages kubeadm | 1.30.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.30/deb Packages mao@k8s-control-plane-01:~$ \u0026ldquo;kubeadm\u0026quot;を\u0026quot;1.30.3\u0026quot;にアップデートする\n1 2 3 sudo apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubeadm=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubeadm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 mao@k8s-control-plane-01:~$ sudo apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubeadm=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubeadm Canceled hold on kubeadm. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:3 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubeadm\u0026#39; The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 32 not upgraded. Need to get 10.4 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubeadm 1.30.3-1.1 [10.4 MB] Fetched 10.4 MB in 0s (31.6 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110850 files and directories currently installed.) Preparing to unpack .../kubeadm_1.30.3-1.1_amd64.deb ... Unpacking kubeadm (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubeadm (1.30.3-1.1) ... Scanning processes... Scanning candidates... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-36-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. Restarting services... Service restarts being deferred: systemctl restart systemd-logind.service systemctl restart unattended-upgrades.service No containers need to be restarted. User sessions running outdated binaries: mao @ session #1: sshd[4320] mao @ user manager service: systemd[4324] No VM guests are running outdated hypervisor (qemu) binaries on this host. kubeadm set on hold. mao@k8s-control-plane-01:~$ \u0026ldquo;kubelet\u0026quot;と\u0026quot;kubectl\u0026quot;もアップデートする\n1 2 3 sudo apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=\u0026#39;1.30.3-*\u0026#39; kubectl=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubelet kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 mao@k8s-control-plane-01:~$ sudo apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=\u0026#39;1.30.3-*\u0026#39; kubectl=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubelet kubectl kubelet was already not on hold. kubectl was already not on hold. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:3 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubelet\u0026#39; Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubectl\u0026#39; The following packages will be upgraded: kubectl kubelet 2 upgraded, 0 newly installed, 0 to remove and 30 not upgraded. Need to get 28.9 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubectl 1.30.3-1.1 [10.8 MB] Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubelet 1.30.3-1.1 [18.1 MB] Fetched 28.9 MB in 1s (57.2 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110850 files and directories currently installed.) Preparing to unpack .../kubectl_1.30.3-1.1_amd64.deb ... Unpacking kubectl (1.30.3-1.1) over (1.30.2-1.1) ... Preparing to unpack .../kubelet_1.30.3-1.1_amd64.deb ... Unpacking kubelet (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubectl (1.30.3-1.1) ... Setting up kubelet (1.30.3-1.1) ... Scanning processes... Scanning candidates... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-36-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. Restarting services... systemctl restart kubelet.service Service restarts being deferred: systemctl restart systemd-logind.service systemctl restart unattended-upgrades.service No containers need to be restarted. User sessions running outdated binaries: mao @ session #1: sshd[4320] mao @ user manager service: systemd[4324] No VM guests are running outdated hypervisor (qemu) binaries on this host. kubelet set on hold. kubectl set on hold. mao@k8s-control-plane-01:~$ \u0026ldquo;kubeadm\u0026quot;のバージョンを確認する\n1 kubeadm version 1 2 3 mao@k8s-control-plane-01:~$ kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;30\u0026#34;, GitVersion:\u0026#34;v1.30.3\u0026#34;, GitCommit:\u0026#34;6fc0a69044f1ac4c13841ec4391224a2df241460\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2024-07-16T23:53:15Z\u0026#34;, GoVersion:\u0026#34;go1.22.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} mao@k8s-control-plane-01:~$ \u0026ldquo;v1.30.3\u0026quot;となっていることを確認\nアップグレードプランを確認する\n1 sudo kubeadm upgrade plan 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 mao@k8s-control-plane-01:~$ sudo kubeadm upgrade plan [preflight] Running pre-flight checks. [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [upgrade] Running cluster health checks [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: 1.30.2 [upgrade/versions] kubeadm version: v1.30.3 [upgrade/versions] Target version: v1.30.3 [upgrade/versions] Latest version in the v1.30 series: v1.30.3 Components that must be upgraded manually after you have upgraded the control plane with \u0026#39;kubeadm upgrade apply\u0026#39;: COMPONENT NODE CURRENT TARGET kubelet k8s-control-plane-02 v1.30.2 v1.30.3 kubelet k8s-control-plane-03 v1.30.2 v1.30.3 kubelet k8s-worker-01 v1.30.2 v1.30.3 kubelet k8s-worker-02 v1.30.2 v1.30.3 kubelet k8s-control-plane-01 v1.30.3 v1.30.3 Upgrade to the latest version in the v1.30 series: COMPONENT NODE CURRENT TARGET kube-apiserver k8s-control-plane-01 v1.30.2 v1.30.3 kube-apiserver k8s-control-plane-02 v1.30.2 v1.30.3 kube-apiserver k8s-control-plane-03 v1.30.2 v1.30.3 kube-controller-manager k8s-control-plane-01 v1.30.2 v1.30.3 kube-controller-manager k8s-control-plane-02 v1.30.2 v1.30.3 kube-controller-manager k8s-control-plane-03 v1.30.2 v1.30.3 kube-scheduler k8s-control-plane-01 v1.30.2 v1.30.3 kube-scheduler k8s-control-plane-02 v1.30.2 v1.30.3 kube-scheduler k8s-control-plane-03 v1.30.2 v1.30.3 kube-proxy 1.30.2 v1.30.3 CoreDNS v1.11.1 v1.11.1 etcd k8s-control-plane-01 3.5.12-0 3.5.12-0 etcd k8s-control-plane-02 3.5.12-0 3.5.12-0 etcd k8s-control-plane-03 3.5.12-0 3.5.12-0 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.30.3 _____________________________________________________________________ The table below shows the current state of component configs as understood by this version of kubeadm. Configs that have a \u0026#34;yes\u0026#34; mark in the \u0026#34;MANUAL UPGRADE REQUIRED\u0026#34; column require manual config upgrade or resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually upgrade to is denoted in the \u0026#34;PREFERRED VERSION\u0026#34; column. API GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIRED kubeproxy.config.k8s.io v1alpha1 v1alpha1 no kubelet.config.k8s.io v1beta1 v1beta1 no _____________________________________________________________________ mao@k8s-control-plane-01:~$ 実際にアップグレードする\n1 sudo kubeadm upgrade apply v1.30.3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 mao@k8s-control-plane-01:~$ sudo kubeadm upgrade apply v1.30.3 [preflight] Running pre-flight checks. [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [upgrade] Running cluster health checks [upgrade/version] You have chosen to change the cluster version to \u0026#34;v1.30.3\u0026#34; [upgrade/versions] Cluster version: v1.30.2 [upgrade/versions] kubeadm version: v1.30.3 [upgrade] Are you sure you want to proceed? [y/N]: y [upgrade/prepull] Pulling images required for setting up a Kubernetes cluster [upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection [upgrade/prepull] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026#34;v1.30.3\u0026#34; (timeout: 5m0s)... [upgrade/etcd] Upgrading to TLS for etcd [upgrade/staticpods] Preparing for \u0026#34;etcd\u0026#34; upgrade [upgrade/staticpods] Current and new manifests of etcd are equal, skipping upgrade [upgrade/etcd] Waiting for etcd to become available [upgrade/staticpods] Writing new Static Pod manifests to \u0026#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests890193377\u0026#34; [upgrade/staticpods] Preparing for \u0026#34;kube-apiserver\u0026#34; upgrade [upgrade/staticpods] Renewing apiserver certificate [upgrade/staticpods] Renewing apiserver-kubelet-client certificate [upgrade/staticpods] Renewing front-proxy-client certificate [upgrade/staticpods] Renewing apiserver-etcd-client certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-05-10/kube-apiserver.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \u0026#34;kube-apiserver\u0026#34; upgraded successfully! [upgrade/staticpods] Preparing for \u0026#34;kube-controller-manager\u0026#34; upgrade [upgrade/staticpods] Renewing controller-manager.conf certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-05-10/kube-controller-manager.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \u0026#34;kube-controller-manager\u0026#34; upgraded successfully! [upgrade/staticpods] Preparing for \u0026#34;kube-scheduler\u0026#34; upgrade [upgrade/staticpods] Renewing scheduler.conf certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-05-10/kube-scheduler.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \u0026#34;kube-scheduler\u0026#34; upgraded successfully! [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config745490494/config.yaml [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [upgrade/addons] skip upgrade addons because control plane instances [k8s-control-plane-02 k8s-control-plane-03] have not been upgraded [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026#34;v1.30.3\u0026#34;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026#39;t already done so. mao@k8s-control-plane-01:~$ 無事アップグレードされました\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready,SchedulingDisabled control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.2 k8s-control-plane-03 Ready control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-01:~$ nodeからpodを退避させているのを解除する \u0026ldquo;kubectl drain\u0026quot;を解除する\n1 2 kubectl uncordon \u0026lt;node name\u0026gt; kubectl uncordon k8s-control-plane-01 1 2 3 mao@k8s-control-plane-01:~$ kubectl uncordon k8s-control-plane-01 node/k8s-control-plane-01 uncordoned mao@k8s-control-plane-01:~$ \u0026ldquo;STATUS\u0026quot;に\u0026quot;SchedulingDisabled\u0026quot;が表示されなくなったことを確認\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.2 k8s-control-plane-03 Ready control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-01:~$ 他のContol-Planeもアップグレードをする k8s-control-plane-02 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 mao@k8s-control-plane-02:~$ kubectl drain --ignore-daemonsets k8s-control-plane-0 2 node/k8s-control-plane-02 cordoned Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-26sbk, calico-system/csi-node-driver-cljz8, kube-system/kube-proxy-xkvj7, metallb-system/speaker-cjz7j evicting pod calico-system/calico-typha-5579b889c8-kbqk7 evicting pod calico-apiserver/calico-apiserver-5f78767767-89z5t pod/calico-apiserver-5f78767767-89z5t evicted pod/calico-typha-5579b889c8-kbqk7 evicted node/k8s-control-plane-02 drained mao@k8s-control-plane-02:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready,SchedulingDisabled control-plane 42d v1.30.2 k8s-control-plane-03 Ready control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-02:~$ sudo apt update [sudo] password for mao: Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:3 http://security.ubuntu.com/ubuntu noble-security InRelease Get:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB] Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Get:6 http://jp.archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [344 kB] Get:7 http://jp.archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [321 kB] Fetched 791 kB in 3s (298 kB/s) Reading package lists... Done Building dependency tree... Done Reading state information... Done 31 packages can be upgraded. Run \u0026#39;apt list --upgradable\u0026#39; to see them. mao@k8s-control-plane-02:~$ sudo apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubeadm=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubeadm Canceled hold on kubeadm. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:3 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubeadm\u0026#39; The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 30 not upgraded. Need to get 10.4 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubeadm 1.30.3-1.1 [10.4 MB] Fetched 10.4 MB in 0s (30.5 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110849 files and directories currently installed.) Preparing to unpack .../kubeadm_1.30.3-1.1_amd64.deb ... Unpacking kubeadm (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubeadm (1.30.3-1.1) ... Scanning processes... Scanning candidates... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-36-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. Restarting services... Service restarts being deferred: systemctl restart systemd-logind.service systemctl restart unattended-upgrades.service No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubeadm set on hold. mao@k8s-control-plane-02:~$ sudo apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=\u0026#39;1.30.3-*\u0026#39; kubectl=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubelet kubectl Canceled hold on kubelet. Canceled hold on kubectl. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:3 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubelet\u0026#39; Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubectl\u0026#39; The following packages will be upgraded: kubectl kubelet 2 upgraded, 0 newly installed, 0 to remove and 28 not upgraded. Need to get 28.9 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubectl 1.30.3-1.1 [10.8 MB] Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubelet 1.30.3-1.1 [18.1 MB] Fetched 28.9 MB in 1s (55.7 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110849 files and directories currently installed.) Preparing to unpack .../kubectl_1.30.3-1.1_amd64.deb ... Unpacking kubectl (1.30.3-1.1) over (1.30.2-1.1) ... Preparing to unpack .../kubelet_1.30.3-1.1_amd64.deb ... Unpacking kubelet (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubectl (1.30.3-1.1) ... Setting up kubelet (1.30.3-1.1) ... Scanning processes... Scanning candidates... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-36-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. Restarting services... systemctl restart kubelet.service Service restarts being deferred: systemctl restart systemd-logind.service systemctl restart unattended-upgrades.service No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubelet set on hold. kubectl set on hold. mao@k8s-control-plane-02:~$ kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;30\u0026#34;, GitVersion:\u0026#34;v1.30.3\u0026#34;, GitCommit:\u0026#34;6fc0a69044f1ac4c13841ec4391224a2df241460\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2024-07-16T23:53:15Z\u0026#34;, GoVersion:\u0026#34;go1.22.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} mao@k8s-control-plane-02:~$ sudo kubeadm upgrade plan [preflight] Running pre-flight checks. [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [upgrade] Running cluster health checks [upgrade] Fetching available versions to upgrade to W0811 23:15:18.916628 16854 compute.go:93] Different API server versions in the cluster were discovered: v1.30.3 on nodes [k8s-control-plane-01], v1.30.2 on nodes [k8s-control-plane-02 k8s-control-plane-03]. Please upgrade your control plane nodes to the same version of Kubernetes [upgrade/versions] Cluster version: 1.30.3 [upgrade/versions] kubeadm version: v1.30.3 [upgrade/versions] Target version: v1.30.3 [upgrade/versions] Latest version in the v1.30 series: v1.30.3 mao@k8s-control-plane-02:~$ sudo kubeadm upgrade apply v1.30.3 [preflight] Running pre-flight checks. [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [upgrade] Running cluster health checks [upgrade/version] You have chosen to change the cluster version to \u0026#34;v1.30.3\u0026#34; [upgrade/versions] Cluster version: v1.30.2 [upgrade/versions] kubeadm version: v1.30.3 [upgrade] Are you sure you want to proceed? [y/N]: y [upgrade/prepull] Pulling images required for setting up a Kubernetes cluster [upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection [upgrade/prepull] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026#34;v1.30.3\u0026#34; (timeout: 5m0s)... [upgrade/etcd] Upgrading to TLS for etcd [upgrade/staticpods] Preparing for \u0026#34;etcd\u0026#34; upgrade [upgrade/staticpods] Renewing etcd-server certificate [upgrade/staticpods] Renewing etcd-peer certificate [upgrade/staticpods] Renewing etcd-healthcheck-client certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/etcd.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-15-44/etcd.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=etcd [upgrade/staticpods] Component \u0026#34;etcd\u0026#34; upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [upgrade/staticpods] Writing new Static Pod manifests to \u0026#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests3449985093\u0026#34; [upgrade/staticpods] Preparing for \u0026#34;kube-apiserver\u0026#34; upgrade [upgrade/staticpods] Renewing apiserver certificate [upgrade/staticpods] Renewing apiserver-kubelet-client certificate [upgrade/staticpods] Renewing front-proxy-client certificate [upgrade/staticpods] Renewing apiserver-etcd-client certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-15-44/kube-apiserver.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \u0026#34;kube-apiserver\u0026#34; upgraded successfully! [upgrade/staticpods] Preparing for \u0026#34;kube-controller-manager\u0026#34; upgrade [upgrade/staticpods] Renewing controller-manager.conf certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-15-44/kube-controller-manager.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \u0026#34;kube-controller-manager\u0026#34; upgraded successfully! [upgrade/staticpods] Preparing for \u0026#34;kube-scheduler\u0026#34; upgrade [upgrade/staticpods] Renewing scheduler.conf certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-15-44/kube-scheduler.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \u0026#34;kube-scheduler\u0026#34; upgraded successfully! [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config534750710/config.yaml [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [upgrade/addons] skip upgrade addons because control plane instances [k8s-control-plane-03] have not been upgraded [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026#34;v1.30.3\u0026#34;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven\u0026#39;t already done so. mao@k8s-control-plane-02:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready,SchedulingDisabled control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-02:~$ kubectl uncordon k8s-control-plane-02 node/k8s-control-plane-02 uncordoned mao@k8s-control-plane-02:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-02:~$ k8s-control-plane-03 \u0026ldquo;sudo kubeadm upgrade plan\u0026quot;と\u0026quot;sudo kubeadm upgrade apply v1.30.3\u0026quot;の変わりに\u0026quot;sudo kubeadm upgrade node\u0026quot;を実行する 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 mao@k8s-control-plane-03:~$ kubectl drain --ignore-daemonsets k8s-control-plane-0 3 node/k8s-control-plane-03 cordoned Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-g2ks8, calico-system/csi-node-driver-9w86p, kube-system/kube-proxy-4l8w6, metallb-system/speaker-gg452 evicting pod tigera-operator/tigera-operator-76ff79f7fd-tc9t7 evicting pod calico-system/calico-kube-controllers-5f5665469b-nh6qm pod/tigera-operator-76ff79f7fd-tc9t7 evicted pod/calico-kube-controllers-5f5665469b-nh6qm evicted node/k8s-control-plane-03 drained mao@k8s-control-plane-03:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready,SchedulingDisabled control-plane 42d v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-03:~$ sudo apt update [sudo] password for mao: Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Get:3 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB] Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Get:6 http://jp.archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [344 kB] Get:7 http://jp.archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [321 kB] Fetched 791 kB in 2s (466 kB/s) Reading package lists... Done Building dependency tree... Done Reading state information... Done 33 packages can be upgraded. Run \u0026#39;apt list --upgradable\u0026#39; to see them. mao@k8s-control-plane-03:~$ sudo apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubeadm=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubeadm Canceled hold on kubeadm. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:3 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubeadm\u0026#39; The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 32 not upgraded. Need to get 10.4 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubeadm 1.30.3-1.1 [10.4 MB] Fetched 10.4 MB in 0s (31.7 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110850 files and directories currently installed.) Preparing to unpack .../kubeadm_1.30.3-1.1_amd64.deb ... Unpacking kubeadm (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubeadm (1.30.3-1.1) ... Scanning processes... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-39-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. No services need to be restarted. No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubeadm set on hold. mao@k8s-control-plane-03:~$ sudo apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=\u0026#39;1.30.3-*\u0026#39; kubectl=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubelet kubectl Canceled hold on kubelet. Canceled hold on kubectl. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:3 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubelet\u0026#39; Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubectl\u0026#39; The following packages will be upgraded: kubectl kubelet 2 upgraded, 0 newly installed, 0 to remove and 30 not upgraded. Need to get 28.9 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubectl 1.30.3-1.1 [10.8 MB] Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubelet 1.30.3-1.1 [18.1 MB] Fetched 28.9 MB in 0s (58.1 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110850 files and directories currently installed.) Preparing to unpack .../kubectl_1.30.3-1.1_amd64.deb ... Unpacking kubectl (1.30.3-1.1) over (1.30.2-1.1) ... Preparing to unpack .../kubelet_1.30.3-1.1_amd64.deb ... Unpacking kubelet (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubectl (1.30.3-1.1) ... Setting up kubelet (1.30.3-1.1) ... Scanning processes... Scanning candidates... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-39-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. Restarting services... systemctl restart kubelet.service No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubelet set on hold. kubectl set on hold. mao@k8s-control-plane-03:~$ kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;30\u0026#34;, GitVersion:\u0026#34;v1.30.3\u0026#34;, GitCommit:\u0026#34;6fc0a69044f1ac4c13841ec4391224a2df241460\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2024-07-16T23:53:15Z\u0026#34;, GoVersion:\u0026#34;go1.22.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} mao@k8s-control-plane-03:~$ sudo kubeadm upgrade node [upgrade] Reading configuration from the cluster... [upgrade] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [upgrade] Upgrading your Static Pod-hosted control plane instance to version \u0026#34;v1.30.3\u0026#34;... [upgrade/etcd] Upgrading to TLS for etcd [upgrade/staticpods] Preparing for \u0026#34;etcd\u0026#34; upgrade [upgrade/staticpods] Renewing etcd-server certificate [upgrade/staticpods] Renewing etcd-peer certificate [upgrade/staticpods] Renewing etcd-healthcheck-client certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/etcd.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-21-58/etcd.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=etcd [upgrade/staticpods] Component \u0026#34;etcd\u0026#34; upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [upgrade/staticpods] Writing new Static Pod manifests to \u0026#34;/etc/kubernetes/tmp/kubeadm-upgraded-manifests3409919858\u0026#34; [upgrade/staticpods] Preparing for \u0026#34;kube-apiserver\u0026#34; upgrade [upgrade/staticpods] Renewing apiserver certificate [upgrade/staticpods] Renewing apiserver-kubelet-client certificate [upgrade/staticpods] Renewing front-proxy-client certificate [upgrade/staticpods] Renewing apiserver-etcd-client certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-21-58/kube-apiserver.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \u0026#34;kube-apiserver\u0026#34; upgraded successfully! [upgrade/staticpods] Preparing for \u0026#34;kube-controller-manager\u0026#34; upgrade [upgrade/staticpods] Renewing controller-manager.conf certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-21-58/kube-controller-manager.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \u0026#34;kube-controller-manager\u0026#34; upgraded successfully! [upgrade/staticpods] Preparing for \u0026#34;kube-scheduler\u0026#34; upgrade [upgrade/staticpods] Renewing scheduler.conf certificate [upgrade/staticpods] Moved new manifest to \u0026#34;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026#34; and backed up old manifest to \u0026#34;/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-08-11-23-21-58/kube-scheduler.yaml\u0026#34; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This can take up to 5m0s [apiclient] Found 3 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \u0026#34;kube-scheduler\u0026#34; upgraded successfully! [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [upgrade] The control plane instance for this node was successfully updated! [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config1587591676/config.yaml [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [upgrade] The configuration for this node was successfully updated! [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. mao@k8s-control-plane-03:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready,SchedulingDisabled control-plane 42d v1.30.3 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-03:~$ kubectl uncordon k8s-control-plane-03 node/k8s-control-plane-03 uncordoned mao@k8s-control-plane-03:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.3 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-03:~$ これでControl-Planeのアップグレードは完了 Worker-Nodeをアップグレードする kubeadmをアップデート \u0026ldquo;kubeadm\u0026quot;をアップデートする\n1 2 3 sudo apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubeadm=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubeadm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 mao@k8s-worker-01:~$ sudo apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubeadm=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubeadm [sudo] password for mao: Canceled hold on kubeadm. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:3 http://jp.archive.ubuntu.com/ubuntu noble InRelease Get:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB] Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Get:6 http://jp.archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [344 kB] Get:7 http://jp.archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [321 kB] Fetched 791 kB in 2s (365 kB/s) Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubeadm\u0026#39; The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 32 not upgraded. Need to get 10.4 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubeadm 1.30.3-1.1 [10.4 MB] Fetched 10.4 MB in 0s (30.4 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110854 files and directories currently installed.) Preparing to unpack .../kubeadm_1.30.3-1.1_amd64.deb ... Unpacking kubeadm (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubeadm (1.30.3-1.1) ... Scanning processes... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-39-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. No services need to be restarted. No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubeadm set on hold. mao@k8s-worker-01:~$ \u0026ldquo;kubeadm upgrade\u0026quot;を実行する\n1 sudo kubeadm upgrade node 1 2 3 4 5 6 7 8 9 10 11 mao@k8s-worker-01:~$ sudo kubeadm upgrade node [upgrade] Reading configuration from the cluster... [upgrade] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [preflight] Running pre-flight checks [preflight] Skipping prepull. Not a control plane node. [upgrade] Skipping phase. Not a control plane node. [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config1611631804/config.yaml [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [upgrade] The configuration for this node was successfully updated! [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. mao@k8s-worker-01:~$ Woker-Nodeをドレインする Control-Planeで下記のコマンドを実行する\n1 2 kubectl drain \u0026lt;node-to-drain\u0026gt; --ignore-daemonsets kubectl drain k8s-worker-01 --ignore-daemonsets 1 2 3 4 5 6 7 8 9 10 11 12 13 mao@k8s-control-plane-01:~$ kubectl drain k8s-worker-01 --ignore-daemonsets node/k8s-worker-01 cordoned Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-l2ll2, calico-system/csi-node-driver-2pzn6, kube-system/kube-proxy-rmxrw, metallb-system/speaker-2hccg evicting pod metallb-system/controller-86f5578878-xxpt6 evicting pod calico-system/calico-typha-5579b889c8-dc9zt evicting pod calico-apiserver/calico-apiserver-5f78767767-cnx87 evicting pod kube-system/coredns-7db6d8ff4d-m29qd pod/controller-86f5578878-xxpt6 evicted pod/calico-apiserver-5f78767767-cnx87 evicted pod/calico-typha-5579b889c8-dc9zt evicted pod/coredns-7db6d8ff4d-m29qd evicted node/k8s-worker-01 drained mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.3 k8s-worker-01 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 42d v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-01:~$ kubeletとkubectlをアップデートする Worker-Nodeで実行する\n1 2 3 sudo apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=\u0026#39;1.30.3-*\u0026#39; kubectl=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubelet kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 mao@k8s-worker-01:~$ sudo apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=\u0026#39;1.30.3-*\u0026#39; kubectl=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubelet kubectl Canceled hold on kubelet. Canceled hold on kubectl. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:3 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:4 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubelet\u0026#39; Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubectl\u0026#39; The following packages will be upgraded: kubectl kubelet 2 upgraded, 0 newly installed, 0 to remove and 30 not upgraded. Need to get 28.9 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubectl 1.30.3-1.1 [10.8 MB] Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubelet 1.30.3-1.1 [18.1 MB] Fetched 28.9 MB in 1s (53.4 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110854 files and directories currently installed.) Preparing to unpack .../kubectl_1.30.3-1.1_amd64.deb ... Unpacking kubectl (1.30.3-1.1) over (1.30.2-1.1) ... Preparing to unpack .../kubelet_1.30.3-1.1_amd64.deb ... Unpacking kubelet (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubectl (1.30.3-1.1) ... Setting up kubelet (1.30.3-1.1) ... Scanning processes... Scanning candidates... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-39-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. Restarting services... systemctl restart kubelet.service No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubelet set on hold. kubectl set on hold. mao@k8s-worker-01:~$ リロードする\n1 2 sudo systemctl daemon-reload sudo systemctl restart kubelet nodeからpodを退避させているのを解除する \u0026ldquo;kubectl drain\u0026quot;を解除する\nControl-Planeで実行する\n1 2 kubectl uncordon \u0026lt;node-to-uncordon\u0026gt; kubectl uncordon k8s-worker-01 1 2 3 4 5 6 7 8 9 10 mao@k8s-control-plane-01:~$ kubectl uncordon k8s-worker-01 node/k8s-worker-01 uncordoned mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.3 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.3 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-01:~$ Woker-Nodeのバージョン確認 \u0026ldquo;kubectl get nodes\u0026quot;をControl-Planeで実行して\u0026quot;k8s-worker-01\u0026quot;のバージョンが\u0026quot;v1.30.3\u0026quot;にアップグレードされていることを確認\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.3 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.3 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.2 mao@k8s-control-plane-01:~$ これでWoker-Nodeのアップグレードは完了 他のWoker-Nodeもアップグレードする Woker-Node-02 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mao@k8s-control-plane-01:~$ kubectl drain k8s-worker-02 --ignore-daemonsets node/k8s-worker-02 cordoned Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-28xrv, calico-system/csi-node-driver-jddlc, kube-system/kube-proxy-8hsdx, metallb-system/speaker-dpfpl evicting pod tigera-operator/tigera-operator-76ff79f7fd-hm5bk evicting pod calico-system/calico-typha-5579b889c8-pdlqz evicting pod metallb-system/controller-86f5578878-chzwd evicting pod kube-system/coredns-7db6d8ff4d-5kfxd evicting pod calico-apiserver/calico-apiserver-5f78767767-gjh5t evicting pod calico-system/calico-kube-controllers-5f5665469b-fbfh2 pod/controller-86f5578878-chzwd evicted pod/tigera-operator-76ff79f7fd-hm5bk evicted pod/calico-kube-controllers-5f5665469b-fbfh2 evicted pod/calico-apiserver-5f78767767-gjh5t evicted pod/coredns-7db6d8ff4d-5kfxd evicted pod/calico-typha-5579b889c8-pdlqz evicted node/k8s-worker-02 drained mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 mao@k8s-worker-02:~$ sudo apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubeadm=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubeadm Canceled hold on kubeadm. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Hit:3 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubeadm\u0026#39; The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 30 not upgraded. Need to get 10.4 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubeadm 1.30.3-1.1 [10.4 MB] Fetched 10.4 MB in 0s (31.1 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110852 files and directories currently installed.) Preparing to unpack .../kubeadm_1.30.3-1.1_amd64.deb ... Unpacking kubeadm (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubeadm (1.30.3-1.1) ... Scanning processes... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-39-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. No services need to be restarted. No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubeadm set on hold. mao@k8s-worker-02:~$ sudo kubeadm upgrade node [upgrade] Reading configuration from the cluster... [upgrade] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [preflight] Running pre-flight checks [preflight] Skipping prepull. Not a control plane node. [upgrade] Skipping phase. Not a control plane node. [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config2769406187/config.yaml [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [upgrade] The configuration for this node was successfully updated! [upgrade] Now you should go ahead and upgrade the kubelet package using your package manager. mao@k8s-worker-02:~$ sudo apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y kubelet=\u0026#39;1.30.3-*\u0026#39; kubectl=\u0026#39;1.30.3-*\u0026#39; \u0026amp;\u0026amp; \\ sudo apt-mark hold kubelet kubectl [sudo] password for mao: Canceled hold on kubelet. Canceled hold on kubectl. Hit:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb InRelease Hit:2 http://jp.archive.ubuntu.com/ubuntu noble InRelease Get:3 http://jp.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB] Hit:4 http://security.ubuntu.com/ubuntu noble-security InRelease Hit:5 http://jp.archive.ubuntu.com/ubuntu noble-backports InRelease Get:6 http://jp.archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [344 kB] Get:7 http://jp.archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [321 kB] Fetched 791 kB in 2s (469 kB/s) Reading package lists... Done Reading package lists... Done Building dependency tree... Done Reading state information... Done Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubelet\u0026#39; Selected version \u0026#39;1.30.3-1.1\u0026#39; (isv:kubernetes:core:stable:v1.30:pkgs.k8s.io [amd64]) for \u0026#39;kubectl\u0026#39; The following packages will be upgraded: kubectl kubelet 2 upgraded, 0 newly installed, 0 to remove and 31 not upgraded. Need to get 28.9 MB of archives. After this operation, 0 B of additional disk space will be used. Get:1 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubectl 1.30.3-1.1 [10.8 MB] Get:2 https://prod-cdn.packages.k8s.io/repositories/isv:/kubernetes:/core:/stable:/v1.30/deb kubelet 1.30.3-1.1 [18.1 MB] Fetched 28.9 MB in 0s (57.8 MB/s) debconf: delaying package configuration, since apt-utils is not installed (Reading database ... 110852 files and directories currently installed.) Preparing to unpack .../kubectl_1.30.3-1.1_amd64.deb ... Unpacking kubectl (1.30.3-1.1) over (1.30.2-1.1) ... Preparing to unpack .../kubelet_1.30.3-1.1_amd64.deb ... Unpacking kubelet (1.30.3-1.1) over (1.30.2-1.1) ... Setting up kubectl (1.30.3-1.1) ... Setting up kubelet (1.30.3-1.1) ... Scanning processes... Scanning candidates... Scanning linux images... Pending kernel upgrade! Running kernel version: 6.8.0-39-generic Diagnostics: The currently running kernel version is not the expected kernel version 6.8.0-40-generic. Restarting the system to load the new kernel will not be handled automatically, so you should consider rebooting. Restarting services... systemctl restart kubelet.service No containers need to be restarted. No user sessions are running outdated binaries. No VM guests are running outdated hypervisor (qemu) binaries on this host. kubelet set on hold. kubectl set on hold. mao@k8s-worker-02:~$ sudo systemctl daemon-reload mao@k8s-worker-02:~$ sudo systemctl restart kubelet mao@k8s-worker-02:~$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.3 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.3 k8s-worker-02 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 42d v1.30.3 mao@k8s-control-plane-01:~$ kubectl uncordon k8s-worker-02 node/k8s-worker-02 uncordoned mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.3 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.3 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.3 mao@k8s-control-plane-01:~$ これでWoker-Nodeのアップグレードは完了 クラスタのアップグレードが完了しました \u0026ldquo;v1.30.2\u0026quot;が\u0026quot;v1.30.3\u0026quot;へ\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 42d v1.30.3 k8s-control-plane-02 Ready control-plane 42d v1.30.3 k8s-control-plane-03 Ready control-plane 42d v1.30.3 k8s-worker-01 Ready \u0026lt;none\u0026gt; 42d v1.30.3 k8s-worker-02 Ready \u0026lt;none\u0026gt; 42d v1.30.3 mao@k8s-control-plane-01:~$ podも問題なく動作しているか確認\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 mao@k8s-control-plane-01:~$ kubectl get pod -A -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES calico-apiserver calico-apiserver-5f78767767-78qj9 1/1 Running 0 9m1s 10.128.36.245 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-apiserver calico-apiserver-5f78767767-9q5bf 1/1 Running 0 16m 10.128.204.140 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-kube-controllers-5f5665469b-qjbzw 1/1 Running 0 9m1s 10.128.36.244 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-node-26sbk 1/1 Running 4 (82m ago) 42d 192.168.10.44 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-node-28xrv 1/1 Running 4 (82m ago) 42d 192.168.10.43 k8s-worker-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-node-dc87d 1/1 Running 4 (83m ago) 42d 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-node-g2ks8 1/1 Running 4 (82m ago) 42d 192.168.10.46 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-node-l2ll2 1/1 Running 4 (82m ago) 42d 192.168.10.42 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-typha-5579b889c8-b4jzx 1/1 Running 0 12m 192.168.10.42 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-typha-5579b889c8-fxdx8 1/1 Running 0 6m27s 192.168.10.43 k8s-worker-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system calico-typha-5579b889c8-wrmhq 1/1 Running 0 25m 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system csi-node-driver-2pzn6 2/2 Running 8 (82m ago) 42d 10.128.36.239 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system csi-node-driver-8b6ts 2/2 Running 8 (83m ago) 42d 10.128.251.148 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system csi-node-driver-9w86p 2/2 Running 8 (82m ago) 42d 10.128.204.138 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system csi-node-driver-cljz8 2/2 Running 8 (36h ago) 42d 10.128.194.202 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; calico-system csi-node-driver-jddlc 2/2 Running 8 (82m ago) 42d 10.128.118.105 k8s-worker-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-7db6d8ff4d-cbws9 1/1 Running 0 9m1s 10.128.36.242 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-7db6d8ff4d-lx728 1/1 Running 0 16m 10.128.204.139 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-k8s-control-plane-01 1/1 Running 34 (83m ago) 42d 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-k8s-control-plane-02 1/1 Running 0 34m 192.168.10.44 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-k8s-control-plane-03 1/1 Running 0 27m 192.168.10.46 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-k8s-control-plane-01 1/1 Running 0 44m 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-k8s-control-plane-02 1/1 Running 0 33m 192.168.10.44 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-k8s-control-plane-03 1/1 Running 0 27m 192.168.10.46 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-k8s-control-plane-01 1/1 Running 0 44m 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-k8s-control-plane-02 1/1 Running 0 33m 192.168.10.44 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-k8s-control-plane-03 1/1 Running 0 26m 192.168.10.46 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-8hsdx 1/1 Running 0 26m 192.168.10.43 k8s-worker-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-ldpnm 1/1 Running 0 26m 192.168.10.44 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-qb2c4 1/1 Running 0 26m 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-rmxrw 1/1 Running 0 26m 192.168.10.42 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-v6k6r 1/1 Running 0 26m 192.168.10.46 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-k8s-control-plane-01 1/1 Running 0 43m 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-k8s-control-plane-02 1/1 Running 0 32m 192.168.10.44 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-k8s-control-plane-03 1/1 Running 0 26m 192.168.10.46 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-system controller-86f5578878-9gqs5 1/1 Running 0 9m1s 10.128.36.243 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-system speaker-2hccg 1/1 Running 8 (81m ago) 42d 192.168.10.42 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-system speaker-cjz7j 1/1 Running 8 (81m ago) 42d 192.168.10.44 k8s-control-plane-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-system speaker-dpfpl 1/1 Running 8 (81m ago) 42d 192.168.10.43 k8s-worker-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-system speaker-gg452 1/1 Running 8 (81m ago) 42d 192.168.10.46 k8s-control-plane-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; metallb-system speaker-zj5x9 1/1 Running 8 (81m ago) 42d 192.168.10.41 k8s-control-plane-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; tigera-operator tigera-operator-76ff79f7fd-rq76h 1/1 Running 0 9m1s 192.168.10.42 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; mao@k8s-control-plane-01:~$ 参考URL https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/ https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/ https://goodbyegangster.hatenablog.com/entry/2021/01/19/205313 ","date":"2024-08-13T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-upgrade-v1-30-3/","title":"Kubernetesのクラスタをv1.30.2からv1.30.3へアップグレードをする"},{"content":"環境 Kubernetes 1.30.2 Helm v3.15.3 kubernetes-dashboard-7.5.0 Helmの導入 参考URL\nhttps://helm.sh/ja/docs/intro/install/ https://github.com/helm/helm/releases https://qiita.com/loftkun/items/bcfe1f205cde4d74a384 今回はバイナリを使用する方法でインストールします\nバイナリをダウンロードします\n1 wget https://get.helm.sh/helm-v3.15.3-linux-amd64.tar.gz 1 2 3 4 5 6 7 8 9 10 11 12 13 mao@k8s-control-plane-01:~$ wget https://get.helm.sh/helm-v3.15.3-linux-amd64.tar.gz --2024-08-08 09:11:52-- https://get.helm.sh/helm-v3.15.3-linux-amd64.tar.gz Resolving get.helm.sh (get.helm.sh)... 152.199.39.108, 2606:2800:247:1cb7:261b:1f9c:2074:3c Connecting to get.helm.sh (get.helm.sh)|152.199.39.108|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 16626424 (16M) [application/x-tar] Saving to: ‘helm-v3.15.3-linux-amd64.tar.gz’ helm-v3.15.3-linux-am 100%[======================\u0026gt;] 15.86M 102MB/s in 0.2s 2024-08-08 09:11:52 (102 MB/s) - ‘helm-v3.15.3-linux-amd64.tar.gz’ saved [16626424/16626424] mao@k8s-control-plane-01:~$ 圧縮されているので展開します\n展開するとバイナリファイルが出てきます\n1 tar -zxvf helm-v3.15.3-linux-amd64.tar.gz 1 2 3 4 5 6 mao@k8s-control-plane-01:~$ tar -zxvf helm-v3.15.3-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/README.md linux-amd64/LICENSE mao@k8s-control-plane-01:~$ バイナリファイルを移動します\n1 sudo mv linux-amd64/helm /usr/local/bin/helm バイナリを移動したらバージョンを確認してみます\n1 helm version 1 2 3 mao@k8s-control-plane-01:~$ helm version version.BuildInfo{Version:\u0026#34;v3.15.3\u0026#34;, GitCommit:\u0026#34;3bb50bbbdd9c946ba9989fbe4fb4104766302a64\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.22.5\u0026#34;} mao@k8s-control-plane-01:~$ \u0026ldquo;v3.15.3\u0026quot;と表示されたのでこれでインストールは完了しました\nアップデートする際はこのバイナリを新しいものに置き換えます\nアンインストール方法 1 2 3 $ which helm /usr/local/bin/helm $ sudo rm /usr/local/bin/helm Helmを使用してKubernetes-dashboardをデプロイする 参考URL\nhttps://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard https://qiita.com/loftkun/items/bcfe1f205cde4d74a38 Helmにリポジトリを追加 1 helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ 1 2 3 mao@k8s-control-plane-01:~$ helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ \u0026#34;kubernetes-dashboard\u0026#34; has been added to your repositories mao@k8s-control-plane-01:~$ 追加されているか確認をする\n1 2 3 4 mao@k8s-control-plane-01:~$ helm repo list NAME URL kubernetes-dashboard https://kubernetes.github.io/dashboard/ mao@k8s-control-plane-01:~$ Helmを使用してデプロイ 公式サイトに記載にある手順でデプロイします\n1 helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 mao@k8s-control-plane-01:~$ helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard Release \u0026#34;kubernetes-dashboard\u0026#34; does not exist. Installing it now. NAME: kubernetes-dashboard LAST DEPLOYED: Thu Aug 8 12:22:06 2024 NAMESPACE: kubernetes-dashboard STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ************************************************************************************************* *** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready *** ************************************************************************************************* Congratulations! You have just installed Kubernetes Dashboard in your cluster. To access Dashboard run: kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 NOTE: In case port-forward command does not work, make sure that kong service name is correct. Check the services in Kubernetes Dashboard namespace using: kubectl -n kubernetes-dashboard get svc Dashboard will be available at: https://localhost:8443 mao@k8s-control-plane-01:~$ サービスアカウントの作成 参考URL\nhttps://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md マニフェストファイルを作成して以下の内容を書き込みます\ndashboard-adminuser.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard デプロイします\n1 kubectl apply -f dashboard-adminuser.yaml 1 2 3 4 mao@k8s-control-plane-01:~$ kubectl apply -f dashboard-adminuser.yaml serviceaccount/admin-user created clusterrolebinding.rbac.authorization.k8s.io/admin-user created mao@k8s-control-plane-01:~$ 外部からアクセスできるようにする ポートフォワードをする\n1 kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 Ctrl+Cでポートフォワード停止\nログイン用のトークンを確認 外部からアクセスできるようにしてブラウザにIPアドレスを入力して表示するとログインを求められるので、\n以下の方法でログイン用のトークンを発行します\n1 kubectl -n kubernetes-dashboard create token admin-user 1 2 3 mao@k8s-control-plane-01:~$ kubectl -n kubernetes-dashboard create token admin-user eyJhbGciOiJSUzI1NiIsImtpZCI6ImtRS1lrek1TRUE3LXRCeEVpU2hJRDhEUGhWT0lzeG1QM1lwNjBHbkl1MlkifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzIzMTI0NjUyLCJpYXQiOjE3MjMxMjEwNTIsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiNDk4ZTg0YjctNTFmZi00NWRjLTgxODYtOGVhODMwMTY4NDdkIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiNDczMzgwYWMtZWM3Yi00ZTBkLThiOTMtN2E1YjdkNDkzMTI5In19LCJuYmYiOjE3MjMxMjEwNTIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.WoGFlfvaUxchL1RqUP60GPEr7Q4laleBmixc-XyeZp48R4dVaZILC9eCsgDGJZbc3Q9_uh9ynVBle40lOLShwvsvbMcvrecNT-konG2HHCaUnMRV_nZeS7qaT5pYZCwKskW9AkdRsXS4dDKY7Wlj6jEStryM0OcYNdPm0JtxtXX6ejK5qA0wl6zPpcqLmnPHqMhRcUw0gHuIu9AL9cAoTT7sDUPBZSBwE1P5MS-eNAQx05xfNkUkilL1kNN-gnE-PX7u79uCXTi8lhoYlO48fQMOoo6MIGsqw9QBZcfx-uFGolwrWa3KmyQuaOAc8pghUjipGtV1KHSkEA1TDjx69g mao@k8s-control-plane-01:~$ ログイン画面 サービスアカウントの削除 1 kubectl delete -f dashboard-adminuser.yaml 1 2 3 4 mao@k8s-control-plane-01:~$ kubectl delete -f dashboard-adminuser.yaml serviceaccount \u0026#34;admin-user\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;admin-user\u0026#34; deleted mao@k8s-control-plane-01:~$ 上記の方法でサービスアカウントを削除すれば以下はやらなくても大丈夫 1 2 kubectl -n kubernetes-dashboard delete serviceaccount admin-user kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user ダッシュボードを削除 1 helm uninstall release_name -n release_namespace 1 helm uninstall kubernetes-dashboard -n kubernetes-dashboard 1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~/k8s$ helm ls -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION kubernetes-dashboard kubernetes-dashboard 1 2024-08-08 10:09:12.228703023 +0000 UTC deployed kubernetes-dashboard-7.5.0 mao@k8s-control-plane-01:~/k8s$ helm uninstall kubernetes-dashboard -n kubernetes-dashboard release \u0026#34;kubernetes-dashboard\u0026#34; uninstalled mao@k8s-control-plane-01:~/k8s$ helm ls -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mao@k8s-control-plane-01:~/k8s$ 無事削除されたことを確認 1 2 3 mao@k8s-control-plane-01:~$ kubectl -n kubernetes-dashboard get svc No resources found in kubernetes-dashboard namespace. mao@k8s-control-plane-01:~$ kubernetes-dashboardをクラスタ外からLoadBlancerを使いアクセスできるようにする 今までの方法だと外部からアクセスする際にポートフォワードをしなければならず、大変＋LoadBalancerがあるので、LoadBalancerでIPアドレスを割り当てて、そこに外部からアクセスできるようにしました。\n設定ファイルのダウンロード\nhttps://github.com/kubernetes/dashboard/blob/master/charts/kubernetes-dashboard/values.yaml 1 wget https://raw.githubusercontent.com/kubernetes/dashboard/master/charts/kubernetes-dashboard/values.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 mao@k8s-control-plane-01:~$ wget https://raw.githubusercontent.com/kubernetes/dashboard/master/charts/kubernetes-dashboard/values.yaml --2024-08-09 23:37:44-- https://raw.githubusercontent.com/kubernetes/dashboard/master/charts/kubernetes-dashboard/values.yaml Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 12820 (13K) [text/plain] Saving to: ‘values.yaml’ values.yaml 100%[===================\u0026gt;] 12.52K --.-KB/s in 0s 2024-08-09 23:37:44 (128 MB/s) - ‘values.yaml’ saved [12820/12820] mao@k8s-control-plane-01:~$ デプロイする\n\u0026ldquo;-f values.yaml\u0026quot;でファイルを指定する https://github.com/kubernetes/dashboard/tree/master/charts/kubernetes-dashboard 1 helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard -f values.yaml \u0026ldquo;values.yaml\u0026quot;ファイルの\u0026quot;kong.proxy.type\u0026quot;が\u0026quot;ClusterIP\u0026quot;になっていたので\u0026quot;LoadBalancer\u0026quot;へと変更したらクラスタ外からでもアクセスできるようになった 普通にIPアドレスのみでアクセスすると\u0026quot;400 Bad Request\u0026quot;になるので\u0026quot;https\u0026quot;を付けたらアクセスできた 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 kong: enabled: true ## Configuration reference: https://docs.konghq.com/gateway/3.6.x/reference/configuration env: dns_order: LAST,A,CNAME,AAAA,SRV plugins: \u0026#39;off\u0026#39; nginx_worker_processes: 1 ingressController: enabled: false dblessConfig: configMap: kong-dbless-config proxy: #type: ClusterIP type: LoadBalancer #loadBalancerIP: 192.168.10.57 http: enabled: false \u0026ldquo;EXTERNAL-IP\u0026quot;が割り当てられている 1 2 3 4 5 6 7 8 9 mao@k8s-control-plane-01:~$ kubectl -n kubernetes-dashboard get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard-api ClusterIP 10.111.166.219 \u0026lt;none\u0026gt; 8000/TCP 10m kubernetes-dashboard-auth ClusterIP 10.110.8.100 \u0026lt;none\u0026gt; 8000/TCP 10m kubernetes-dashboard-kong-manager NodePort 10.99.108.147 \u0026lt;none\u0026gt; 8002:31994/TCP,8445:31824/TCP 10m kubernetes-dashboard-kong-proxy LoadBalancer 10.103.61.118 192.168.10.55 443:32210/TCP 10m kubernetes-dashboard-metrics-scraper ClusterIP 10.111.179.107 \u0026lt;none\u0026gt; 8000/TCP 10m kubernetes-dashboard-web ClusterIP 10.100.158.51 \u0026lt;none\u0026gt; 8000/TCP 10m mao@k8s-control-plane-01:~$ リソースの削除 サービスアカウントの削除 1 kubectl delete -f dashboard-adminuser.yaml 1 2 3 4 mao@k8s-control-plane-01:~$ kubectl delete -f dashboard-adminuser.yaml serviceaccount \u0026#34;admin-user\u0026#34; deleted clusterrolebinding.rbac.authorization.k8s.io \u0026#34;admin-user\u0026#34; deleted mao@k8s-control-plane-01:~$ 上記の方法でサービスアカウントを削除すれば以下はやらなくても大丈夫 1 2 kubectl -n kubernetes-dashboard delete serviceaccount admin-user kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user ダッシュボードを削除 1 helm uninstall release_name -n release_namespace 1 helm uninstall kubernetes-dashboard -n kubernetes-dashboard 1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~/k8s$ helm ls -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION kubernetes-dashboard kubernetes-dashboard 1 2024-08-08 10:09:12.228703023 +0000 UTC deployed kubernetes-dashboard-7.5.0 mao@k8s-control-plane-01:~/k8s$ helm uninstall kubernetes-dashboard -n kubernetes-dashboard release \u0026#34;kubernetes-dashboard\u0026#34; uninstalled mao@k8s-control-plane-01:~/k8s$ helm ls -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mao@k8s-control-plane-01:~/k8s$ 無事削除されたことを確認 1 2 3 mao@k8s-control-plane-01:~$ kubectl -n kubernetes-dashboard get svc No resources found in kubernetes-dashboard namespace. mao@k8s-control-plane-01:~$ ","date":"2024-08-12T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-dashboard/","title":"Kubernetesのダッシュボードをデプロイする＋LoadBalancerでIPアドレスを割り振る"},{"content":"環境 Kubernetes 1.30.2 containerd 1.7.18 Calico 3.28.0 MetalLB 0.14.5 IPアドレスのプール:192.168.10.55-192.168.10.58 背景 MetalLBを使用してtype:loadBalancerが使用できるようになったが、IPアドレスの指定方法がわからなかったので、調べて作業してみました。\n\u0026ldquo;このサービスにはこのIPアドレスを使用する\u0026quot;のように自分でIPアドレスを指定したい。\n元のマニフェストファイル nginx-test.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 10 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.27 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-deployment-lb annotations: spec: type: LoadBalancer ports: - port: 80 targetPort: 80 selector: app: nginx 現状 普通にデプロイするとIPプールの中から空いているIPアドレスが付与される\n実行結果 1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl apply -f nginx-test.yaml deployment.apps/nginx-deployment created service/nginx-deployment-lb created mao@k8s-control-plane-01:~$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 38d nginx-deployment-lb LoadBalancer 10.96.182.105 192.168.10.55 80:30974/TCP 3s mao@k8s-control-plane-01:~$ 削除します 1 2 3 mao@k8s-control-plane-01:~$ kubectl delete -f nginx-test.yaml deployment.apps \u0026#34;nginx-deployment\u0026#34; deleted service \u0026#34;nginx-deployment-lb\u0026#34; deleted LoadBalancerでIPアドレスを指定する MetalLBを使用していると2つの方法があるようなので両方試してみます\nMetalLBのページでは\u0026quot;metadata\u0026quot;を使用した方法が推奨されている（spec.LoadBalancerIPはk8s apisで非推奨となる予定だからのようです）\nspec.loadBalancerIPに指定する \u0026ldquo;spec\u0026quot;に\u0026quot;loadBalancerIP:192.168.10.57\u0026quot;を追加してデプロイしてみます\nマニフェストファイル 1 2 3 4 5 6 7 8 9 10 11 12 13 --- apiVersion: v1 kind: Service metadata: name: nginx-deployment-lb spec: type: LoadBalancer + loadBalancerIP: 192.168.10.57 ports: - port: 80 targetPort: 80 selector: app: nginx 実行結果 1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl apply -f nginx-test.yaml deployment.apps/nginx-deployment created service/nginx-deployment-lb created mao@k8s-control-plane-01:~$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 38d nginx-deployment-lb LoadBalancer 10.111.232.189 192.168.10.57 80:31150/TCP 4s mao@k8s-control-plane-01:~$ しっかりと指定したIPアドレスがが付与されています\n削除します 1 2 3 mao@k8s-control-plane-01:~$ kubectl delete -f nginx-test.yaml deployment.apps \u0026#34;nginx-deployment\u0026#34; deleted service \u0026#34;nginx-deployment-lb\u0026#34; deleted metadata.annotationsに指定する マニフェストファイルを修正します\n\u0026ldquo;metadata\u0026quot;に\u0026quot;annotations:\u0026ldquo;と\u0026quot;metallb.universe.tf/loadBalancerIPs: 192.168.10.56\u0026quot;を追加する \u0026ldquo;loadBalancerIP: 192.168.10.57\u0026quot;を削除する 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion: v1 kind: Service metadata: name: nginx-deployment-lb + annotations: + metallb.universe.tf/loadBalancerIPs: 192.168.10.56 spec: type: LoadBalancer - loadBalancerIP: 192.168.10.57 ports: - port: 80 targetPort: 80 selector: app: nginx 実行結果 1 2 3 4 5 6 7 8 mao@k8s-control-plane-01:~$ kubectl apply -f nginx-test.yaml deployment.apps/nginx-deployment created service/nginx-deployment-lb created mao@k8s-control-plane-01:~$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 38d nginx-deployment-lb LoadBalancer 10.110.178.62 192.168.10.56 80:32442/TCP 3s mao@k8s-control-plane-01:~$ マニフェストファイルで指定したIPアドレスが付与されている\n終わったら削除する\n1 2 3 4 mao@k8s-control-plane-01:~$ kubectl delete -f nginx-test.yaml deployment.apps \u0026#34;nginx-deployment\u0026#34; deleted service \u0026#34;nginx-deployment-lb\u0026#34; deleted mao@k8s-control-plane-01:~$ 参考URL https://metallb.universe.tf/usage/ https://cstoku.dev/posts/2018/k8sdojo-09/ https://qiita.com/suzuyui/items/8f53a80edf2b32d45be2 https://blog.framinal.life/entry/2020/04/16/022042 ","date":"2024-08-11T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-loadbalancer-ip/","title":"KubernetesのLoadBalancerでIPアドレスを指定する方法"},{"content":"環境 Ansible 2.16.9 Kubernetes 1.30.3 containerd 1.7.20 runC 1.1.13 cni-plugins 1.5.1 ファイル・ディレクトリ構成 1 2 3 4 5 6 . ├── ansible.cfg ├── host.yaml └── playbook_k8s.yaml 1 directory, 3 files ansible.cfg\n1 2 3 [defaults] # fingerprintを検証しない設定 host_key_checking = False host.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 # YAML は ”---\u0026#34; から開始する --- # \u0026#34;all\u0026#34; グループの宣言 all: # \u0026#34;all\u0026#34; グループに含まれるホストに関する情報を定義する宣言 hosts: # 管理対象ノードの情報を定義する宣言 ansible-test-server: ansible_host: 192.168.10.18 ansible_user: mao ansible_password: mao ansible_ssh_private_key_file: /home/mao/ansible-test/ansible-ssh ansible_port: 22 playbook_k8s.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 # Ansible-playbook - name: k8s setup hosts: - all become: yes tasks: ##### 1 - name: ansible.builtin.get_url: dest: /home/mao/ url: https://github.com/containerd/containerd/releases/download/v1.7.20/containerd-1.7.20-linux-amd64.tar.gz - name: bin containerd ansible.builtin.unarchive: remote_src: true src: /home/mao/containerd-1.7.20-linux-amd64.tar.gz dest: /usr/local - name: ansible.builtin.get_url: dest: /home/mao/ url: https://raw.githubusercontent.com/containerd/containerd/main/containerd.service - name: ansible.builtin.copy: remote_src: true src: /home/mao/containerd.service dest: /etc/systemd/system/containerd.service - name: daemon reload ansible.builtin.systemd_service: daemon_reload: true - name: containerd enable ansible.builtin.systemd_service: name: containerd enabled: true ##### 2 - name: runC download ansible.builtin.get_url: dest: /home/mao/ url: https://github.com/opencontainers/runc/releases/download/v1.1.13/runc.amd64 - name: runC install ansible.builtin.command: cmd: install -m 755 runc.amd64 /usr/local/sbin/runc ##### 3 - name: CNI(Container Network Interface) plugin ansible.builtin.get_url: dest: /home/mao/ url: https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz - name: make directory ansible.builtin.file: path: /opt/cni/bin state: directory - name: ansible.builtin.unarchive: remote_src: true src: /home/mao/cni-plugins-linux-amd64-v1.5.1.tgz dest: /opt/cni/bin ##### 4 - name: ansible.builtin.shell: cmd: | cat \u0026gt; /etc/modules-load.d/k8s.conf \u0026lt;\u0026lt; EOF overlay br_netfilter EOF - name: ansible.builtin.command: cmd: modprobe overlay - name: ansible.builtin.command: cmd: modprobe br_netfilter - name: ansible.builtin.shell: cmd: | cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF - name: ansible.builtin.command: cmd: sysctl --system ##### 5 - name: make directory containerd ansible.builtin.file: path: /etc/containerd state: directory - name: cpoy ansible.builtin.shell: cmd: sudo containerd config default | sudo tee /etc/containerd/config.toml - name: 1 ansible.builtin.lineinfile: dest: /etc/containerd/config.toml state: present backrefs: yes regexp: sandbox_image = \u0026#34;registry.k8s.io/pause:3.8\u0026#34; line: \u0026#39; sandbox_image = \u0026#34;registry.k8s.io/pause:3.9\u0026#34;\u0026#39; - name: 2 ansible.builtin.lineinfile: dest: /etc/containerd/config.toml state: present backrefs: yes regexp: SystemdCgroup = false line: \u0026#39; SystemdCgroup = true\u0026#39; - name: 3 ansible.builtin.service: name: containerd state: restarted ##### 6 - name: install ansible.builtin.apt: pkg: - apt-transport-https - ca-certificates - curl - gpg update_cache: yes state: present - name: ansible.builtin.shell: cmd: curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg - name: ansible.builtin.shell: cmd: echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list # kubeadm:1.30.3,kubectl:1.30.3,kubelet:1.30.3 - name: ansible.builtin.apt: pkg: - kubelet - kubeadm - kubectl update_cache: yes state: present 内容 上記playbookの中にある\u0026quot;#####\u0026ldquo;は内容ごとに分割しています。 ファイルを分けるのは後々やりたいと思っています。\n1 containerdのインストール 2 runCのインストール 3 CNI pluginのインストール 4 パラメータの設定 5 containerdの追加設定 6 kubeadm等のインストール 上記playbookとは別にIPアドレスの固定化、kubernetesクラスタへの参加は手動です。\nこちらも近いうちに自動化できるようにしようと思います。\nそうすれば自動でクラスタが構築できるようになります！\nあとはリポジトリの追加等、Ansibleモジュールをちゃんと利用した方法へ置き換えていこうと思います。\n","date":"2024-07-27T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/ansible-k8s/","title":"Ansibleを使ってKubernetesの初期設定をしてみた"},{"content":"Ansibleの拡張機能のインストール VScodeの拡張機能\u0026quot;redhat.ansible\u0026quot;があるのでインストールしておくと便利になる Ansibleのインストール 参考URL\nhttps://docs.ansible.com/ansible/latest/installation_guide/installation_distros.html#installing-ansible-on-ubuntu 1 2 3 sudo apt install software-properties-common sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible バージョンの確認\n1 2 ansible --version which ansible ファイル構造を見れるようにtreeをインストール 1 sudo apt install tree アドホック・コマンド フォルダの構造\n1 2 3 4 5 6 7 8 mao@ansible-server:~/ansible-test$ tree . ├── ansible-ssh ├── ansible.cfg └── host.yaml 1 directory, 3 files mao@ansible-server:~/ansible-test$ ファイルの中身 ansible-ssh\n実行されてsshで接続されると生成されるファイル ansible.cfg（設定ファイル）\n1 2 3 [defaults] # fingerprintを検証しない設定 host_key_checking = False host.yaml（インベントリーファイル）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # YAML は ”---\u0026#34; から開始する --- # \u0026#34;all\u0026#34; グループの宣言 all: # \u0026#34;all\u0026#34; グループに含まれるホストに関する情報を定義する宣言 hosts: # 管理対象ノードの情報を定義する宣言 ansible-test-server: ansible_host: 192.168.10.10 ansible_user: mao ansible_password: mao ansible_ssh_private_key_file: /home/mao/ansible-test/ansible-ssh #ansible_python_interpreter: Python インタープリターの path ansible_port: 22 実行 下記コマンドを実行すると、それぞれのサーバーのストレージ容量を確認できる\n1 2 ansible all -i host.yaml -m ansible.builtin.command -a \u0026#34;df -h\u0026#34; ansible all -i hosts.yml -a \u0026#34;df -h\u0026#34; \u0026ldquo;-i\u0026quot;はインベントリーファイルの指定 \u0026ldquo;-m ansible.builtin.command\u0026quot;は短縮可能 実行結果\n1 2 3 4 5 6 7 8 9 10 mao@ansible-server:~/ansible-test$ ansible all -i host.yaml -m ansible.builtin.command -a \u0026#34;df -h\u0026#34; ansible-test-server | CHANGED | rc=0 \u0026gt;\u0026gt; Filesystem Size Used Avail Use% Mounted on tmpfs 795M 692K 794M 1% /run /dev/mapper/ubuntu--vg-ubuntu--lv 8.1G 2.5G 5.2G 32% / tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock /dev/sda2 1.7G 181M 1.5G 12% /boot tmpfs 795M 12K 795M 1% /run/user/1000 mao@ansible-server:~/ansible-test$ Ansible-playbookの実行 参考URL\nhttps://docs.ansible.com/ansible/latest/collections/index.html ファイル構造 1 2 3 4 5 6 7 8 9 mao@ansible-server:~/ansible-test$ tree . ├── ansible-ssh ├── ansible.cfg ├── host.yaml └── nginx_playbook.yaml 1 directory, 4 files mao@ansible-server:~/ansible-test$ \u0026ldquo;nginx_playbook.yaml\u0026quot;の中身\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Ansible-playbook - name: setup nginx server hosts: - all become: yes tasks: - name: Install ansible.builtin.command: apt install -y nginx - name: Start ansible.builtin.service: name: nginx state: started - name: Enable nginx ansible.builtin.service: name: nginx #enabled: yes enabled: no #- name: nginx version #ansible.builtin.command: nginx -v #- name: status #ansible.builtin.command: systemctl status nginx 実行 nginxがインストールされます\n1 ansible-playbook -i host.yaml nginx_playbook.yaml --ask-become-pass \u0026ldquo;\u0026ndash;ask-become-pass\u0026rdquo;:sudoパスワードを求めるオプション 実行結果\n実行時にパスワードを求められるので入力します 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mao@ansible-server:~/ansible-test$ ansible-playbook -i host.yaml nginx_playbook.yaml --ask-become-pass BECOME password: PLAY [setup nginx server] ******************************************************************** TASK [Gathering Facts] *********************************************************************** ok: [ansible-test-server] TASK [Install] ******************************************************************************* changed: [ansible-test-server] TASK [Start] ********************************************************************************* ok: [ansible-test-server] TASK [Enable nginx] ************************************************************************** ok: [ansible-test-server] PLAY RECAP *********************************************************************************** ansible-test-server : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 mao@ansible-server:~/ansible-test$ 無事nginxが起動しています\n","date":"2024-07-15T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/ansible/","title":"Ansibleを少し触ってみた"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 HAProxyをセットアップする 1 2 sudo apt update sudo apt upgrade IPアドレスを固定する 99-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 network: version: 2 renderer: networkd ethernets: ens18: dhcp4: false addresses: - 192.168.10.45/24 routes: - to: default via: 192.168.10.1 nameservers: search: [] addresses: [192.168.10.1] ファイルを反映する\n1 2 sudo cp 99-config.yaml /etc/netplan/ sudo netplan apply 1 sudo chmod 600 /etc/netplan/99-config.yaml HAProxyをインストールする 1 sudo apt install haproxy 1 2 sudo mv /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.default sudo nano /etc/haproxy/haproxy.cfg 下記の通りに編集する\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 defaults timeout connect 10s timeout client 30s timeout server 30s frontend k8s bind *:6443 mode tcp #option tcplog default_backend k8s_backend backend k8s_backend balance roundrobin server k8s-control-plane-01 192.168.10.41:6443 check server k8s-control-plane-02 192.168.10.44:6443 check #server k8s-control-plane-01 \u0026lt;control node2のip\u0026gt;:6443 check 反映する\n1 sudo systemctl enable --now haproxy HA構成のクラスタを構築する Control-Plane-01で下記コマンドを実行する 1 sudo kubeadm init --control-plane-endpoint=192.168.10.45:6443 --pod-network-cidr=10.128.0.0/16 --upload-certs \u0026ldquo;\u0026ndash;control-plane-endpoint=\u0026lt;IPアドレス\u0026gt;:6443\u0026rdquo;:Control-PlaneのIPアドレスとAPIサーバーのポートを指定する \u0026ldquo;\u0026ndash;pod-network-cidr=10.128.0.0/16\u0026rdquo;:Pod間ネットワークの指定する \u0026ldquo;\u0026ndash;upload-certs\u0026rdquo;:Control-Plane、Worker-Node間で共有する証明書を暗号化する 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 mao@k8s-control-plane-01:~$ sudo kubeadm init --control-plane-endpoint=192.168.10.45:6443 --pod-network-cidr=10.128.0.0/16 --upload-certs [init] Using Kubernetes version: v1.30.2 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-control-plane-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.41 192.168.10.45] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;super-admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34; [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 501.677267ms [api-check] Waiting for a healthy API server. This can take up to 4m0s [api-check] The API server is healthy after 5.012535655s [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Storing the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [upload-certs] Using certificate key: 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: h3eh17.f1j6k27nzi34hd0w [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 \\ --control-plane --certificate-key 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 mao@k8s-control-plane-01:~$ kubeadmコマンドを実行できるようにする 上記の実行結果に記載のある通り下記のコマンドを実行する\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config クラスとにJoinするために必要な情報をメモ（記録）しておく 上記の実行結果に記載のある通りクラスタにJoinする際に必要なコマンドをメモしておく\nControl-PlaneがクラスタにJoinするために必要な情報\n1 2 3 kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 \\ --control-plane --certificate-key 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 Worker-NodeがクラスタにJoinするために必要な情報\n1 2 kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 他のControl-PlaneをクラスタにJoinする 下記コマンドを実行する\n1 sudo kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 --control-plane --certificate-key 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 Woker-NodeをクラスタにJoinする Worker-NodeをクラスタにJoinさせる、1つ目以外のWoker-Nodeも同じようにクラスタに参加させる\n1 sudo kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mao@k8s-worker-01:~$ sudo kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 [sudo] password for mao: [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 500.998945ms [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. mao@k8s-worker-01:~$ HA構成の確認のためにControl-Planeを1つダウンさせてみる Control-Planeを1つシャットダウンさせて擬似的にダウンさせてみる\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-03:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 146m v1.30.2 k8s-control-plane-02 NotReady control-plane 140m v1.30.2 k8s-control-plane-03 Ready control-plane 10m v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 136m v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 127m v1.30.2 mao@k8s-control-plane-03:~$ 確認用のコマンドが問題なく実行され、Control-Planeが1つ\u0026quot;NotReady\u0026quot;になっているが、問題なくクラスタが稼働している\n参考 IPアドレスの構成 192.168.10.45,HAProxy 192.168.10.41,Control-Plane-01 192.168.10.44,Control-Plane-02 192.168.10.46,Control-Plane-03 192.168.10.42,Worker-Node-01 192.168.10.43,Worker-Node-02 参考URL https://kubernetes.io/ja/docs/setup/#production-environment https://kubernetes.io/ja/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/ https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 感想 以上でHA構成のkubernetesの構築は完了しました\n公式ドキュメントを参考にしつつ進めてわからないところは検索したりして構築できました\nネットワーク部分に関しては構築前よりも詳しくなったと思います\n発展として、他にも\u0026quot;keepalive\u0026quot;や\u0026quot;kube-vip\u0026quot;を使用した構成もあるみたいなので、いずれ試してみようと思います\nあとkubernetes上に仮想マシンを構成する\u0026quot;kubevirt\u0026quot;や\u0026quot;openstack on kubernetes\u0026quot;を構築してみようと思います\n","date":"2024-07-15T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-05/","title":"kubernetesをproxmox上に立ててみた（5）/HA構成"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 Worker-Nodeをクラスターから外す 先にControl-Planeですること 先にControl-Plane上でクラスターから外す準備をする\n1 2 kubectl drain k8s-worker-01 --ignore-daemonsets --delete-emptydir-data --force kubectl get node 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 mao@k8s-control-plane-01:~$ kubectl drain k8s-worker-01 --ignore-daemonsets --delete-emptydir-data --force node/k8s-worker-01 cordoned Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-h75vn, calico-system/csi-node-driver-5b5k4, kube-system/kube-proxy-wzvtf, metallb-system/speaker-xwb66 evicting pod metallb-system/controller-86f5578878-dz5zm pod/controller-86f5578878-dz5zm evicted node/k8s-worker-01 drained mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 3d23h v1.30.2 k8s-worker-01 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 3d22h v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 3d14h v1.30.2 mao@k8s-control-plane-01:~$ STATUSが\u0026quot;SchedulingDisabled\u0026quot;になったらOK\nWorker-Nodeでの作業 Control-Planeでの作業ができたら、次はWorker-Nodeで作業をする\n1 sudo kubeadm reset 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mao@k8s-worker-01:~$ sudo kubeadm reset W0630 01:12:34.493241 63962 preflight.go:56] [reset] WARNING: Changes made to this host by \u0026#39;kubeadm init\u0026#39; or \u0026#39;kubeadm join\u0026#39; will be reverted. [reset] Are you sure you want to proceed? [y/N]: y [preflight] Running pre-flight checks W0630 01:12:36.338665 63962 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory [reset] Deleted contents of the etcd data directory: /var/lib/etcd [reset] Stopping the kubelet service [reset] Unmounting mounted directories in \u0026#34;/var/lib/kubelet\u0026#34; [reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki] [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf] The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually by using the \u0026#34;iptables\u0026#34; command. If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar) to reset your system\u0026#39;s IPVS tables. The reset process does not clean your kubeconfig files and you must remove them manually. Please, check the contents of the $HOME/.kube/config file. Worker-Node上のcalicoを削除する 削除するネットワークインターフェースを確認する\n1 ip link 実行結果\n1 2 3 4 5 6 7 8 9 mao@k8s-worker-01:~$ ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:fe:35:a0 brd ff:ff:ff:ff:ff:ff altname enp0s18 10: vxlan.calico: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 66:bb:bb:d8:11:bf brd ff:ff:ff:ff:ff:ff mao@k8s-worker-01:~$ \u0026ldquo;vxlan.calico\u0026quot;を削除する\n1 sudo ip link delete vxlan.calico 最後にControl-PlaneからWorker-Nodeを削除する Control-Plane上で下記コマンドを実行する\n1 kubectl delete node k8s-worker-01 実行結果\n1 2 3 mao@k8s-control-plane-01:~$ kubectl delete node k8s-worker-01 node \u0026#34;k8s-worker-01\u0026#34; deleted mao@k8s-control-plane-01:~$ 削除されたことを確認する\n1 kubectl get node 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 3d23h v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 3d14h v1.30.2 mao@k8s-control-plane-01:~$ Control-Planeをクラスターから外す、クラスターを削除する Control-Plane上で、下記のコマンドを順に実行してクラスタをリセットする\n1 2 3 4 5 6 7 sudo kubeadm reset sudo rm -rf $HOME/.kube sudo systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet sudo systemctl restart containerd sudo ip link sudo ip link delete vxlan.calico sudo ip link 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 mao@k8s-control-plane-01:~$ sudo kubeadm reset [reset] Reading configuration from the cluster... [reset] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; W0630 03:05:22.877860 124743 preflight.go:56] [reset] WARNING: Changes made to this host by \u0026#39;kubeadm init\u0026#39; or \u0026#39;kubeadm join\u0026#39; will be reverted. [reset] Are you sure you want to proceed? [y/N]: y [preflight] Running pre-flight checks [reset] Deleted contents of the etcd data directory: /var/lib/etcd [reset] Stopping the kubelet service [reset] Unmounting mounted directories in \u0026#34;/var/lib/kubelet\u0026#34; W0630 03:05:30.442255 124743 cleanupnode.go:106] [reset] Failed to remove containers: [failed to stop running pod 67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424: output: E0630 03:05:29.277302 126592 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4: output: E0630 03:05:29.422959 126734 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12: output: E0630 03:05:29.566837 126875 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5: output: E0630 03:05:29.713590 127015 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976: output: E0630 03:05:29.858624 127156 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112: output: E0630 03:05:30.010265 127297 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\u0026#34; time=\u0026#34;2024-06-30T03:05:30Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1] [reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki] [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf] The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually by using the \u0026#34;iptables\u0026#34; command. If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar) to reset your system\u0026#39;s IPVS tables. The reset process does not clean your kubeconfig files and you must remove them manually. Please, check the contents of the $HOME/.kube/config file. mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ sudo systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ==== Authentication is required to restart \u0026#39;kubelet.service\u0026#39;. Authenticating as: mao Password: ==== AUTHENTICATION COMPLETE ==== mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 8 9 mao@k8s-control-plane-01:~$ sudo ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:7f:08:e4 brd ff:ff:ff:ff:ff:ff altname enp0s18 9: vxlan.calico: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 66:ca:40:c1:cc:6f brd ff:ff:ff:ff:ff:ff mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ sudo ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:7f:08:e4 brd ff:ff:ff:ff:ff:ff altname enp0s18 mao@k8s-control-plane-01:~$ これでクラスターをリセットができました\n","date":"2024-07-10T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-04/","title":"kubernetesをproxmox上に立ててみた（4）/クラスターを壊す"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 LoadBalancer（MetalLB）の設定をする kubernetesのクラスターの外部からIPアドレスでアクセスするための設定をする ロードバランサー（MetalLB）を使用して外部からアクセスできるようにする マニフェストファイルの\u0026quot;type\u0026quot;に\u0026quot;LoadBalancer\u0026quot;を指定できるようになる MetalLB を実行する 今回はMetalLBを使用する\nhttps://metallb.universe.tf/installation/ ARPの設定をする 1 1 2 3 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl diff -f - -n kube-system 2 1 2 3 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system 実行結果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mao@k8s-control-plane-01:~$ kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl diff -f - -n kube-system diff -u -N /tmp/LIVE-1007559117/v1.ConfigMap.kube-system.kube-proxy /tmp/MERGED-3311346621/v1.ConfigMap.kube-system.kube-proxy --- /tmp/LIVE-1007559117/v1.ConfigMap.kube-system.kube-proxy 2024-06-26 12:33:11.717730136 +0000 +++ /tmp/MERGED-3311346621/v1.ConfigMap.kube-system.kube-proxy 2024-06-26 12:33:11.718730159 +0000 @@ -37,7 +37,7 @@ excludeCIDRs: null minSyncPeriod: 0s scheduler: \u0026#34;\u0026#34; - strictARP: false + strictARP: true syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s mao@k8s-control-plane-01:~$ kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system Warning: resource configmaps/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. configmap/kube-proxy configured mao@k8s-control-plane-01:~$ MetalLBの実行をする 参考URL\nhttps://blog.ntnx.jp/entry/2024/02/14/025309 1 2 wget https://raw.githubusercontent.com/metallb/metallb/v0.14.5/config/manifests/metallb-native.yaml kubectl apply -f metallb-native.yaml 確認をする\n1 kubectl get pod -n metallb-system 実行結果\n1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ kubectl get pod -n metallb-system NAME READY STATUS RESTARTS AGE controller-86f5578878-dz5zm 1/1 Running 0 43s speaker-mbvpk 1/1 Running 0 43s speaker-rx75f 1/1 Running 0 43s speaker-xwb66 1/1 Running 0 43s mao@k8s-control-plane-01:~$ 払い出せるIPアドレスの範囲を設定する\n下記の部分で範囲を設定する 1 2 3 spec: addresses: - 192.168.10.55-192.168.10.60 1 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt; EOF \u0026gt; ippool.yaml apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: default namespace: metallb-system spec: addresses: - 192.168.10.55-192.168.10.60 EOF 2 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt; EOF \u0026gt; l2adv.yaml apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: default namespace: metallb-system spec: ipAddressPools: - default EOF 設定を実行する\n1 kubectl apply -f ippool.yaml -f l2adv.yaml 実行結果\n1 2 3 mao@k8s-control-plane-01:~$ kubectl apply -f ippool.yaml -f l2adv.yaml ipaddresspool.metallb.io/default created l2advertisement.metallb.io/default created マニフェストファイルを実行後、外部IPアドレスが設定されているか確認する マニフェストファイル\nnginx-lb.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 10 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.27 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-deployment-lb spec: type: LoadBalancer #type: ClusterIP ports: - port: 80 targetPort: 80 selector: app: nginx \u0026ldquo;type: LoadBalancer\u0026quot;にするとLoadBalancerからIPアドレスが払い出されてクラスタ外からアクセスできるようになる マニフェストファイルを実行してデプロイする\n1 kubectl apply -f nginx-lb.yaml 確認するためのコマンド\n1 2 3 kubectl get svc or kubectl get service 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 25h nginx-deployment-lb LoadBalancer 10.98.208.100 192.168.10.45 80:32762/TCP 58s mao@k8s-control-plane-01:~$ \u0026ldquo;nginx-deployment-lb\u0026quot;の\u0026quot;TYPE\u0026quot;に\u0026quot;LoadBalancer\u0026quot;が指定されており\u0026quot;EXTERNAL-IP\u0026rdquo;（外部IPアドレス）が割り当てられている\nこのIPアドレスにアクセスすると実際のコンテナにアクセスできる\n","date":"2024-07-07T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-03/","title":"kubernetesをproxmox上に立ててみた（3）/LoadBalancerの設定"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 Control-Plane（Master-Node）の設定をする kubeadm init を実行する Control-Planeにするマシン上で実行する\n1 2 sudo kubeadm init --apiserver-advertise-address=Control-PlaneのIPアドレス --pod-network-cidr=10.128.0.0/16 sudo kubeadm init --apiserver-advertise-address=192.168.10.41 --pod-network-cidr=10.128.0.0/16 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 mao@k8s-control-plane-01:~$ sudo kubeadm init --apiserver-advertise-address=192.168.10.41 --pod-network-cidr=10.128.0.0/16 [init] Using Kubernetes version: v1.30.2 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-control-plane-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.41] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;super-admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34; [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 500.625027ms [api-check] Waiting for a healthy API server. This can take up to 4m0s [api-check] The API server is healthy after 3.50086825s [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: evbpii.dp8y5hcfqkv9jn4n [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n \\ --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 mao@k8s-control-plane-01:~$ 実行結果にも表示されている通り、下記のコマンドを実行する\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Calicoを実行する（Pod間ネットワーク） 参考URL\nhttps://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart tigera-operator.yaml\n1 2 wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml kubectl create -f tigera-operator.yaml custom-resources.yaml\n1 wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml ダウンロードしたcustom-resources.yamlを編集する\nkubeadm init で指定した引数\u0026ndash;pod-network-cidrと同じものへ変更する 1 2 - cidr: 192.168.0.0/16 + cidr: 10.128.0.0/16 実行する\n1 kubectl apply -f custom-resources.yaml Nodeを確認する 1 kubectl get nodes -o wide 実行結果\n1 2 3 4 mao@k8s-control-plane-01:~$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-control-plane-01 Ready control-plane 32m v1.30.2 192.168.10.41 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-35-generic containerd://1.7.18 mao@k8s-control-plane-01:~$ Worker-Nodeの設定をする Worker-NodeをJoinする Control-Planeでkubeadm initを実行した際に表示されたコマンドを実行する\n1 sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mao@k8s-worker-01:~$ sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-to ken-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 501.168161ms [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. mao@k8s-worker-01:~$ Worker-NodeがJoinされているか、Control-Planeで確認する\n1 kubectl get node 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 63m v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 36s v1.30.2 mao@k8s-control-plane-01:~$ 複数のWorker-NodeをJoinする 基本手順は1つ目のWorker-Nodeと同じように実行するとJoinできる\n1 sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 実行したらControl-PlanでNodeを確認してみる\n1 2 3 4 5 6 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 9h v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 8h v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 83s v1.30.2 mao@k8s-control-plane-01:~$ 無事にWorker-NodeがJoinされている\n","date":"2024-07-06T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-02/","title":"kubernetesをproxmox上に立ててみた（2）/Control-Plane・Worker-Nodeの設定"},{"content":" k8sを勉強してみようと思い、デプロイ等の操作は書籍で触っていたら構築もしてみたくなったので、Proxmox上に仮想マシンを作成してk8sを構築してみた\n開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 構成 Proxmox上に以下6つの仮想マシンを立てました\nhaproxy-01 control-plane-node-01 control-plane-node-02 control-plane-node-03 worker-node-01 worker-node-02 Control-Plane（Master-Node）とWorker-Node両方で実行する setup パッケージの更新をする\n1 2 sudo apt update sudo apt upgrade 必要なソフトウェアをインストールする\n1 sudo apt install nano 公式の手順にそって実行する\nhttps://kubernetes.io/ja/docs/setup/production-environment/ Swapをオフにする swapを止めます\n1 sudo swapoff -a 設定ファイルを書き換えて永続的にswapをオフにする\n1 sudo nano /etc/fstab 編集内容\n1 2 - /swap.img none swap sw 0 0 + #/swap.img none swap sw 0 0 swapがオフになっているか確認する\n1 free -h 実行結果\n1 2 3 4 mao@k8s-control-plane-01:~$ free -h total used free shared buff/cache available Mem: 7.8Gi 510Mi 7.2Gi 704Ki 248Mi 7.3Gi Swap: 0B 0B 0B IPアドレスを固定IPアドレスにする ネットワークのデバイスを確認します\n1 ip address 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mao@k8s-control-plane-01:~$ ip address 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether bc:24:11:7f:08:e4 brd ff:ff:ff:ff:ff:ff altname enp0s18 inet 192.168.10.10/24 metric 100 brd 192.168.10.255 scope global dynamic ens18 valid_lft 85953sec preferred_lft 85953sec inet6 fe80::be24:11ff:fe7f:8e4/64 scope link valid_lft forever preferred_lft forever mao@k8s-control-plane-01:~$ netplanファイルを作成します\nファイル名：99-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 network: version: 2 renderer: networkd ethernets: ens18: dhcp4: false addresses: - 192.168.10.41/24 routes: - to: default via: 192.168.10.1 nameservers: search: [] addresses: [192.168.10.1] netplanファイルをコピーして適用します\nssh等で接続している場合は、IPアドレスが変わるので接続が切れます、\n再度固定にしたIPアドレスに変更すれば接続できます 1 2 3 sudo cp 99-config.yaml /etc/netplan/ sudo netplan apply sudo chmod 600 /etc/netplan/99-config.yaml containerdをインストールする 公式手順に従ってインストール\nhttps://github.com/containerd/containerd/blob/main/docs/getting-started.md Option 1: From the official binaries containerd 1.7.18 順にコマンドを実行する\n1 2 wget https://github.com/containerd/containerd/releases/download/v1.7.18/containerd-1.7.18-linux-amd64.tar.gz sudo tar Cxzvf /usr/local containerd-1.7.18-linux-amd64.tar.gz 1 sudo wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -O /etc/systemd/system/containerd.service systemctlをリロードし、containerdを有効にする\n1 2 sudo systemctl daemon-reload sudo systemctl enable --now containerd runCをインストールする 1 sudo wget https://github.com/opencontainers/runc/releases/download/v1.1.13/runc.amd64 1 sudo install -m 755 runc.amd64 /usr/local/sbin/runc CNI(Container Network Interface) pluginをインストールする 1 sudo wget https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz 1 2 sudo mkdir -p /opt/cni/bin sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.5.1.tgz IPv4フォワーディングの設定をする 以下のコマンドを順に実行する\n1 1 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf 実行結果\n1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf \u0026gt; overlay \u0026gt; br_netfilter \u0026gt; EOF overlay br_netfilter mao@k8s-control-plane-01:~$ 1 2 sudo modprobe overlay sudo modprobe br_netfilter 2 1 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf 実行結果\n1 2 3 4 5 6 7 8 9 mao@k8s-control-plane-01:~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf \u0026gt; net.bridge.bridge-nf-call-iptables = 1 \u0026gt; net.bridge.bridge-nf-call-ip6tables = 1 \u0026gt; net.ipv4.ip_forward = 1 \u0026gt; EOF net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 mao@k8s-control-plane-01:~$ 1 sudo sysctl --system 3 1 2 lsmod | grep br_netfilter lsmod | grep overlay 実行結果\n1 2 3 4 5 6 mao@k8s-control-plane-01:~$ lsmod | grep br_netfilter br_netfilter 32768 0 bridge 421888 1 br_netfilter mao@k8s-control-plane-01:~$ lsmod | grep overlay overlay 212992 0 mao@k8s-control-plane-01:~$ 4 1 sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 mao@k8s-control-plane-01:~$ systemd cgroup の設定をする 参考URL\nhttps://kubernetes.io/ja/docs/concepts/architecture/cgroups/ https://kubernetes.io/ja/docs/concepts/architecture/cgroups/#check-cgroup-version https://sogo.dev/posts/2022/12/kubernetes-ubuntu22.04-cgroup-systemd 1 stat -fc %T /sys/fs/cgroup/ cgroup v2では、\u0026ldquo;cgroup2fs\u0026quot;と出力されます。 cgroup v1では、\u0026ldquo;tmpfs\u0026quot;と出力されます。 ディレクトリを作成する\n1 sudo mkdir /etc/containerd 以下のコマンドで、デフォルトのコンフィグを作成できます。\n1 sudo containerd config default | sudo tee /etc/containerd/config.toml 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 mao@k8s-control-plane-01:~$ sudo containerd config default | sudo tee /etc/containerd/config.toml disabled_plugins = [] imports = [] oom_score = 0 plugin_dir = \u0026#34;\u0026#34; required_plugins = [] root = \u0026#34;/var/lib/containerd\u0026#34; state = \u0026#34;/run/containerd\u0026#34; temp = \u0026#34;\u0026#34; version = 2 [cgroup] path = \u0026#34;\u0026#34; [debug] address = \u0026#34;\u0026#34; format = \u0026#34;\u0026#34; gid = 0 level = \u0026#34;\u0026#34; uid = 0 [grpc] address = \u0026#34;/run/containerd/containerd.sock\u0026#34; gid = 0 max_recv_message_size = 16777216 max_send_message_size = 16777216 tcp_address = \u0026#34;\u0026#34; tcp_tls_ca = \u0026#34;\u0026#34; tcp_tls_cert = \u0026#34;\u0026#34; tcp_tls_key = \u0026#34;\u0026#34; uid = 0 [metrics] address = \u0026#34;\u0026#34; grpc_histogram = false [plugins] [plugins.\u0026#34;io.containerd.gc.v1.scheduler\u0026#34;] deletion_threshold = 0 mutation_threshold = 100 pause_threshold = 0.02 schedule_delay = \u0026#34;0s\u0026#34; startup_delay = \u0026#34;100ms\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] cdi_spec_dirs = [\u0026#34;/etc/cdi\u0026#34;, \u0026#34;/var/run/cdi\u0026#34;] device_ownership_from_security_context = false disable_apparmor = false disable_cgroup = false disable_hugetlb_controller = true disable_proc_mount = false disable_tcp_service = true drain_exec_sync_io_timeout = \u0026#34;0s\u0026#34; enable_cdi = false enable_selinux = false enable_tls_streaming = false enable_unprivileged_icmp = false enable_unprivileged_ports = false ignore_deprecation_warnings = [] ignore_image_defined_volumes = false image_pull_progress_timeout = \u0026#34;5m0s\u0026#34; image_pull_with_sync_fs = false max_concurrent_downloads = 3 max_container_log_line_size = 16384 netns_mounts_under_state_dir = false restrict_oom_score_adj = false sandbox_image = \u0026#34;registry.k8s.io/pause:3.8\u0026#34; selinux_category_range = 1024 stats_collect_period = 10 stream_idle_timeout = \u0026#34;4h0m0s\u0026#34; stream_server_address = \u0026#34;127.0.0.1\u0026#34; stream_server_port = \u0026#34;0\u0026#34; systemd_cgroup = false tolerate_missing_hugetlb_controller = true unset_seccomp_profile = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.cni] bin_dir = \u0026#34;/opt/cni/bin\u0026#34; conf_dir = \u0026#34;/etc/cni/net.d\u0026#34; conf_template = \u0026#34;\u0026#34; ip_pref = \u0026#34;\u0026#34; max_conf_num = 1 setup_serially = false [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd] default_runtime_name = \u0026#34;runc\u0026#34; disable_snapshot_annotations = true discard_unpacked_layers = false ignore_blockio_not_enabled_errors = false ignore_rdt_not_enabled_errors = false no_pivot = false snapshotter = \u0026#34;overlayfs\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;\u0026#34; sandbox_mode = \u0026#34;\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime.options] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; sandbox_mode = \u0026#34;podsandbox\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] BinaryName = \u0026#34;\u0026#34; CriuImagePath = \u0026#34;\u0026#34; CriuPath = \u0026#34;\u0026#34; CriuWorkPath = \u0026#34;\u0026#34; IoGid = 0 IoUid = 0 NoNewKeyring = false NoPivotRoot = false Root = \u0026#34;\u0026#34; ShimCgroup = \u0026#34;\u0026#34; SystemdCgroup = false [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.untrusted_workload_runtime] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;\u0026#34; sandbox_mode = \u0026#34;\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.untrusted_workload_runtime.options] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.image_decryption] key_model = \u0026#34;node\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.auths] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.headers] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.x509_key_pair_streaming] tls_cert_file = \u0026#34;\u0026#34; tls_key_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.opt\u0026#34;] path = \u0026#34;/opt/containerd\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.restart\u0026#34;] interval = \u0026#34;10s\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.tracing\u0026#34;] [plugins.\u0026#34;io.containerd.metadata.v1.bolt\u0026#34;] content_sharing_policy = \u0026#34;shared\u0026#34; [plugins.\u0026#34;io.containerd.monitor.v1.cgroups\u0026#34;] no_prometheus = false [plugins.\u0026#34;io.containerd.nri.v1.nri\u0026#34;] disable = true disable_connections = false plugin_config_path = \u0026#34;/etc/nri/conf.d\u0026#34; plugin_path = \u0026#34;/opt/nri/plugins\u0026#34; plugin_registration_timeout = \u0026#34;5s\u0026#34; plugin_request_timeout = \u0026#34;2s\u0026#34; socket_path = \u0026#34;/var/run/nri/nri.sock\u0026#34; [plugins.\u0026#34;io.containerd.runtime.v1.linux\u0026#34;] no_shim = false runtime = \u0026#34;runc\u0026#34; runtime_root = \u0026#34;\u0026#34; shim = \u0026#34;containerd-shim\u0026#34; shim_debug = false [plugins.\u0026#34;io.containerd.runtime.v2.task\u0026#34;] platforms = [\u0026#34;linux/amd64\u0026#34;] sched_core = false [plugins.\u0026#34;io.containerd.service.v1.diff-service\u0026#34;] default = [\u0026#34;walking\u0026#34;] [plugins.\u0026#34;io.containerd.service.v1.tasks-service\u0026#34;] blockio_config_file = \u0026#34;\u0026#34; rdt_config_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.aufs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.blockfile\u0026#34;] fs_type = \u0026#34;\u0026#34; mount_options = [] root_path = \u0026#34;\u0026#34; scratch_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.btrfs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.devmapper\u0026#34;] async_remove = false base_image_size = \u0026#34;\u0026#34; discard_blocks = false fs_options = \u0026#34;\u0026#34; fs_type = \u0026#34;\u0026#34; pool_name = \u0026#34;\u0026#34; root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.native\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.overlayfs\u0026#34;] mount_options = [] root_path = \u0026#34;\u0026#34; sync_remove = false upperdir_label = false [plugins.\u0026#34;io.containerd.snapshotter.v1.zfs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.tracing.processor.v1.otlp\u0026#34;] [plugins.\u0026#34;io.containerd.transfer.v1.local\u0026#34;] config_path = \u0026#34;\u0026#34; max_concurrent_downloads = 3 max_concurrent_uploaded_layers = 3 [[plugins.\u0026#34;io.containerd.transfer.v1.local\u0026#34;.unpack_config]] differ = \u0026#34;\u0026#34; platform = \u0026#34;linux/amd64\u0026#34; snapshotter = \u0026#34;overlayfs\u0026#34; [proxy_plugins] [stream_processors] [stream_processors.\u0026#34;io.containerd.ocicrypt.decoder.v1.tar\u0026#34;] accepts = [\u0026#34;application/vnd.oci.image.layer.v1.tar+encrypted\u0026#34;] args = [\u0026#34;--decryption-keys-path\u0026#34;, \u0026#34;/etc/containerd/ocicrypt/keys\u0026#34;] env = [\u0026#34;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\u0026#34;] path = \u0026#34;ctd-decoder\u0026#34; returns = \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34; [stream_processors.\u0026#34;io.containerd.ocicrypt.decoder.v1.tar.gzip\u0026#34;] accepts = [\u0026#34;application/vnd.oci.image.layer.v1.tar+gzip+encrypted\u0026#34;] args = [\u0026#34;--decryption-keys-path\u0026#34;, \u0026#34;/etc/containerd/ocicrypt/keys\u0026#34;] env = [\u0026#34;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\u0026#34;] path = \u0026#34;ctd-decoder\u0026#34; returns = \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34; [timeouts] \u0026#34;io.containerd.timeout.bolt.open\u0026#34; = \u0026#34;0s\u0026#34; \u0026#34;io.containerd.timeout.metrics.shimstats\u0026#34; = \u0026#34;2s\u0026#34; \u0026#34;io.containerd.timeout.shim.cleanup\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.load\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.shutdown\u0026#34; = \u0026#34;3s\u0026#34; \u0026#34;io.containerd.timeout.task.state\u0026#34; = \u0026#34;2s\u0026#34; [ttrpc] address = \u0026#34;\u0026#34; gid = 0 uid = 0 mao@k8s-control-plane-01:~$ 設定ファイルを編集する\n1 sudo nano /etc/containerd/config.toml 以下の2箇所を編集する\n1 2 3 4 5 6 7 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] - sandbox_image = \u0026#34;registry.k8s.io/pause:3.6\u0026#34; + sandbox_image = \u0026#34;registry.k8s.io/pause:3.9\u0026#34; ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] - SystemdCgroup = false + SystemdCgroup = true containerdを再起動する\n1 sudo systemctl restart containerd kubeadm/kubelet/kubectl をインストールする 参考URL\nhttps://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 以下のコマンドを順番に実行する\n1 sudo apt install apt-transport-https ca-certificates curl gpg 1 curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 1 echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list 一度アップデートした後にインストールする\n1 2 sudo apt update sudo apt install kubelet kubeadm kubectl バージョンを固定する\n1 sudo apt-mark hold kubelet kubeadm kubectl 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ sudo apt-mark hold kubelet kubeadm kubectl kubelet set on hold. kubeadm set on hold. kubectl set on hold. mao@k8s-control-plane-01:~$ バージョン固定の解除コマンド\n1 2 sudo apt-mark showhold sudo apt-mark unhold \u0026lt;パッケージ名\u0026gt; ","date":"2024-07-03T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-01/","title":"kubernetesをproxmox上に立ててみた（1）"},{"content":"環境 Proxmox VE 8.2.4 x86_64 アップグレード前 メモリ16GB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@pve:~# neofetch .://:` `://:. root@pve `hMMMMMMd/ /dMMMMMMh` -------- `sMMMMMMMd: :mMMMMMMMs` OS: Proxmox VE 8.2.4 x86_64 `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` Kernel: 6.8.4-2-pve `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` Uptime: 33 days, 20 hours, 14 mins `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Packages: 852 (dpkg) ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. Shell: bash 5.2.15 .+ooooooo+-`oNMMMMNo`-+ooooooo+. Terminal: /dev/pts/0 -+ooooooo/.`sMMs`./ooooooo+- CPU: AMD Ryzen 7 5700G with Radeon Graphi :oooooooo/`..`/oooooooo: GPU: AMD ATI Radeon Vega Series / Radeon :oooooooo/`..`/oooooooo: Memory: 1858MiB / 13837MiB -+ooooooo/.`sMMs`./ooooooo+- .+ooooooo+-`oNMMMMNo`-+ooooooo+. ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` `sMMMMMMMm: :dMMMMMMMs` `hMMMMMMd/ /dMMMMMMh` `://:` `://:` root@pve:~# アップグレード後 メモリ64GB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@pve:~# neofetch .://:` `://:. root@pve `hMMMMMMd/ /dMMMMMMh` -------- `sMMMMMMMd: :mMMMMMMMs` OS: Proxmox VE 8.2.4 x86_64 `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` Kernel: 6.8.8-1-pve `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` Uptime: 1 min `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Packages: 852 (dpkg) ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. Shell: bash 5.2.15 .+ooooooo+-`oNMMMMNo`-+ooooooo+. Terminal: /dev/pts/0 -+ooooooo/.`sMMs`./ooooooo+- CPU: AMD Ryzen 7 5700G with Radeon Graphics (16) @ 4.673GHz :oooooooo/`..`/oooooooo: GPU: AMD ATI Radeon Vega Series / Radeon Vega Mobile Series :oooooooo/`..`/oooooooo: Memory: 1508MiB / 60133MiB -+ooooooo/.`sMMs`./ooooooo+- .+ooooooo+-`oNMMMMNo`-+ooooooo+. ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` `sMMMMMMMm: :dMMMMMMMs` `hMMMMMMd/ /dMMMMMMh` `://:` `://:` root@pve:~# ","date":"2024-06-22T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/deskminix300-memory-upgrade/","title":"ProxmoxがインストールされているDeskminix300のメモリを増設する"},{"content":"環境 Ubuntu 24.04 Zbbix 7.0 Ubuntu MySQL Nginx 手順 このページの通りにインストールをしていく(\u0026ldquo;https://www.zabbix.com/jp/download?zabbix=7.0\u0026os_distribution=ubuntu\u0026os_version=24.04\u0026components=server_frontend_agent\u0026db=mysql\u0026ws=nginx\") ただし、MySQLは別でインストールをする必要がある 以下に公式手順をコピペしたものを記載しています\nZabbixリポジトリをインストールする 1 2 3 wget https://repo.zabbix.com/zabbix/7.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_7.0-1+ubuntu24.04_all.deb dpkg -i zabbix-release_7.0-1+ubuntu24.04_all.deb apt update Zabbixサーバー、フロントエンド、エージェントをインストールする 1 apt install zabbix-server-mysql zabbix-frontend-php zabbix-nginx-conf zabbix-sql-scripts zabbix-agent MySQLをインストールする 1 sudo apt install mysql-server 初期データベースを作成する 1 2 mysql -uroot -p password 1 2 3 4 5 mysql\u0026gt; create database zabbix character set utf8mb4 collate utf8mb4_bin; mysql\u0026gt; create user zabbix@localhost identified by \u0026#39;password\u0026#39;; mysql\u0026gt; grant all privileges on zabbix.* to zabbix@localhost; mysql\u0026gt; set global log_bin_trust_function_creators = 1; mysql\u0026gt; quit; Zabbix サーバー ホストで初期スキーマとデータをインポートします。新しく作成したパスワードを入力するよう求められます。 1 zcat /usr/share/zabbix-sql-scripts/mysql/server.sql.gz | mysql --default-character-set=utf8mb4 -uzabbix -p zabbix データベース スキーマをインポートした後、log_bin_trust_function_creators オプションを無効にします。\n1 2 mysql -uroot -p password 1 2 mysql\u0026gt; set global log_bin_trust_function_creators = 0; mysql\u0026gt; quit; Zabbixサーバーのデータベースを構成する ファイル /etc/zabbix/zabbix_server.conf を編集します。\n1 DBPassword=password Zabbixフロントエンド用にPHPを構成する ファイル /etc/zabbix/nginx.conf を編集し、コメントアウトを解除して \u0026rsquo;listen\u0026rsquo; および \u0026lsquo;server_name\u0026rsquo; ディレクティブを設定します。\n1 2 listen 8080; server_name example.com; Zabbixサーバーとエージェントのプロセスを起動する Zabbix サーバーおよびエージェント プロセスを起動し、システムの起動時に起動するようにします。\n1 2 systemctl restart zabbix-server zabbix-agent nginx php8.3-fpm systemctl enable zabbix-server zabbix-agent nginx php8.3-fpm Zabbix UI Webページを開く 1 IPアドレス:8080 初期設定をします\nブラウザの画面 ログインする際の初期ID・パスワードは以下の通りです\nUsername：Admin Password：zabbix 参考URL https://www.zabbix.com/jp/download?zabbix=7.0\u0026os_distribution=ubuntu\u0026os_version=24.04\u0026components=server_frontend_agent\u0026db=mysql\u0026ws=nginx https://www.site24x7.jp/blog/zabbix-6-construction/ 備考：MySQLをアンインストールして再インストールする MySQLをインストールしてZbbixのデータベースを作成する際にエラーになってしまったので、再度インストールをした際の手順です\nアンインストール 1 2 3 4 5 6 sudo apt update sudo apt upgrade sudo apt purge mysql* sudo rm -rf /etx/mysql /var/lib/mysql sudo apt autoremove sudo apt autoclean 再インストール 1 sudo apt install mysql-server ","date":"2024-06-22T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/zabbiz-install/","title":"Zbbix7.0LTSをUbuntuにインストールする"},{"content":"環境 Windows 11 Home バージョン 23H2 hugo v0.121.1-extended windows/amd64 hugoのテーマ：stack(\u0026ldquo;https://github.com/CaiJimmy/hugo-theme-stack\") 更新日時の表示 記事のMarkdownファイルに\u0026quot;lastmod:\u0026ldquo;を追加し、更新日時を入れます\n1 2 3 4 title: xxx date: 2024-06-01 lastmod: 2024-06-14 slug: そうすると記事の一番下に更新日時が表示されます\nただ、一番下なので記事を見たとき更新日時をすぐに確認できないので、作成日時の横に更新日時を表示させられるようにします\n作成日時の横に更新日時を表示 下記のパスにある\u0026quot;footer.html\u0026quot;を開きます\n1 ./layouts/partials/article/components/footer.html 上記ファイルの中にある下記の部分をコピーします 下記が更新日時を表示させているコードです\n1 2 3 4 5 6 7 8 {{- if ne .Lastmod .Date -}} \u0026lt;section class=\u0026#34;article-lastmod\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;span\u0026gt; {{ T \u0026#34;article.lastUpdatedOn\u0026#34; }} {{ .Lastmod.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/span\u0026gt; \u0026lt;/section\u0026gt; {{- end -}} 下記のパスにある\u0026quot;details.html\u0026quot;を開きます\n1 ./layouts/partials/article/components/details.html 下記のコメント（20行目から29行目）を付けた部分に先ほどコピーしたコードを追加します\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026lt;footer class=\u0026#34;article-time\u0026#34;\u0026gt; {{ if $showDate }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;date\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--published\u0026#34;\u0026gt; {{- .Date.Format (or .Site.Params.dateFormat.published \u0026#34;Jan 02, 2006\u0026#34;) -}} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ if $showReadingTime }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--reading\u0026#34;\u0026gt; {{ T \u0026#34;article.readingTime\u0026#34; .ReadingTime }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;!--ここから--\u0026gt; {{- if ne .Lastmod .Date -}} \u0026lt;div class=\u0026#34;article-time--lastUpdated\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time\u0026gt; {{ T \u0026#34;article.lastUpdatedOn\u0026#34; }} {{ .Lastmod.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{- end -}} \u0026lt;!--ここを追加--\u0026gt; \u0026lt;/footer\u0026gt; 確認 下記のコマンドで起動して確認してみます\n1 hugo server -D 無事作成日時の横に更新日時が表示されました\n","date":"2024-06-14T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/hugo-stack-custom-lastupdated/","title":"Hugoで作成日時の横に更新日時を表示できるようにする"},{"content":"ファイル・フォルダ構成 以下のような構成になっています\n1 2 3 4 5 6 dev ┣━ db-data ┣━ log ┣━ mysql ┃　┗━ my.cnf ┗━ docker-compose.yaml docker-compose.yaml のファイル 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 services: mysql: image: mysql:8.4.0 ports: - \u0026#34;3306:3306\u0026#34; environment: MYSQL_ROOT_PASSWORD: mysql MYSQL_DATABASE: db MYSQL_USER: user MYSQL_PASSWORD: password TZ: \u0026#39;Asia/Tokyo\u0026#39; volumes: - ./db-data:/var/lib/mysql - ./mysql:/etc/mysql/conf.d - ./log:/var/log/mysql phpmyadmin: image: phpmyadmin:5.2.1 depends_on: - mysql environment: - PMA_ARBITRARY=1 - PMA_HOSTS=mysql - PMA_USER=root - PMA_PASSWORD=mysql ports: - \u0026#34;3001:80\u0026#34; volumes: db-data: docker composeで構築したMySQLのログをローカルに保存したい my.cnfファイルに以下の内容を追記する\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [mysqld] log_output=FILE # General Log general_log=1 general_log_file=/var/log/mysql/mysql-query.log # Slow Query Log slow_query_log=1 slow_query_log_file=/var/log/mysql/mysql-slow.log # slow_query_time = 1.0s long_query_time=1.0 log_queries_not_using_indexes=0 # Error Log log_error=/var/log/mysql/mysql-error.log log_error_verbosity=3 docker-compose.yamlに以下の内容を追記する\n1 2 volumes: - ./log:/var/log/mysql ログが書き込まれない パーミッションがありすぎるとログファイルが生成されないので権限を必要最小限にする\n1 sudo chmod -R 775 . 1 2 3 4 5 mao@mao:~/dev$ sudo ls -l ./log total 32 -rw-r----- 1 999 systemd-journal 16239 6月 1 23:33 mysql-error.log -rw-r----- 1 999 systemd-journal 10468 6月 1 23:29 mysql-query.log -rw-r----- 1 999 systemd-journal 180 6月 1 23:29 mysql-slow.log docker-composeのスタートとストップ スタート\n1 sudo docker compose up -d ストップ\n1 sudo docker compose down -v 以下のURLからphpmyadminにアクセスし、少し作業をします\nするとログファイルを作成されます\nhttp://localhost:3001/ ","date":"2024-06-02T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/docker-compose-mysql-log/","title":"docker-composeで構築したMySQLのログをローカルに保存する"},{"content":"発生現象 Deskmini x300(CPU:Ryzen7 5700G) に元々SATA SSDを差していたがデータをM.2 SSDにクローンをして差し替えると、有線LANが繋がらなくなる\n結果、ProxmoxのWebGUIにアクセスできなくなる\n対処方法 WebGUIではなく本体からコマンドで、ブリッジネットワークにリンクしている物理LANを「enp1s0」から「enp2s0」へ変更する\n手順 SATA SSDのとき 元々は500GBのSATA SSDを使用していた ネットワークの設定 M.2 SSDのとき M.2 SSDに差し替えた 元々「enp1s0」だったが「enp2s0」にすると通信可能になった ","date":"2024-05-26T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/proxmox-ssd-lan/","title":"Deskmini x300でSATA SSDからM.2 SSDに差し替えるとProxmoxのWebGUIにアクセスできなくなる"},{"content":"環境 Ubuntu 23.10 Intel Core i5 13500 メモリ64GB 1：setup ソースからビルドするのに必要なソフトを確認する\n1 2 make --version gcc --version インストールされていない場合は以下のコマンドでインストールする\n1 2 sudo apt install make sudo apt install gcc ビルドするのに必要なバージョン\nThe minimum version of Go required depends on the target version of Go:\nGo \u0026lt;= 1.4: a C toolchain. 1.5 \u0026lt;= Go \u0026lt;= 1.19: a Go 1.4 compiler. 1.20 \u0026lt;= Go \u0026lt;= 1.21: a Go 1.17 compiler. 1.22 \u0026lt;= Go \u0026lt;= 1.23: a Go 1.20 compiler. Going forward, Go version 1.N will require a Go 1.M compiler, where M is N-2 rounded down to an even number. Example: Go 1.24 and 1.25 require Go 1.22. 2：build go1.4 go1.4-bootstrapをビルドする\n1 2 3 wget https://dl.google.com/go/go1.4-bootstrap-20171003.tar.gz mkdir go1.4-bootstrap \u0026amp;\u0026amp; tar xzvf go1.4-bootstrap-20171003.tar.gz -C go1.4-bootstrap --strip-components 1 cd ./go1.4-bootstrap/src 1 CGO_ENABLED=0 bash ./make.bash 1 2 Installed Go for linux/amd64 in /home/mao/Desktop/go1.4-bootstrap Installed commands in /home/mao/Desktop/go1.4-bootstrap/bin 3：build go1.17 go1.17をビルドする\n1 2 3 wget https://dl.google.com/go/go1.17.src.tar.gz mkdir go1.17 \u0026amp;\u0026amp; tar xzvf go1.17.src.tar.gz -C go1.17 --strip-components 1 cd ./go1.17/src 1 2 GOROOT_BOOTSTRAP=${PWD}/go1.4-bootstrap bash ./all.bash GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.4-bootstrap bash ./all.bash 1 2 3 4 5 6 7 Go version is \u0026#34;go1.17\u0026#34;, ignoring -next /home/mao/Desktop/go1.17/api/next.txt ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.17 Installed commands in /home/mao/Desktop/go1.17/bin *** You need to add /home/mao/Desktop/go1.17/bin to your PATH. 4：build go1.20 go1.20をビルドする\n1 2 3 wget https://dl.google.com/go/go1.20.src.tar.gz mkdir go1.20 \u0026amp;\u0026amp; tar xzvf go1.20.src.tar.gz -C go1.20 --strip-components 1 cd ./go1.20/src 1 2 /home/mao/Desktop/go1.17/bin GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.17 bash ./all.bash 1 2 3 4 5 ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.20 Installed commands in /home/mao/Desktop/go1.20/bin *** You need to add /home/mao/Desktop/go1.20/bin to your PATH. 5：build go1.22.2 latest go1.22.2をビルドする\n1 2 3 wget https://dl.google.com/go/go1.22.2.src.tar.gz mkdir go1.22.2 \u0026amp;\u0026amp; tar xzvf go1.22.2.src.tar.gz -C go1.22.2 --strip-components 1 cd ./go1.22.2/src 1 2 /home/mao/Desktop/go1.20/bin GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.20 bash ./all.bash 1 2 3 4 5 ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.22.2 Installed commands in /home/mao/Desktop/go1.22.2/bin *** You need to add /home/mao/Desktop/go1.22.2/bin to your PATH. 6：Pathを通す パスを通してバージョンを確認する\n1 sudo cp -rp ./go1.22.2 /usr/local/ .bashrc\n1 2 export PATH=$PATH:/usr/local/go/bin export PATH=$PATH:/usr/local/go1.22.2/bin 1 source ~/.bashrc 1 go version 1 go version go1.22.2 linux/amd64 参考URL https://go.dev/doc/install/source https://qiita.com/myoshimi/items/5d1f6a2ee8a849bac7eb https://qiita.com/soarflat/items/d5015bec37f8a8254380 ","date":"2024-05-25T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/golang-source-build/","title":"Go言語をソースコードからビルドする"}]