[{"content":"Ansibleの拡張機能のインストール VScodeの拡張機能\u0026quot;redhat.ansible\u0026quot;があるのでインストールしておくと便利になる Ansibleのインストール 参考URL\nhttps://docs.ansible.com/ansible/latest/installation_guide/installation_distros.html#installing-ansible-on-ubuntu 1 2 3 sudo apt install software-properties-common sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible バージョンの確認\n1 2 ansible --version which ansible ファイル構造を見れるようにtreeをインストール 1 sudo apt install tree アドホック・コマンド フォルダの構造\n1 2 3 4 5 6 7 8 mao@ansible-server:~/ansible-test$ tree . ├── ansible-ssh ├── ansible.cfg └── host.yaml 1 directory, 3 files mao@ansible-server:~/ansible-test$ ファイルの中身 ansible-ssh\n実行されてsshで接続されると生成されるファイル ansible.cfg（設定ファイル）\n1 2 3 [defaults] # fingerprintを検証しない設定 host_key_checking = False host.yaml（インベントリーファイル）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # YAML は ”---\u0026#34; から開始する --- # \u0026#34;all\u0026#34; グループの宣言 all: # \u0026#34;all\u0026#34; グループに含まれるホストに関する情報を定義する宣言 hosts: # 管理対象ノードの情報を定義する宣言 ansible-test-server: ansible_host: 192.168.10.10 ansible_user: mao ansible_password: mao ansible_ssh_private_key_file: /home/mao/ansible-test/ansible-ssh #ansible_python_interpreter: Python インタープリターの path ansible_port: 22 実行 下記コマンドを実行すると、それぞれのサーバーのストレージ容量を確認できる\n1 2 ansible all -i host.yaml -m ansible.builtin.command -a \u0026#34;df -h\u0026#34; ansible all -i hosts.yml -a \u0026#34;df -h\u0026#34; \u0026ldquo;-i\u0026quot;はインベントリーファイルの指定 \u0026ldquo;-m ansible.builtin.command\u0026quot;は短縮可能 実行結果\n1 2 3 4 5 6 7 8 9 10 mao@ansible-server:~/ansible-test$ ansible all -i host.yaml -m ansible.builtin.command -a \u0026#34;df -h\u0026#34; ansible-test-server | CHANGED | rc=0 \u0026gt;\u0026gt; Filesystem Size Used Avail Use% Mounted on tmpfs 795M 692K 794M 1% /run /dev/mapper/ubuntu--vg-ubuntu--lv 8.1G 2.5G 5.2G 32% / tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock /dev/sda2 1.7G 181M 1.5G 12% /boot tmpfs 795M 12K 795M 1% /run/user/1000 mao@ansible-server:~/ansible-test$ Ansible-playbookの実行 参考URL\nhttps://docs.ansible.com/ansible/latest/collections/index.html ファイル構造 1 2 3 4 5 6 7 8 9 mao@ansible-server:~/ansible-test$ tree . ├── ansible-ssh ├── ansible.cfg ├── host.yaml └── nginx_playbook.yaml 1 directory, 4 files mao@ansible-server:~/ansible-test$ \u0026ldquo;nginx_playbook.yaml\u0026quot;の中身\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # Ansible-playbook - name: setup nginx server hosts: - all become: yes tasks: - name: Install ansible.builtin.command: apt install -y nginx - name: Start ansible.builtin.service: name: nginx state: started - name: Enable nginx ansible.builtin.service: name: nginx #enabled: yes enabled: no #- name: nginx version #ansible.builtin.command: nginx -v #- name: status #ansible.builtin.command: systemctl status nginx 実行 nginxがインストールされます\n1 ansible-playbook -i host.yaml nginx_playbook.yaml --ask-become-pass \u0026ldquo;\u0026ndash;ask-become-pass\u0026rdquo;:sudoパスワードを求めるオプション 実行結果\n実行時にパスワードを求められるので入力します 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mao@ansible-server:~/ansible-test$ ansible-playbook -i host.yaml nginx_playbook.yaml --ask-become-pass BECOME password: PLAY [setup nginx server] ******************************************************************** TASK [Gathering Facts] *********************************************************************** ok: [ansible-test-server] TASK [Install] ******************************************************************************* changed: [ansible-test-server] TASK [Start] ********************************************************************************* ok: [ansible-test-server] TASK [Enable nginx] ************************************************************************** ok: [ansible-test-server] PLAY RECAP *********************************************************************************** ansible-test-server : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 mao@ansible-server:~/ansible-test$ 無事nginxが起動しています\n","date":"2024-07-15T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/ansible/","title":"Ansibleを少し触ってみた"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 HAProxyをセットアップする 1 2 sudo apt update sudo apt upgrade IPアドレスを固定する 99-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 network: version: 2 renderer: networkd ethernets: ens18: dhcp4: false addresses: - 192.168.10.45/24 routes: - to: default via: 192.168.10.1 nameservers: search: [] addresses: [192.168.10.1] ファイルを反映する\n1 2 sudo cp 99-config.yaml /etc/netplan/ sudo netplan apply 1 sudo chmod 600 /etc/netplan/99-config.yaml HAProxyをインストールする 1 sudo apt install haproxy 1 2 sudo mv /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.default sudo nano /etc/haproxy/haproxy.cfg 下記の通りに編集する\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 defaults timeout connect 10s timeout client 30s timeout server 30s frontend k8s bind *:6443 mode tcp #option tcplog default_backend k8s_backend backend k8s_backend balance roundrobin server k8s-control-plane-01 192.168.10.41:6443 check server k8s-control-plane-02 192.168.10.44:6443 check #server k8s-control-plane-01 \u0026lt;control node2のip\u0026gt;:6443 check 反映する\n1 sudo systemctl enable --now haproxy HA構成のクラスタを構築する Control-Plane-01で下記コマンドを実行する 1 sudo kubeadm init --control-plane-endpoint=192.168.10.45:6443 --pod-network-cidr=10.128.0.0/16 --upload-certs \u0026ldquo;\u0026ndash;control-plane-endpoint=\u0026lt;IPアドレス\u0026gt;:6443\u0026rdquo;:Control-PlaneのIPアドレスとAPIサーバーのポートを指定する \u0026ldquo;\u0026ndash;pod-network-cidr=10.128.0.0/16\u0026rdquo;:Pod間ネットワークの指定する \u0026ldquo;\u0026ndash;upload-certs\u0026rdquo;:Control-Plane、Worker-Node間で共有する証明書を暗号化する 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 mao@k8s-control-plane-01:~$ sudo kubeadm init --control-plane-endpoint=192.168.10.45:6443 --pod-network-cidr=10.128.0.0/16 --upload-certs [init] Using Kubernetes version: v1.30.2 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-control-plane-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.41 192.168.10.45] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;super-admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34; [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 501.677267ms [api-check] Waiting for a healthy API server. This can take up to 4m0s [api-check] The API server is healthy after 5.012535655s [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Storing the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [upload-certs] Using certificate key: 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: h3eh17.f1j6k27nzi34hd0w [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 \\ --control-plane --certificate-key 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 mao@k8s-control-plane-01:~$ kubeadmコマンドを実行できるようにする 上記の実行結果に記載のある通り下記のコマンドを実行する\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config クラスとにJoinするために必要な情報をメモ（記録）しておく 上記の実行結果に記載のある通りクラスタにJoinする際に必要なコマンドをメモしておく\nControl-PlaneがクラスタにJoinするために必要な情報\n1 2 3 kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 \\ --control-plane --certificate-key 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 Worker-NodeがクラスタにJoinするために必要な情報\n1 2 kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w \\ --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 他のControl-PlaneをクラスタにJoinする 下記コマンドを実行する\n1 sudo kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 --control-plane --certificate-key 46c4727ea4da0877fd6e152f0c5d4842837ea949909e9ccb83a5c2f7331f53a9 Woker-NodeをクラスタにJoinする Worker-NodeをクラスタにJoinさせる、1つ目以外のWoker-Nodeも同じようにクラスタに参加させる\n1 sudo kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mao@k8s-worker-01:~$ sudo kubeadm join 192.168.10.45:6443 --token h3eh17.f1j6k27nzi34hd0w --discovery-token-ca-cert-hash sha256:6ee64840cf218d5f9ee05a5138e0f543bf6b2359f51ab3d13fde7405370ba7a7 [sudo] password for mao: [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 500.998945ms [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. mao@k8s-worker-01:~$ HA構成の確認のためにControl-Planeを1つダウンさせてみる Control-Planeを1つシャットダウンさせて擬似的にダウンさせてみる\n1 2 3 4 5 6 7 8 mao@k8s-control-plane-03:~$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 146m v1.30.2 k8s-control-plane-02 NotReady control-plane 140m v1.30.2 k8s-control-plane-03 Ready control-plane 10m v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 136m v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 127m v1.30.2 mao@k8s-control-plane-03:~$ 確認用のコマンドが問題なく実行され、Control-Planeが1つ\u0026quot;NotReady\u0026quot;になっているが、問題なくクラスタが稼働している\n参考 IPアドレスの構成 192.168.10.45,HAProxy 192.168.10.41,Control-Plane-01 192.168.10.44,Control-Plane-02 192.168.10.46,Control-Plane-03 192.168.10.42,Worker-Node-01 192.168.10.43,Worker-Node-02 参考URL https://kubernetes.io/ja/docs/setup/#production-environment https://kubernetes.io/ja/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/ https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 感想 以上でHA構成のkubernetesの構築は完了しました\n公式ドキュメントを参考にしつつ進めてわからないところは検索したりして構築できました\nネットワーク部分に関しては構築前よりも詳しくなったと思います\n発展として、他にも\u0026quot;keepalive\u0026quot;や\u0026quot;kube-vip\u0026quot;を使用した構成もあるみたいなので、いずれ試してみようと思います\nあとkubernetes上に仮想マシンを構成する\u0026quot;kubevirt\u0026quot;や\u0026quot;openstack on kubernetes\u0026quot;を構築してみようと思います\n","date":"2024-07-15T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-05/","title":"kubernetesをproxmox上に立ててみた（5）/HA構成"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 Worker-Nodeをクラスターから外す 先にControl-Planeですること 先にControl-Plane上でクラスターから外す準備をする\n1 2 kubectl drain k8s-worker-01 --ignore-daemonsets --delete-emptydir-data --force kubectl get node 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 mao@k8s-control-plane-01:~$ kubectl drain k8s-worker-01 --ignore-daemonsets --delete-emptydir-data --force node/k8s-worker-01 cordoned Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-h75vn, calico-system/csi-node-driver-5b5k4, kube-system/kube-proxy-wzvtf, metallb-system/speaker-xwb66 evicting pod metallb-system/controller-86f5578878-dz5zm pod/controller-86f5578878-dz5zm evicted node/k8s-worker-01 drained mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 3d23h v1.30.2 k8s-worker-01 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 3d22h v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 3d14h v1.30.2 mao@k8s-control-plane-01:~$ STATUSが\u0026quot;SchedulingDisabled\u0026quot;になったらOK\nWorker-Nodeでの作業 Control-Planeでの作業ができたら、次はWorker-Nodeで作業をする\n1 sudo kubeadm reset 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mao@k8s-worker-01:~$ sudo kubeadm reset W0630 01:12:34.493241 63962 preflight.go:56] [reset] WARNING: Changes made to this host by \u0026#39;kubeadm init\u0026#39; or \u0026#39;kubeadm join\u0026#39; will be reverted. [reset] Are you sure you want to proceed? [y/N]: y [preflight] Running pre-flight checks W0630 01:12:36.338665 63962 removeetcdmember.go:106] [reset] No kubeadm config, using etcd pod spec to get data directory [reset] Deleted contents of the etcd data directory: /var/lib/etcd [reset] Stopping the kubelet service [reset] Unmounting mounted directories in \u0026#34;/var/lib/kubelet\u0026#34; [reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki] [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf] The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually by using the \u0026#34;iptables\u0026#34; command. If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar) to reset your system\u0026#39;s IPVS tables. The reset process does not clean your kubeconfig files and you must remove them manually. Please, check the contents of the $HOME/.kube/config file. Worker-Node上のcalicoを削除する 削除するネットワークインターフェースを確認する\n1 ip link 実行結果\n1 2 3 4 5 6 7 8 9 mao@k8s-worker-01:~$ ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:fe:35:a0 brd ff:ff:ff:ff:ff:ff altname enp0s18 10: vxlan.calico: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 66:bb:bb:d8:11:bf brd ff:ff:ff:ff:ff:ff mao@k8s-worker-01:~$ \u0026ldquo;vxlan.calico\u0026quot;を削除する\n1 sudo ip link delete vxlan.calico 最後にControl-PlaneからWorker-Nodeを削除する Control-Plane上で下記コマンドを実行する\n1 kubectl delete node k8s-worker-01 実行結果\n1 2 3 mao@k8s-control-plane-01:~$ kubectl delete node k8s-worker-01 node \u0026#34;k8s-worker-01\u0026#34; deleted mao@k8s-control-plane-01:~$ 削除されたことを確認する\n1 kubectl get node 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 3d23h v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 3d14h v1.30.2 mao@k8s-control-plane-01:~$ Control-Planeをクラスターから外す、クラスターを削除する Control-Plane上で、下記のコマンドを順に実行してクラスタをリセットする\n1 2 3 4 5 6 7 sudo kubeadm reset sudo rm -rf $HOME/.kube sudo systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet sudo systemctl restart containerd sudo ip link sudo ip link delete vxlan.calico sudo ip link 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 mao@k8s-control-plane-01:~$ sudo kubeadm reset [reset] Reading configuration from the cluster... [reset] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; W0630 03:05:22.877860 124743 preflight.go:56] [reset] WARNING: Changes made to this host by \u0026#39;kubeadm init\u0026#39; or \u0026#39;kubeadm join\u0026#39; will be reverted. [reset] Are you sure you want to proceed? [y/N]: y [preflight] Running pre-flight checks [reset] Deleted contents of the etcd data directory: /var/lib/etcd [reset] Stopping the kubelet service [reset] Unmounting mounted directories in \u0026#34;/var/lib/kubelet\u0026#34; W0630 03:05:30.442255 124743 cleanupnode.go:106] [reset] Failed to remove containers: [failed to stop running pod 67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424: output: E0630 03:05:29.277302 126592 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;67f96751dedca872f742388cb86d92f0388de2eef490d813d380e706cb8c7424\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4: output: E0630 03:05:29.422959 126734 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;9b4a0f41c7cafc48e51a68e89a0e9ad265fee646ef148a7ade366f90be956ee4\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12: output: E0630 03:05:29.566837 126875 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;976d40faccfc6491f13fc990ae3c94ee0348faa04ab74968d05b5c5904c75e12\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5: output: E0630 03:05:29.713590 127015 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;28a5274b17af2a1a78e80d18c48a8732826f7a020fef31901eec76fcaec91fc5\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976: output: E0630 03:05:29.858624 127156 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\u0026#34; time=\u0026#34;2024-06-30T03:05:29Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;2a4ddac4e1d2d11184fb82e8283250c0756a73c23fe053d12edf9a4f037eb976\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1, failed to stop running pod 70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112: output: E0630 03:05:30.010265 127297 remote_runtime.go:222] \u0026#34;StopPodSandbox from runtime service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; podSandboxID=\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\u0026#34; time=\u0026#34;2024-06-30T03:05:30Z\u0026#34; level=fatal msg=\u0026#34;stopping the pod sandbox \\\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\\\u0026#34;: rpc error: code = Unknown desc = failed to destroy network for sandbox \\\u0026#34;70174a13942604fd21e17d0425064f463e35c347067301b9051206c088123112\\\u0026#34;: plugin type=\\\u0026#34;calico\\\u0026#34; failed (delete): error getting ClusterInformation: Get \\\u0026#34;https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default\\\u0026#34;: dial tcp 10.96.0.1:443: connect: connection refused\u0026#34; : exit status 1] [reset] Deleting contents of directories: [/etc/kubernetes/manifests /var/lib/kubelet /etc/kubernetes/pki] [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf] The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually by using the \u0026#34;iptables\u0026#34; command. If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar) to reset your system\u0026#39;s IPVS tables. The reset process does not clean your kubeconfig files and you must remove them manually. Please, check the contents of the $HOME/.kube/config file. mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ sudo systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart kubelet ==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ==== Authentication is required to restart \u0026#39;kubelet.service\u0026#39;. Authenticating as: mao Password: ==== AUTHENTICATION COMPLETE ==== mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 8 9 mao@k8s-control-plane-01:~$ sudo ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:7f:08:e4 brd ff:ff:ff:ff:ff:ff altname enp0s18 9: vxlan.calico: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 66:ca:40:c1:cc:6f brd ff:ff:ff:ff:ff:ff mao@k8s-control-plane-01:~$ 1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ sudo ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:7f:08:e4 brd ff:ff:ff:ff:ff:ff altname enp0s18 mao@k8s-control-plane-01:~$ これでクラスターをリセットができました\n","date":"2024-07-10T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-04/","title":"kubernetesをproxmox上に立ててみた（4）/クラスターを壊す"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 LoadBalancer（MetalLB）の設定をする kubernetesのクラスターの外部からIPアドレスでアクセスするための設定をする ロードバランサー（MetalLB）を使用して外部からアクセスできるようにする マニフェストファイルの\u0026quot;type\u0026quot;に\u0026quot;LoadBalancer\u0026quot;を指定できるようになる MetalLB を実行する 今回はMetalLBを使用する\nhttps://metallb.universe.tf/installation/ ARPの設定をする 1 1 2 3 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl diff -f - -n kube-system 2 1 2 3 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system 実行結果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mao@k8s-control-plane-01:~$ kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl diff -f - -n kube-system diff -u -N /tmp/LIVE-1007559117/v1.ConfigMap.kube-system.kube-proxy /tmp/MERGED-3311346621/v1.ConfigMap.kube-system.kube-proxy --- /tmp/LIVE-1007559117/v1.ConfigMap.kube-system.kube-proxy 2024-06-26 12:33:11.717730136 +0000 +++ /tmp/MERGED-3311346621/v1.ConfigMap.kube-system.kube-proxy 2024-06-26 12:33:11.718730159 +0000 @@ -37,7 +37,7 @@ excludeCIDRs: null minSyncPeriod: 0s scheduler: \u0026#34;\u0026#34; - strictARP: false + strictARP: true syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s mao@k8s-control-plane-01:~$ kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system Warning: resource configmaps/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. configmap/kube-proxy configured mao@k8s-control-plane-01:~$ MetalLBの実行をする 参考URL\nhttps://blog.ntnx.jp/entry/2024/02/14/025309 1 2 wget https://raw.githubusercontent.com/metallb/metallb/v0.14.5/config/manifests/metallb-native.yaml kubectl apply -f metallb-native.yaml 確認をする\n1 kubectl get pod -n metallb-system 実行結果\n1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ kubectl get pod -n metallb-system NAME READY STATUS RESTARTS AGE controller-86f5578878-dz5zm 1/1 Running 0 43s speaker-mbvpk 1/1 Running 0 43s speaker-rx75f 1/1 Running 0 43s speaker-xwb66 1/1 Running 0 43s mao@k8s-control-plane-01:~$ 払い出せるIPアドレスの範囲を設定する\n下記の部分で範囲を設定する 1 2 3 spec: addresses: - 192.168.10.55-192.168.10.60 1 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt; EOF \u0026gt; ippool.yaml apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: default namespace: metallb-system spec: addresses: - 192.168.10.55-192.168.10.60 EOF 2 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt; EOF \u0026gt; l2adv.yaml apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: default namespace: metallb-system spec: ipAddressPools: - default EOF 設定を実行する\n1 kubectl apply -f ippool.yaml -f l2adv.yaml 実行結果\n1 2 3 mao@k8s-control-plane-01:~$ kubectl apply -f ippool.yaml -f l2adv.yaml ipaddresspool.metallb.io/default created l2advertisement.metallb.io/default created マニフェストファイルを実行後、外部IPアドレスが設定されているか確認する マニフェストファイル\nnginx-lb.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 10 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.27 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-deployment-lb spec: type: LoadBalancer #type: ClusterIP ports: - port: 80 targetPort: 80 selector: app: nginx \u0026ldquo;type: LoadBalancer\u0026quot;にするとLoadBalancerからIPアドレスが払い出されてクラスタ外からアクセスできるようになる マニフェストファイルを実行してデプロイする\n1 kubectl apply -f nginx-lb.yaml 確認するためのコマンド\n1 2 3 kubectl get svc or kubectl get service 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 25h nginx-deployment-lb LoadBalancer 10.98.208.100 192.168.10.45 80:32762/TCP 58s mao@k8s-control-plane-01:~$ \u0026ldquo;nginx-deployment-lb\u0026quot;の\u0026quot;TYPE\u0026quot;に\u0026quot;LoadBalancer\u0026quot;が指定されており\u0026quot;EXTERNAL-IP\u0026rdquo;（外部IPアドレス）が割り当てられている\nこのIPアドレスにアクセスすると実際のコンテナにアクセスできる\n","date":"2024-07-07T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-03/","title":"kubernetesをproxmox上に立ててみた（3）/LoadBalancerの設定"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 Control-Plane（Master-Node）の設定をする kubeadm init を実行する Control-Planeにするマシン上で実行する\n1 2 sudo kubeadm init --apiserver-advertise-address=Control-PlaneのIPアドレス --pod-network-cidr=10.128.0.0/16 sudo kubeadm init --apiserver-advertise-address=192.168.10.41 --pod-network-cidr=10.128.0.0/16 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 mao@k8s-control-plane-01:~$ sudo kubeadm init --apiserver-advertise-address=192.168.10.41 --pod-network-cidr=10.128.0.0/16 [init] Using Kubernetes version: v1.30.2 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-control-plane-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.41] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;super-admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34; [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 500.625027ms [api-check] Waiting for a healthy API server. This can take up to 4m0s [api-check] The API server is healthy after 3.50086825s [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: evbpii.dp8y5hcfqkv9jn4n [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n \\ --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 mao@k8s-control-plane-01:~$ 実行結果にも表示されている通り、下記のコマンドを実行する\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Calicoを実行する（Pod間ネットワーク） 参考URL\nhttps://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart tigera-operator.yaml\n1 2 wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml kubectl create -f tigera-operator.yaml custom-resources.yaml\n1 wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml ダウンロードしたcustom-resources.yamlを編集する\nkubeadm init で指定した引数\u0026ndash;pod-network-cidrと同じものへ変更する 1 2 - cidr: 192.168.0.0/16 + cidr: 10.128.0.0/16 実行する\n1 kubectl apply -f custom-resources.yaml Nodeを確認する 1 kubectl get nodes -o wide 実行結果\n1 2 3 4 mao@k8s-control-plane-01:~$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-control-plane-01 Ready control-plane 32m v1.30.2 192.168.10.41 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-35-generic containerd://1.7.18 mao@k8s-control-plane-01:~$ Worker-Nodeの設定をする Worker-NodeをJoinする Control-Planeでkubeadm initを実行した際に表示されたコマンドを実行する\n1 sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mao@k8s-worker-01:~$ sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-to ken-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 501.168161ms [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. mao@k8s-worker-01:~$ Worker-NodeがJoinされているか、Control-Planeで確認する\n1 kubectl get node 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 63m v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 36s v1.30.2 mao@k8s-control-plane-01:~$ 複数のWorker-NodeをJoinする 基本手順は1つ目のWorker-Nodeと同じように実行するとJoinできる\n1 sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 実行したらControl-PlanでNodeを確認してみる\n1 2 3 4 5 6 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 9h v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 8h v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 83s v1.30.2 mao@k8s-control-plane-01:~$ 無事にWorker-NodeがJoinされている\n","date":"2024-07-06T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-02/","title":"kubernetesをproxmox上に立ててみた（2）/Control-Plane・Worker-Nodeの設定"},{"content":" k8sを勉強してみようと思い、デプロイ等の操作は書籍で触っていたら構築もしてみたくなったので、Proxmox上に仮想マシンを作成してk8sを構築してみた\n開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Kubernetes v1.30.2 構成 Proxmox上に以下6つの仮想マシンを立てました\nhaproxy-01 control-plane-node-01 control-plane-node-02 control-plane-node-03 worker-node-01 worker-node-02 Control-Plane（Master-Node）とWorker-Node両方で実行する setup パッケージの更新をする\n1 2 sudo apt update sudo apt upgrade 必要なソフトウェアをインストールする\n1 sudo apt install nano 公式の手順にそって実行する\nhttps://kubernetes.io/ja/docs/setup/production-environment/ Swapをオフにする swapを止めます\n1 sudo swapoff -a 設定ファイルを書き換えて永続的にswapをオフにする\n1 sudo nano /etc/fstab 編集内容\n1 2 - /swap.img none swap sw 0 0 + #/swap.img none swap sw 0 0 swapがオフになっているか確認する\n1 free -h 実行結果\n1 2 3 4 mao@k8s-control-plane-01:~$ free -h total used free shared buff/cache available Mem: 7.8Gi 510Mi 7.2Gi 704Ki 248Mi 7.3Gi Swap: 0B 0B 0B IPアドレスを固定IPアドレスにする ネットワークのデバイスを確認します\n1 ip address 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mao@k8s-control-plane-01:~$ ip address 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether bc:24:11:7f:08:e4 brd ff:ff:ff:ff:ff:ff altname enp0s18 inet 192.168.10.10/24 metric 100 brd 192.168.10.255 scope global dynamic ens18 valid_lft 85953sec preferred_lft 85953sec inet6 fe80::be24:11ff:fe7f:8e4/64 scope link valid_lft forever preferred_lft forever mao@k8s-control-plane-01:~$ netplanファイルを作成します\nファイル名：99-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 network: version: 2 renderer: networkd ethernets: ens18: dhcp4: false addresses: - 192.168.10.41/24 routes: - to: default via: 192.168.10.1 nameservers: search: [] addresses: [192.168.10.1] netplanファイルをコピーして適用します\nssh等で接続している場合は、IPアドレスが変わるので接続が切れます、\n再度固定にしたIPアドレスに変更すれば接続できます 1 2 3 sudo cp 99-config.yaml /etc/netplan/ sudo netplan apply sudo chmod 600 /etc/netplan/99-config.yaml containerdをインストールする 公式手順に従ってインストール\nhttps://github.com/containerd/containerd/blob/main/docs/getting-started.md Option 1: From the official binaries containerd 1.7.18 順にコマンドを実行する\n1 2 wget https://github.com/containerd/containerd/releases/download/v1.7.18/containerd-1.7.18-linux-amd64.tar.gz sudo tar Cxzvf /usr/local containerd-1.7.18-linux-amd64.tar.gz 1 sudo wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -O /etc/systemd/system/containerd.service systemctlをリロードし、containerdを有効にする\n1 2 sudo systemctl daemon-reload sudo systemctl enable --now containerd runCをインストールする 1 sudo wget https://github.com/opencontainers/runc/releases/download/v1.1.13/runc.amd64 1 sudo install -m 755 runc.amd64 /usr/local/sbin/runc CNI(Container Network Interface) pluginをインストールする 1 sudo wget https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz 1 2 sudo mkdir -p /opt/cni/bin sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.5.1.tgz IPv4フォワーディングの設定をする 以下のコマンドを順に実行する\n1 1 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf 実行結果\n1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf \u0026gt; overlay \u0026gt; br_netfilter \u0026gt; EOF overlay br_netfilter mao@k8s-control-plane-01:~$ 1 2 sudo modprobe overlay sudo modprobe br_netfilter 2 1 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf 実行結果\n1 2 3 4 5 6 7 8 9 mao@k8s-control-plane-01:~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf \u0026gt; net.bridge.bridge-nf-call-iptables = 1 \u0026gt; net.bridge.bridge-nf-call-ip6tables = 1 \u0026gt; net.ipv4.ip_forward = 1 \u0026gt; EOF net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 mao@k8s-control-plane-01:~$ 1 sudo sysctl --system 3 1 2 lsmod | grep br_netfilter lsmod | grep overlay 実行結果\n1 2 3 4 5 6 mao@k8s-control-plane-01:~$ lsmod | grep br_netfilter br_netfilter 32768 0 bridge 421888 1 br_netfilter mao@k8s-control-plane-01:~$ lsmod | grep overlay overlay 212992 0 mao@k8s-control-plane-01:~$ 4 1 sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 mao@k8s-control-plane-01:~$ systemd cgroup の設定をする 参考URL\nhttps://kubernetes.io/ja/docs/concepts/architecture/cgroups/ https://kubernetes.io/ja/docs/concepts/architecture/cgroups/#check-cgroup-version https://sogo.dev/posts/2022/12/kubernetes-ubuntu22.04-cgroup-systemd 1 stat -fc %T /sys/fs/cgroup/ cgroup v2では、\u0026ldquo;cgroup2fs\u0026quot;と出力されます。 cgroup v1では、\u0026ldquo;tmpfs\u0026quot;と出力されます。 ディレクトリを作成する\n1 sudo mkdir /etc/containerd 以下のコマンドで、デフォルトのコンフィグを作成できます。\n1 sudo containerd config default | sudo tee /etc/containerd/config.toml 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 mao@k8s-control-plane-01:~$ sudo containerd config default | sudo tee /etc/containerd/config.toml disabled_plugins = [] imports = [] oom_score = 0 plugin_dir = \u0026#34;\u0026#34; required_plugins = [] root = \u0026#34;/var/lib/containerd\u0026#34; state = \u0026#34;/run/containerd\u0026#34; temp = \u0026#34;\u0026#34; version = 2 [cgroup] path = \u0026#34;\u0026#34; [debug] address = \u0026#34;\u0026#34; format = \u0026#34;\u0026#34; gid = 0 level = \u0026#34;\u0026#34; uid = 0 [grpc] address = \u0026#34;/run/containerd/containerd.sock\u0026#34; gid = 0 max_recv_message_size = 16777216 max_send_message_size = 16777216 tcp_address = \u0026#34;\u0026#34; tcp_tls_ca = \u0026#34;\u0026#34; tcp_tls_cert = \u0026#34;\u0026#34; tcp_tls_key = \u0026#34;\u0026#34; uid = 0 [metrics] address = \u0026#34;\u0026#34; grpc_histogram = false [plugins] [plugins.\u0026#34;io.containerd.gc.v1.scheduler\u0026#34;] deletion_threshold = 0 mutation_threshold = 100 pause_threshold = 0.02 schedule_delay = \u0026#34;0s\u0026#34; startup_delay = \u0026#34;100ms\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] cdi_spec_dirs = [\u0026#34;/etc/cdi\u0026#34;, \u0026#34;/var/run/cdi\u0026#34;] device_ownership_from_security_context = false disable_apparmor = false disable_cgroup = false disable_hugetlb_controller = true disable_proc_mount = false disable_tcp_service = true drain_exec_sync_io_timeout = \u0026#34;0s\u0026#34; enable_cdi = false enable_selinux = false enable_tls_streaming = false enable_unprivileged_icmp = false enable_unprivileged_ports = false ignore_deprecation_warnings = [] ignore_image_defined_volumes = false image_pull_progress_timeout = \u0026#34;5m0s\u0026#34; image_pull_with_sync_fs = false max_concurrent_downloads = 3 max_container_log_line_size = 16384 netns_mounts_under_state_dir = false restrict_oom_score_adj = false sandbox_image = \u0026#34;registry.k8s.io/pause:3.8\u0026#34; selinux_category_range = 1024 stats_collect_period = 10 stream_idle_timeout = \u0026#34;4h0m0s\u0026#34; stream_server_address = \u0026#34;127.0.0.1\u0026#34; stream_server_port = \u0026#34;0\u0026#34; systemd_cgroup = false tolerate_missing_hugetlb_controller = true unset_seccomp_profile = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.cni] bin_dir = \u0026#34;/opt/cni/bin\u0026#34; conf_dir = \u0026#34;/etc/cni/net.d\u0026#34; conf_template = \u0026#34;\u0026#34; ip_pref = \u0026#34;\u0026#34; max_conf_num = 1 setup_serially = false [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd] default_runtime_name = \u0026#34;runc\u0026#34; disable_snapshot_annotations = true discard_unpacked_layers = false ignore_blockio_not_enabled_errors = false ignore_rdt_not_enabled_errors = false no_pivot = false snapshotter = \u0026#34;overlayfs\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;\u0026#34; sandbox_mode = \u0026#34;\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime.options] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; sandbox_mode = \u0026#34;podsandbox\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] BinaryName = \u0026#34;\u0026#34; CriuImagePath = \u0026#34;\u0026#34; CriuPath = \u0026#34;\u0026#34; CriuWorkPath = \u0026#34;\u0026#34; IoGid = 0 IoUid = 0 NoNewKeyring = false NoPivotRoot = false Root = \u0026#34;\u0026#34; ShimCgroup = \u0026#34;\u0026#34; SystemdCgroup = false [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.untrusted_workload_runtime] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;\u0026#34; sandbox_mode = \u0026#34;\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.untrusted_workload_runtime.options] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.image_decryption] key_model = \u0026#34;node\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.auths] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.headers] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.x509_key_pair_streaming] tls_cert_file = \u0026#34;\u0026#34; tls_key_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.opt\u0026#34;] path = \u0026#34;/opt/containerd\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.restart\u0026#34;] interval = \u0026#34;10s\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.tracing\u0026#34;] [plugins.\u0026#34;io.containerd.metadata.v1.bolt\u0026#34;] content_sharing_policy = \u0026#34;shared\u0026#34; [plugins.\u0026#34;io.containerd.monitor.v1.cgroups\u0026#34;] no_prometheus = false [plugins.\u0026#34;io.containerd.nri.v1.nri\u0026#34;] disable = true disable_connections = false plugin_config_path = \u0026#34;/etc/nri/conf.d\u0026#34; plugin_path = \u0026#34;/opt/nri/plugins\u0026#34; plugin_registration_timeout = \u0026#34;5s\u0026#34; plugin_request_timeout = \u0026#34;2s\u0026#34; socket_path = \u0026#34;/var/run/nri/nri.sock\u0026#34; [plugins.\u0026#34;io.containerd.runtime.v1.linux\u0026#34;] no_shim = false runtime = \u0026#34;runc\u0026#34; runtime_root = \u0026#34;\u0026#34; shim = \u0026#34;containerd-shim\u0026#34; shim_debug = false [plugins.\u0026#34;io.containerd.runtime.v2.task\u0026#34;] platforms = [\u0026#34;linux/amd64\u0026#34;] sched_core = false [plugins.\u0026#34;io.containerd.service.v1.diff-service\u0026#34;] default = [\u0026#34;walking\u0026#34;] [plugins.\u0026#34;io.containerd.service.v1.tasks-service\u0026#34;] blockio_config_file = \u0026#34;\u0026#34; rdt_config_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.aufs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.blockfile\u0026#34;] fs_type = \u0026#34;\u0026#34; mount_options = [] root_path = \u0026#34;\u0026#34; scratch_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.btrfs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.devmapper\u0026#34;] async_remove = false base_image_size = \u0026#34;\u0026#34; discard_blocks = false fs_options = \u0026#34;\u0026#34; fs_type = \u0026#34;\u0026#34; pool_name = \u0026#34;\u0026#34; root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.native\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.overlayfs\u0026#34;] mount_options = [] root_path = \u0026#34;\u0026#34; sync_remove = false upperdir_label = false [plugins.\u0026#34;io.containerd.snapshotter.v1.zfs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.tracing.processor.v1.otlp\u0026#34;] [plugins.\u0026#34;io.containerd.transfer.v1.local\u0026#34;] config_path = \u0026#34;\u0026#34; max_concurrent_downloads = 3 max_concurrent_uploaded_layers = 3 [[plugins.\u0026#34;io.containerd.transfer.v1.local\u0026#34;.unpack_config]] differ = \u0026#34;\u0026#34; platform = \u0026#34;linux/amd64\u0026#34; snapshotter = \u0026#34;overlayfs\u0026#34; [proxy_plugins] [stream_processors] [stream_processors.\u0026#34;io.containerd.ocicrypt.decoder.v1.tar\u0026#34;] accepts = [\u0026#34;application/vnd.oci.image.layer.v1.tar+encrypted\u0026#34;] args = [\u0026#34;--decryption-keys-path\u0026#34;, \u0026#34;/etc/containerd/ocicrypt/keys\u0026#34;] env = [\u0026#34;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\u0026#34;] path = \u0026#34;ctd-decoder\u0026#34; returns = \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34; [stream_processors.\u0026#34;io.containerd.ocicrypt.decoder.v1.tar.gzip\u0026#34;] accepts = [\u0026#34;application/vnd.oci.image.layer.v1.tar+gzip+encrypted\u0026#34;] args = [\u0026#34;--decryption-keys-path\u0026#34;, \u0026#34;/etc/containerd/ocicrypt/keys\u0026#34;] env = [\u0026#34;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\u0026#34;] path = \u0026#34;ctd-decoder\u0026#34; returns = \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34; [timeouts] \u0026#34;io.containerd.timeout.bolt.open\u0026#34; = \u0026#34;0s\u0026#34; \u0026#34;io.containerd.timeout.metrics.shimstats\u0026#34; = \u0026#34;2s\u0026#34; \u0026#34;io.containerd.timeout.shim.cleanup\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.load\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.shutdown\u0026#34; = \u0026#34;3s\u0026#34; \u0026#34;io.containerd.timeout.task.state\u0026#34; = \u0026#34;2s\u0026#34; [ttrpc] address = \u0026#34;\u0026#34; gid = 0 uid = 0 mao@k8s-control-plane-01:~$ 設定ファイルを編集する\n1 sudo nano /etc/containerd/config.toml 以下の2箇所を編集する\n1 2 3 4 5 6 7 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] - sandbox_image = \u0026#34;registry.k8s.io/pause:3.6\u0026#34; + sandbox_image = \u0026#34;registry.k8s.io/pause:3.9\u0026#34; ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] - SystemdCgroup = false + SystemdCgroup = true containerdを再起動する\n1 sudo systemctl restart containerd kubeadm/kubelet/kubectl をインストールする 参考URL\nhttps://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 以下のコマンドを順番に実行する\n1 sudo apt install apt-transport-https ca-certificates curl gpg 1 curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 1 echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list 一度アップデートした後にインストールする\n1 2 sudo apt update sudo apt install kubelet kubeadm kubectl バージョンを固定する\n1 sudo apt-mark hold kubelet kubeadm kubectl 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ sudo apt-mark hold kubelet kubeadm kubectl kubelet set on hold. kubeadm set on hold. kubectl set on hold. mao@k8s-control-plane-01:~$ バージョン固定の解除コマンド\n1 2 sudo apt-mark showhold sudo apt-mark unhold \u0026lt;パッケージ名\u0026gt; ","date":"2024-07-03T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-01/","title":"kubernetesをproxmox上に立ててみた（1）"},{"content":"環境 Proxmox VE 8.2.4 x86_64 アップグレード前 メモリ16GB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@pve:~# neofetch .://:` `://:. root@pve `hMMMMMMd/ /dMMMMMMh` -------- `sMMMMMMMd: :mMMMMMMMs` OS: Proxmox VE 8.2.4 x86_64 `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` Kernel: 6.8.4-2-pve `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` Uptime: 33 days, 20 hours, 14 mins `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Packages: 852 (dpkg) ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. Shell: bash 5.2.15 .+ooooooo+-`oNMMMMNo`-+ooooooo+. Terminal: /dev/pts/0 -+ooooooo/.`sMMs`./ooooooo+- CPU: AMD Ryzen 7 5700G with Radeon Graphi :oooooooo/`..`/oooooooo: GPU: AMD ATI Radeon Vega Series / Radeon :oooooooo/`..`/oooooooo: Memory: 1858MiB / 13837MiB -+ooooooo/.`sMMs`./ooooooo+- .+ooooooo+-`oNMMMMNo`-+ooooooo+. ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` `sMMMMMMMm: :dMMMMMMMs` `hMMMMMMd/ /dMMMMMMh` `://:` `://:` root@pve:~# アップグレード後 メモリ64GB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@pve:~# neofetch .://:` `://:. root@pve `hMMMMMMd/ /dMMMMMMh` -------- `sMMMMMMMd: :mMMMMMMMs` OS: Proxmox VE 8.2.4 x86_64 `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` Kernel: 6.8.8-1-pve `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` Uptime: 1 min `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Packages: 852 (dpkg) ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. Shell: bash 5.2.15 .+ooooooo+-`oNMMMMNo`-+ooooooo+. Terminal: /dev/pts/0 -+ooooooo/.`sMMs`./ooooooo+- CPU: AMD Ryzen 7 5700G with Radeon Graphics (16) @ 4.673GHz :oooooooo/`..`/oooooooo: GPU: AMD ATI Radeon Vega Series / Radeon Vega Mobile Series :oooooooo/`..`/oooooooo: Memory: 1508MiB / 60133MiB -+ooooooo/.`sMMs`./ooooooo+- .+ooooooo+-`oNMMMMNo`-+ooooooo+. ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` `sMMMMMMMm: :dMMMMMMMs` `hMMMMMMd/ /dMMMMMMh` `://:` `://:` root@pve:~# ","date":"2024-06-22T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/deskminix300-memory-upgrade/","title":"ProxmoxがインストールされているDeskminix300のメモリを増設する"},{"content":"環境 Ubuntu 24.04 Zbbix 7.0 Ubuntu MySQL Nginx 手順 このページの通りにインストールをしていく(\u0026ldquo;https://www.zabbix.com/jp/download?zabbix=7.0\u0026os_distribution=ubuntu\u0026os_version=24.04\u0026components=server_frontend_agent\u0026db=mysql\u0026ws=nginx\") ただし、MySQLは別でインストールをする必要がある 以下に公式手順をコピペしたものを記載しています\nZabbixリポジトリをインストールする 1 2 3 wget https://repo.zabbix.com/zabbix/7.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_7.0-1+ubuntu24.04_all.deb dpkg -i zabbix-release_7.0-1+ubuntu24.04_all.deb apt update Zabbixサーバー、フロントエンド、エージェントをインストールする 1 apt install zabbix-server-mysql zabbix-frontend-php zabbix-nginx-conf zabbix-sql-scripts zabbix-agent MySQLをインストールする 1 sudo apt install mysql-server 初期データベースを作成する 1 2 mysql -uroot -p password 1 2 3 4 5 mysql\u0026gt; create database zabbix character set utf8mb4 collate utf8mb4_bin; mysql\u0026gt; create user zabbix@localhost identified by \u0026#39;password\u0026#39;; mysql\u0026gt; grant all privileges on zabbix.* to zabbix@localhost; mysql\u0026gt; set global log_bin_trust_function_creators = 1; mysql\u0026gt; quit; Zabbix サーバー ホストで初期スキーマとデータをインポートします。新しく作成したパスワードを入力するよう求められます。 1 zcat /usr/share/zabbix-sql-scripts/mysql/server.sql.gz | mysql --default-character-set=utf8mb4 -uzabbix -p zabbix データベース スキーマをインポートした後、log_bin_trust_function_creators オプションを無効にします。\n1 2 mysql -uroot -p password 1 2 mysql\u0026gt; set global log_bin_trust_function_creators = 0; mysql\u0026gt; quit; Zabbixサーバーのデータベースを構成する ファイル /etc/zabbix/zabbix_server.conf を編集します。\n1 DBPassword=password Zabbixフロントエンド用にPHPを構成する ファイル /etc/zabbix/nginx.conf を編集し、コメントアウトを解除して \u0026rsquo;listen\u0026rsquo; および \u0026lsquo;server_name\u0026rsquo; ディレクティブを設定します。\n1 2 listen 8080; server_name example.com; Zabbixサーバーとエージェントのプロセスを起動する Zabbix サーバーおよびエージェント プロセスを起動し、システムの起動時に起動するようにします。\n1 2 systemctl restart zabbix-server zabbix-agent nginx php8.3-fpm systemctl enable zabbix-server zabbix-agent nginx php8.3-fpm Zabbix UI Webページを開く 1 IPアドレス:8080 初期設定をします\nブラウザの画面 ログインする際の初期ID・パスワードは以下の通りです\nUsername：Admin Password：zabbix 参考URL https://www.zabbix.com/jp/download?zabbix=7.0\u0026os_distribution=ubuntu\u0026os_version=24.04\u0026components=server_frontend_agent\u0026db=mysql\u0026ws=nginx https://www.site24x7.jp/blog/zabbix-6-construction/ 備考：MySQLをアンインストールして再インストールする MySQLをインストールしてZbbixのデータベースを作成する際にエラーになってしまったので、再度インストールをした際の手順です\nアンインストール 1 2 3 4 5 6 sudo apt update sudo apt upgrade sudo apt purge mysql* sudo rm -rf /etx/mysql /var/lib/mysql sudo apt autoremove sudo apt autoclean 再インストール 1 sudo apt install mysql-server ","date":"2024-06-22T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/zabbiz-install/","title":"Zbbix7.0LTSをUbuntuにインストールする"},{"content":"環境 Windows 11 Home バージョン 23H2 hugo v0.121.1-extended windows/amd64 hugoのテーマ：stack(\u0026ldquo;https://github.com/CaiJimmy/hugo-theme-stack\") 更新日時の表示 記事のMarkdownファイルに\u0026quot;lastmod:\u0026ldquo;を追加し、更新日時を入れます\n1 2 3 4 title: xxx date: 2024-06-01 lastmod: 2024-06-14 slug: そうすると記事の一番下に更新日時が表示されます\nただ、一番下なので記事を見たとき更新日時をすぐに確認できないので、作成日時の横に更新日時を表示させられるようにします\n作成日時の横に更新日時を表示 下記のパスにある\u0026quot;footer.html\u0026quot;を開きます\n1 ./layouts/partials/article/components/footer.html 上記ファイルの中にある下記の部分をコピーします 下記が更新日時を表示させているコードです\n1 2 3 4 5 6 7 8 {{- if ne .Lastmod .Date -}} \u0026lt;section class=\u0026#34;article-lastmod\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;span\u0026gt; {{ T \u0026#34;article.lastUpdatedOn\u0026#34; }} {{ .Lastmod.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/span\u0026gt; \u0026lt;/section\u0026gt; {{- end -}} 下記のパスにある\u0026quot;details.html\u0026quot;を開きます\n1 ./layouts/partials/article/components/details.html 下記のコメント（20行目から29行目）を付けた部分に先ほどコピーしたコードを追加します\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026lt;footer class=\u0026#34;article-time\u0026#34;\u0026gt; {{ if $showDate }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;date\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--published\u0026#34;\u0026gt; {{- .Date.Format (or .Site.Params.dateFormat.published \u0026#34;Jan 02, 2006\u0026#34;) -}} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ if $showReadingTime }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--reading\u0026#34;\u0026gt; {{ T \u0026#34;article.readingTime\u0026#34; .ReadingTime }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;!--ここから--\u0026gt; {{- if ne .Lastmod .Date -}} \u0026lt;div class=\u0026#34;article-time--lastUpdated\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time\u0026gt; {{ T \u0026#34;article.lastUpdatedOn\u0026#34; }} {{ .Lastmod.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{- end -}} \u0026lt;!--ここを追加--\u0026gt; \u0026lt;/footer\u0026gt; 確認 下記のコマンドで起動して確認してみます\n1 hugo server -D 無事作成日時の横に更新日時が表示されました\n","date":"2024-06-14T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/hugo-stack-custom-lastupdated/","title":"Hugoで作成日時の横に更新日時を表示できるようにする"},{"content":"ファイル・フォルダ構成 以下のような構成になっています\n1 2 3 4 5 6 dev ┣━ db-data ┣━ log ┣━ mysql ┃　┗━ my.cnf ┗━ docker-compose.yaml docker-compose.yaml のファイル 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 services: mysql: image: mysql:8.4.0 ports: - \u0026#34;3306:3306\u0026#34; environment: MYSQL_ROOT_PASSWORD: mysql MYSQL_DATABASE: db MYSQL_USER: user MYSQL_PASSWORD: password TZ: \u0026#39;Asia/Tokyo\u0026#39; volumes: - ./db-data:/var/lib/mysql - ./mysql:/etc/mysql/conf.d - ./log:/var/log/mysql phpmyadmin: image: phpmyadmin:5.2.1 depends_on: - mysql environment: - PMA_ARBITRARY=1 - PMA_HOSTS=mysql - PMA_USER=root - PMA_PASSWORD=mysql ports: - \u0026#34;3001:80\u0026#34; volumes: db-data: docker composeで構築したMySQLのログをローカルに保存したい my.cnfファイルに以下の内容を追記する\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [mysqld] log_output=FILE # General Log general_log=1 general_log_file=/var/log/mysql/mysql-query.log # Slow Query Log slow_query_log=1 slow_query_log_file=/var/log/mysql/mysql-slow.log # slow_query_time = 1.0s long_query_time=1.0 log_queries_not_using_indexes=0 # Error Log log_error=/var/log/mysql/mysql-error.log log_error_verbosity=3 docker-compose.yamlに以下の内容を追記する\n1 2 volumes: - ./log:/var/log/mysql ログが書き込まれない パーミッションがありすぎるとログファイルが生成されないので権限を必要最小限にする\n1 sudo chmod -R 775 . 1 2 3 4 5 mao@mao:~/dev$ sudo ls -l ./log total 32 -rw-r----- 1 999 systemd-journal 16239 6月 1 23:33 mysql-error.log -rw-r----- 1 999 systemd-journal 10468 6月 1 23:29 mysql-query.log -rw-r----- 1 999 systemd-journal 180 6月 1 23:29 mysql-slow.log docker-composeのスタートとストップ スタート\n1 sudo docker compose up -d ストップ\n1 sudo docker compose down -v 以下のURLからphpmyadminにアクセスし、少し作業をします\nするとログファイルを作成されます\nhttp://localhost:3001/ ","date":"2024-06-02T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/docker-compose-mysql-log/","title":"docker-composeで構築したMySQLのログをローカルに保存する"},{"content":"発生現象 Deskmini x300(CPU:Ryzen7 5700G) に元々SATA SSDを差していたがデータをM.2 SSDにクローンをして差し替えると、有線LANが繋がらなくなる\n結果、ProxmoxのWebGUIにアクセスできなくなる\n対処方法 WebGUIではなく本体からコマンドで、ブリッジネットワークにリンクしている物理LANを「enp1s0」から「enp2s0」へ変更する\n手順 SATA SSDのとき 元々は500GBのSATA SSDを使用していた ネットワークの設定 M.2 SSDのとき M.2 SSDに差し替えた 元々「enp1s0」だったが「enp2s0」にすると通信可能になった ","date":"2024-05-26T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/proxmox-ssd-lan/","title":"Deskmini x300でSATA SSDからM.2 SSDに差し替えるとProxmoxのWebGUIにアクセスできなくなる"},{"content":"環境 Ubuntu 23.10 Intel Core i5 13500 メモリ64GB 1：setup ソースからビルドするのに必要なソフトを確認する\n1 2 make --version gcc --version インストールされていない場合は以下のコマンドでインストールする\n1 2 sudo apt install make sudo apt install gcc ビルドするのに必要なバージョン\nThe minimum version of Go required depends on the target version of Go:\nGo \u0026lt;= 1.4: a C toolchain. 1.5 \u0026lt;= Go \u0026lt;= 1.19: a Go 1.4 compiler. 1.20 \u0026lt;= Go \u0026lt;= 1.21: a Go 1.17 compiler. 1.22 \u0026lt;= Go \u0026lt;= 1.23: a Go 1.20 compiler. Going forward, Go version 1.N will require a Go 1.M compiler, where M is N-2 rounded down to an even number. Example: Go 1.24 and 1.25 require Go 1.22. 2：build go1.4 go1.4-bootstrapをビルドする\n1 2 3 wget https://dl.google.com/go/go1.4-bootstrap-20171003.tar.gz mkdir go1.4-bootstrap \u0026amp;\u0026amp; tar xzvf go1.4-bootstrap-20171003.tar.gz -C go1.4-bootstrap --strip-components 1 cd ./go1.4-bootstrap/src 1 CGO_ENABLED=0 bash ./make.bash 1 2 Installed Go for linux/amd64 in /home/mao/Desktop/go1.4-bootstrap Installed commands in /home/mao/Desktop/go1.4-bootstrap/bin 3：build go1.17 go1.17をビルドする\n1 2 3 wget https://dl.google.com/go/go1.17.src.tar.gz mkdir go1.17 \u0026amp;\u0026amp; tar xzvf go1.17.src.tar.gz -C go1.17 --strip-components 1 cd ./go1.17/src 1 2 GOROOT_BOOTSTRAP=${PWD}/go1.4-bootstrap bash ./all.bash GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.4-bootstrap bash ./all.bash 1 2 3 4 5 6 7 Go version is \u0026#34;go1.17\u0026#34;, ignoring -next /home/mao/Desktop/go1.17/api/next.txt ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.17 Installed commands in /home/mao/Desktop/go1.17/bin *** You need to add /home/mao/Desktop/go1.17/bin to your PATH. 4：build go1.20 go1.20をビルドする\n1 2 3 wget https://dl.google.com/go/go1.20.src.tar.gz mkdir go1.20 \u0026amp;\u0026amp; tar xzvf go1.20.src.tar.gz -C go1.20 --strip-components 1 cd ./go1.20/src 1 2 /home/mao/Desktop/go1.17/bin GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.17 bash ./all.bash 1 2 3 4 5 ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.20 Installed commands in /home/mao/Desktop/go1.20/bin *** You need to add /home/mao/Desktop/go1.20/bin to your PATH. 5：build go1.22.2 latest go1.22.2をビルドする\n1 2 3 wget https://dl.google.com/go/go1.22.2.src.tar.gz mkdir go1.22.2 \u0026amp;\u0026amp; tar xzvf go1.22.2.src.tar.gz -C go1.22.2 --strip-components 1 cd ./go1.22.2/src 1 2 /home/mao/Desktop/go1.20/bin GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.20 bash ./all.bash 1 2 3 4 5 ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.22.2 Installed commands in /home/mao/Desktop/go1.22.2/bin *** You need to add /home/mao/Desktop/go1.22.2/bin to your PATH. 6：Pathを通す パスを通してバージョンを確認する\n1 sudo cp -rp ./go1.22.2 /usr/local/ .bashrc\n1 2 export PATH=$PATH:/usr/local/go/bin export PATH=$PATH:/usr/local/go1.22.2/bin 1 source ~/.bashrc 1 go version 1 go version go1.22.2 linux/amd64 参考URL https://go.dev/doc/install/source https://qiita.com/myoshimi/items/5d1f6a2ee8a849bac7eb https://qiita.com/soarflat/items/d5015bec37f8a8254380 ","date":"2024-05-25T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/golang-source-build/","title":"Go言語をソースコードからビルドする"}]