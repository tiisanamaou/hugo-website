[{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS LoadBalancer（MetalLB）の設定をする kubernetesのクラスターの外部からIPアドレスでアクセスするための設定をする ロードバランサー（MetalLB）を使用して外部からアクセスできるようにする マニフェストファイルの\u0026quot;type\u0026quot;に\u0026quot;LoadBalancer\u0026quot;を指定できるようになる MetalLB を実行する 今回はMetalLBを使用する\nhttps://metallb.universe.tf/installation/ ARPの設定をする 1 1 2 3 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl diff -f - -n kube-system 2 1 2 3 kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system 実行結果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 mao@k8s-control-plane-01:~$ kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl diff -f - -n kube-system diff -u -N /tmp/LIVE-1007559117/v1.ConfigMap.kube-system.kube-proxy /tmp/MERGED-3311346621/v1.ConfigMap.kube-system.kube-proxy --- /tmp/LIVE-1007559117/v1.ConfigMap.kube-system.kube-proxy 2024-06-26 12:33:11.717730136 +0000 +++ /tmp/MERGED-3311346621/v1.ConfigMap.kube-system.kube-proxy 2024-06-26 12:33:11.718730159 +0000 @@ -37,7 +37,7 @@ excludeCIDRs: null minSyncPeriod: 0s scheduler: \u0026#34;\u0026#34; - strictARP: false + strictARP: true syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s mao@k8s-control-plane-01:~$ kubectl get configmap kube-proxy -n kube-system -o yaml | \\ sed -e \u0026#34;s/strictARP: false/strictARP: true/\u0026#34; | \\ kubectl apply -f - -n kube-system Warning: resource configmaps/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. configmap/kube-proxy configured mao@k8s-control-plane-01:~$ MetalLBの実行をする 参考URL\nhttps://blog.ntnx.jp/entry/2024/02/14/025309 1 2 wget https://raw.githubusercontent.com/metallb/metallb/v0.14.5/config/manifests/metallb-native.yaml kubectl apply -f metallb-native.yaml 確認をする\n1 kubectl get pod -n metallb-system 実行結果\n1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ kubectl get pod -n metallb-system NAME READY STATUS RESTARTS AGE controller-86f5578878-dz5zm 1/1 Running 0 43s speaker-mbvpk 1/1 Running 0 43s speaker-rx75f 1/1 Running 0 43s speaker-xwb66 1/1 Running 0 43s mao@k8s-control-plane-01:~$ 払い出せるIPアドレスの範囲を設定する\n下記の部分で範囲を設定する 1 2 3 spec: addresses: - 192.168.10.55-192.168.10.60 1 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt; EOF \u0026gt; ippool.yaml apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: default namespace: metallb-system spec: addresses: - 192.168.10.55-192.168.10.60 EOF 2 1 2 3 4 5 6 7 8 9 10 cat \u0026lt;\u0026lt; EOF \u0026gt; l2adv.yaml apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: default namespace: metallb-system spec: ipAddressPools: - default EOF 設定を実行する\n1 kubectl apply -f ippool.yaml -f l2adv.yaml 実行結果\n1 2 3 mao@k8s-control-plane-01:~$ kubectl apply -f ippool.yaml -f l2adv.yaml ipaddresspool.metallb.io/default created l2advertisement.metallb.io/default created マニフェストファイルを実行後、外部IPアドレスが設定されているか確認する 確認するためのコマンド\n1 2 3 kubectl get svc or kubectl get service 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 25h nginx-deployment-lb LoadBalancer 10.98.208.100 192.168.10.45 80:32762/TCP 58s mao@k8s-control-plane-01:~$ \u0026ldquo;nginx-deployment-lb\u0026quot;の\u0026quot;TYPE\u0026quot;に\u0026quot;LoadBalancer\u0026quot;が指定されており\u0026quot;EXTERNAL-IP\u0026rdquo;（外部IPアドレス）が割り当てられている\nこのIPアドレスにアクセスすると実際のコンテナにアクセスできる\n","date":"2024-07-07T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-03/","title":"kubernetesをproxmox上に立ててみた（3）/LoadBalancerの設定"},{"content":"開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS Control-Plane（Master-Node）の設定をする kubeadm init を実行する Control-Planeにするマシン上で実行する\n1 2 sudo kubeadm init --apiserver-advertise-address=Control-PlaneのIPアドレス --pod-network-cidr=10.128.0.0/16 sudo kubeadm init --apiserver-advertise-address=192.168.10.41 --pod-network-cidr=10.128.0.0/16 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 mao@k8s-control-plane-01:~$ sudo kubeadm init --apiserver-advertise-address=192.168.10.41 --pod-network-cidr=10.128.0.0/16 [init] Using Kubernetes version: v1.30.2 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-control-plane-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.41] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-control-plane-01 localhost] and IPs [192.168.10.41 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;super-admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34; [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 500.625027ms [api-check] Waiting for a healthy API server. This can take up to 4m0s [api-check] The API server is healthy after 3.50086825s [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-control-plane-01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: evbpii.dp8y5hcfqkv9jn4n [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n \\ --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 mao@k8s-control-plane-01:~$ 実行結果にも表示されている通り、下記のコマンドを実行する\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Calicoを実行する（Pod間ネットワーク） 参考URL\nhttps://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart tigera-operator.yaml\n1 2 wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml kubectl create -f tigera-operator.yaml custom-resources.yaml\n1 wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/custom-resources.yaml ダウンロードしたcustom-resources.yamlを編集する\nkubeadm init で指定した引数\u0026ndash;pod-network-cidrと同じものへ変更する 1 2 - cidr: 192.168.0.0/16 + cidr: 10.128.0.0/16 実行する\n1 kubectl apply -f custom-resources.yaml Nodeを確認する 1 kubectl get nodes -o wide 実行結果\n1 2 3 4 mao@k8s-control-plane-01:~$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-control-plane-01 Ready control-plane 32m v1.30.2 192.168.10.41 \u0026lt;none\u0026gt; Ubuntu 24.04 LTS 6.8.0-35-generic containerd://1.7.18 mao@k8s-control-plane-01:~$ Worker-Nodeの設定をする Worker-NodeをJoinする Control-Planeでkubeadm initを実行した際に表示されたコマンドを実行する\n1 sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 mao@k8s-worker-01:~$ sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-to ken-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s [kubelet-check] The kubelet is healthy after 501.168161ms [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. mao@k8s-worker-01:~$ Worker-NodeがJoinされているか、Control-Planeで確認する\n1 kubectl get node 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 63m v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 36s v1.30.2 mao@k8s-control-plane-01:~$ 複数のWorker-NodeをJoinする 基本手順は1つ目のWorker-Nodeと同じように実行するとJoinできる\n1 sudo kubeadm join 192.168.10.41:6443 --token evbpii.dp8y5hcfqkv9jn4n --discovery-token-ca-cert-hash sha256:dd7a24f7fcd7aeea509476025652a8a1aee32e9e8d5f54ec48de16345eb1a425 実行したらControl-PlanでNodeを確認してみる\n1 2 3 4 5 6 mao@k8s-control-plane-01:~$ kubectl get node NAME STATUS ROLES AGE VERSION k8s-control-plane-01 Ready control-plane 9h v1.30.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 8h v1.30.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 83s v1.30.2 mao@k8s-control-plane-01:~$ 無事にWorker-NodeがJoinされている\n","date":"2024-07-06T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-02/","title":"kubernetesをproxmox上に立ててみた（2）/Control-Plane・Worker-Nodeの設定"},{"content":" k8sを勉強してみようと思い、デプロイ等の操作は書籍で触っていたら構築もしてみたくなったので、Proxmox上に仮想マシンを作成してk8sを構築してみた\n開発環境 Proxmox 8.2.4 Ubuntu Server 24.04 LTS 構成 Proxmox上に以下6つの仮想マシンを立てました\nhaproxy-01 control-plane-node-01 control-plane-node-02 control-plane-node-03 worker-node-01 worker-node-02 Control-Plane（Master-Node）とWorker-Node両方で実行する setup パッケージの更新をする\n1 2 sudo apt update sudo apt upgrade 必要なソフトウェアをインストールする\n1 sudo apt install nano 公式の手順にそって実行する\nhttps://kubernetes.io/ja/docs/setup/production-environment/ Swapをオフにする swapを止めます\n1 sudo swapoff -a 設定ファイルを書き換えて永続的にswapをオフにする\n1 sudo nano /etc/fstab 編集内容\n1 2 - /swap.img none swap sw 0 0 + #/swap.img none swap sw 0 0 swapがオフになっているか確認する\n1 free -h 実行結果\n1 2 3 4 mao@k8s-control-plane-01:~$ free -h total used free shared buff/cache available Mem: 7.8Gi 510Mi 7.2Gi 704Ki 248Mi 7.3Gi Swap: 0B 0B 0B IPアドレスを固定IPアドレスにする ネットワークのデバイスを確認します\n1 ip address 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 mao@k8s-control-plane-01:~$ ip address 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether bc:24:11:7f:08:e4 brd ff:ff:ff:ff:ff:ff altname enp0s18 inet 192.168.10.10/24 metric 100 brd 192.168.10.255 scope global dynamic ens18 valid_lft 85953sec preferred_lft 85953sec inet6 fe80::be24:11ff:fe7f:8e4/64 scope link valid_lft forever preferred_lft forever mao@k8s-control-plane-01:~$ netplanファイルを作成します\nファイル名：99-config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 network: version: 2 renderer: networkd ethernets: ens18: dhcp4: false addresses: - 192.168.10.41/24 routes: - to: default via: 192.168.10.1 nameservers: search: [] addresses: [192.168.10.1] netplanファイルをコピーして適用します\nssh等で接続している場合は、IPアドレスが変わるので接続が切れます、\n再度固定にしたIPアドレスに変更すれば接続できます 1 2 3 sudo cp 99-config.yaml /etc/netplan/ sudo netplan apply sudo chmod 600 /etc/netplan/99-config.yaml containerdをインストールする 公式手順に従ってインストール\nhttps://github.com/containerd/containerd/blob/main/docs/getting-started.md Option 1: From the official binaries containerd 1.7.18 順にコマンドを実行する\n1 2 wget https://github.com/containerd/containerd/releases/download/v1.7.18/containerd-1.7.18-linux-amd64.tar.gz sudo tar Cxzvf /usr/local containerd-1.7.18-linux-amd64.tar.gz 1 sudo wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -O /etc/systemd/system/containerd.service systemctlをリロードし、containerdを有効にする\n1 2 sudo systemctl daemon-reload sudo systemctl enable --now containerd runCをインストールする 1 sudo wget https://github.com/opencontainers/runc/releases/download/v1.1.13/runc.amd64 1 sudo install -m 755 runc.amd64 /usr/local/sbin/runc CNI(Container Network Interface) pluginをインストールする 1 sudo wget https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz 1 2 sudo mkdir -p /opt/cni/bin sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.5.1.tgz IPv4フォワーディングの設定をする 以下のコマンドを順に実行する\n1 1 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf 実行結果\n1 2 3 4 5 6 7 mao@k8s-control-plane-01:~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf \u0026gt; overlay \u0026gt; br_netfilter \u0026gt; EOF overlay br_netfilter mao@k8s-control-plane-01:~$ 1 2 sudo modprobe overlay sudo modprobe br_netfilter 2 1 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf 実行結果\n1 2 3 4 5 6 7 8 9 mao@k8s-control-plane-01:~$ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf \u0026gt; net.bridge.bridge-nf-call-iptables = 1 \u0026gt; net.bridge.bridge-nf-call-ip6tables = 1 \u0026gt; net.ipv4.ip_forward = 1 \u0026gt; EOF net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 mao@k8s-control-plane-01:~$ 1 sudo sysctl --system 3 1 2 lsmod | grep br_netfilter lsmod | grep overlay 実行結果\n1 2 3 4 5 6 mao@k8s-control-plane-01:~$ lsmod | grep br_netfilter br_netfilter 32768 0 bridge 421888 1 br_netfilter mao@k8s-control-plane-01:~$ lsmod | grep overlay overlay 212992 0 mao@k8s-control-plane-01:~$ 4 1 sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 mao@k8s-control-plane-01:~$ systemd cgroup の設定をする 参考URL\nhttps://kubernetes.io/ja/docs/concepts/architecture/cgroups/ https://kubernetes.io/ja/docs/concepts/architecture/cgroups/#check-cgroup-version https://sogo.dev/posts/2022/12/kubernetes-ubuntu22.04-cgroup-systemd 1 stat -fc %T /sys/fs/cgroup/ cgroup v2では、\u0026ldquo;cgroup2fs\u0026quot;と出力されます。 cgroup v1では、\u0026ldquo;tmpfs\u0026quot;と出力されます。 ディレクトリを作成する\n1 sudo mkdir /etc/containerd 以下のコマンドで、デフォルトのコンフィグを作成できます。\n1 sudo containerd config default | sudo tee /etc/containerd/config.toml 実行結果\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 mao@k8s-control-plane-01:~$ sudo containerd config default | sudo tee /etc/containerd/config.toml disabled_plugins = [] imports = [] oom_score = 0 plugin_dir = \u0026#34;\u0026#34; required_plugins = [] root = \u0026#34;/var/lib/containerd\u0026#34; state = \u0026#34;/run/containerd\u0026#34; temp = \u0026#34;\u0026#34; version = 2 [cgroup] path = \u0026#34;\u0026#34; [debug] address = \u0026#34;\u0026#34; format = \u0026#34;\u0026#34; gid = 0 level = \u0026#34;\u0026#34; uid = 0 [grpc] address = \u0026#34;/run/containerd/containerd.sock\u0026#34; gid = 0 max_recv_message_size = 16777216 max_send_message_size = 16777216 tcp_address = \u0026#34;\u0026#34; tcp_tls_ca = \u0026#34;\u0026#34; tcp_tls_cert = \u0026#34;\u0026#34; tcp_tls_key = \u0026#34;\u0026#34; uid = 0 [metrics] address = \u0026#34;\u0026#34; grpc_histogram = false [plugins] [plugins.\u0026#34;io.containerd.gc.v1.scheduler\u0026#34;] deletion_threshold = 0 mutation_threshold = 100 pause_threshold = 0.02 schedule_delay = \u0026#34;0s\u0026#34; startup_delay = \u0026#34;100ms\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] cdi_spec_dirs = [\u0026#34;/etc/cdi\u0026#34;, \u0026#34;/var/run/cdi\u0026#34;] device_ownership_from_security_context = false disable_apparmor = false disable_cgroup = false disable_hugetlb_controller = true disable_proc_mount = false disable_tcp_service = true drain_exec_sync_io_timeout = \u0026#34;0s\u0026#34; enable_cdi = false enable_selinux = false enable_tls_streaming = false enable_unprivileged_icmp = false enable_unprivileged_ports = false ignore_deprecation_warnings = [] ignore_image_defined_volumes = false image_pull_progress_timeout = \u0026#34;5m0s\u0026#34; image_pull_with_sync_fs = false max_concurrent_downloads = 3 max_container_log_line_size = 16384 netns_mounts_under_state_dir = false restrict_oom_score_adj = false sandbox_image = \u0026#34;registry.k8s.io/pause:3.8\u0026#34; selinux_category_range = 1024 stats_collect_period = 10 stream_idle_timeout = \u0026#34;4h0m0s\u0026#34; stream_server_address = \u0026#34;127.0.0.1\u0026#34; stream_server_port = \u0026#34;0\u0026#34; systemd_cgroup = false tolerate_missing_hugetlb_controller = true unset_seccomp_profile = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.cni] bin_dir = \u0026#34;/opt/cni/bin\u0026#34; conf_dir = \u0026#34;/etc/cni/net.d\u0026#34; conf_template = \u0026#34;\u0026#34; ip_pref = \u0026#34;\u0026#34; max_conf_num = 1 setup_serially = false [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd] default_runtime_name = \u0026#34;runc\u0026#34; disable_snapshot_annotations = true discard_unpacked_layers = false ignore_blockio_not_enabled_errors = false ignore_rdt_not_enabled_errors = false no_pivot = false snapshotter = \u0026#34;overlayfs\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;\u0026#34; sandbox_mode = \u0026#34;\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.default_runtime.options] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;io.containerd.runc.v2\u0026#34; sandbox_mode = \u0026#34;podsandbox\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] BinaryName = \u0026#34;\u0026#34; CriuImagePath = \u0026#34;\u0026#34; CriuPath = \u0026#34;\u0026#34; CriuWorkPath = \u0026#34;\u0026#34; IoGid = 0 IoUid = 0 NoNewKeyring = false NoPivotRoot = false Root = \u0026#34;\u0026#34; ShimCgroup = \u0026#34;\u0026#34; SystemdCgroup = false [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.untrusted_workload_runtime] base_runtime_spec = \u0026#34;\u0026#34; cni_conf_dir = \u0026#34;\u0026#34; cni_max_conf_num = 0 container_annotations = [] pod_annotations = [] privileged_without_host_devices = false privileged_without_host_devices_all_devices_allowed = false runtime_engine = \u0026#34;\u0026#34; runtime_path = \u0026#34;\u0026#34; runtime_root = \u0026#34;\u0026#34; runtime_type = \u0026#34;\u0026#34; sandbox_mode = \u0026#34;\u0026#34; snapshotter = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.untrusted_workload_runtime.options] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.image_decryption] key_model = \u0026#34;node\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry] config_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.auths] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.configs] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.headers] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.registry.mirrors] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.x509_key_pair_streaming] tls_cert_file = \u0026#34;\u0026#34; tls_key_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.opt\u0026#34;] path = \u0026#34;/opt/containerd\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.restart\u0026#34;] interval = \u0026#34;10s\u0026#34; [plugins.\u0026#34;io.containerd.internal.v1.tracing\u0026#34;] [plugins.\u0026#34;io.containerd.metadata.v1.bolt\u0026#34;] content_sharing_policy = \u0026#34;shared\u0026#34; [plugins.\u0026#34;io.containerd.monitor.v1.cgroups\u0026#34;] no_prometheus = false [plugins.\u0026#34;io.containerd.nri.v1.nri\u0026#34;] disable = true disable_connections = false plugin_config_path = \u0026#34;/etc/nri/conf.d\u0026#34; plugin_path = \u0026#34;/opt/nri/plugins\u0026#34; plugin_registration_timeout = \u0026#34;5s\u0026#34; plugin_request_timeout = \u0026#34;2s\u0026#34; socket_path = \u0026#34;/var/run/nri/nri.sock\u0026#34; [plugins.\u0026#34;io.containerd.runtime.v1.linux\u0026#34;] no_shim = false runtime = \u0026#34;runc\u0026#34; runtime_root = \u0026#34;\u0026#34; shim = \u0026#34;containerd-shim\u0026#34; shim_debug = false [plugins.\u0026#34;io.containerd.runtime.v2.task\u0026#34;] platforms = [\u0026#34;linux/amd64\u0026#34;] sched_core = false [plugins.\u0026#34;io.containerd.service.v1.diff-service\u0026#34;] default = [\u0026#34;walking\u0026#34;] [plugins.\u0026#34;io.containerd.service.v1.tasks-service\u0026#34;] blockio_config_file = \u0026#34;\u0026#34; rdt_config_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.aufs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.blockfile\u0026#34;] fs_type = \u0026#34;\u0026#34; mount_options = [] root_path = \u0026#34;\u0026#34; scratch_file = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.btrfs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.devmapper\u0026#34;] async_remove = false base_image_size = \u0026#34;\u0026#34; discard_blocks = false fs_options = \u0026#34;\u0026#34; fs_type = \u0026#34;\u0026#34; pool_name = \u0026#34;\u0026#34; root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.native\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.snapshotter.v1.overlayfs\u0026#34;] mount_options = [] root_path = \u0026#34;\u0026#34; sync_remove = false upperdir_label = false [plugins.\u0026#34;io.containerd.snapshotter.v1.zfs\u0026#34;] root_path = \u0026#34;\u0026#34; [plugins.\u0026#34;io.containerd.tracing.processor.v1.otlp\u0026#34;] [plugins.\u0026#34;io.containerd.transfer.v1.local\u0026#34;] config_path = \u0026#34;\u0026#34; max_concurrent_downloads = 3 max_concurrent_uploaded_layers = 3 [[plugins.\u0026#34;io.containerd.transfer.v1.local\u0026#34;.unpack_config]] differ = \u0026#34;\u0026#34; platform = \u0026#34;linux/amd64\u0026#34; snapshotter = \u0026#34;overlayfs\u0026#34; [proxy_plugins] [stream_processors] [stream_processors.\u0026#34;io.containerd.ocicrypt.decoder.v1.tar\u0026#34;] accepts = [\u0026#34;application/vnd.oci.image.layer.v1.tar+encrypted\u0026#34;] args = [\u0026#34;--decryption-keys-path\u0026#34;, \u0026#34;/etc/containerd/ocicrypt/keys\u0026#34;] env = [\u0026#34;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\u0026#34;] path = \u0026#34;ctd-decoder\u0026#34; returns = \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34; [stream_processors.\u0026#34;io.containerd.ocicrypt.decoder.v1.tar.gzip\u0026#34;] accepts = [\u0026#34;application/vnd.oci.image.layer.v1.tar+gzip+encrypted\u0026#34;] args = [\u0026#34;--decryption-keys-path\u0026#34;, \u0026#34;/etc/containerd/ocicrypt/keys\u0026#34;] env = [\u0026#34;OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf\u0026#34;] path = \u0026#34;ctd-decoder\u0026#34; returns = \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34; [timeouts] \u0026#34;io.containerd.timeout.bolt.open\u0026#34; = \u0026#34;0s\u0026#34; \u0026#34;io.containerd.timeout.metrics.shimstats\u0026#34; = \u0026#34;2s\u0026#34; \u0026#34;io.containerd.timeout.shim.cleanup\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.load\u0026#34; = \u0026#34;5s\u0026#34; \u0026#34;io.containerd.timeout.shim.shutdown\u0026#34; = \u0026#34;3s\u0026#34; \u0026#34;io.containerd.timeout.task.state\u0026#34; = \u0026#34;2s\u0026#34; [ttrpc] address = \u0026#34;\u0026#34; gid = 0 uid = 0 mao@k8s-control-plane-01:~$ 設定ファイルを編集する\n1 sudo nano /etc/containerd/config.toml 以下の2箇所を編集する\n1 2 3 4 5 6 7 [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] - sandbox_image = \u0026#34;registry.k8s.io/pause:3.6\u0026#34; + sandbox_image = \u0026#34;registry.k8s.io/pause:3.9\u0026#34; ... [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.runc.options] - SystemdCgroup = false + SystemdCgroup = true containerdを再起動する\n1 sudo systemctl restart containerd kubeadm/kubelet/kubectl をインストールする 参考URL\nhttps://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ 以下のコマンドを順番に実行する\n1 sudo apt install apt-transport-https ca-certificates curl gpg 1 curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 1 echo \u0026#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\u0026#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list 一度アップデートした後にインストールする\n1 2 sudo apt update sudo apt install kubelet kubeadm kubectl バージョンを固定する\n1 sudo apt-mark hold kubelet kubeadm kubectl 実行結果\n1 2 3 4 5 mao@k8s-control-plane-01:~$ sudo apt-mark hold kubelet kubeadm kubectl kubelet set on hold. kubeadm set on hold. kubectl set on hold. mao@k8s-control-plane-01:~$ バージョン固定の解除コマンド\n1 2 sudo apt-mark showhold sudo apt-mark unhold \u0026lt;パッケージ名\u0026gt; ","date":"2024-07-03T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/kubernetes-on-proxmox-01/","title":"kubernetesをproxmox上に立ててみた（1）"},{"content":"環境 Proxmox VE 8.2.4 x86_64 アップグレード前 メモリ16GB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@pve:~# neofetch .://:` `://:. root@pve `hMMMMMMd/ /dMMMMMMh` -------- `sMMMMMMMd: :mMMMMMMMs` OS: Proxmox VE 8.2.4 x86_64 `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` Kernel: 6.8.4-2-pve `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` Uptime: 33 days, 20 hours, 14 mins `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Packages: 852 (dpkg) ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. Shell: bash 5.2.15 .+ooooooo+-`oNMMMMNo`-+ooooooo+. Terminal: /dev/pts/0 -+ooooooo/.`sMMs`./ooooooo+- CPU: AMD Ryzen 7 5700G with Radeon Graphi :oooooooo/`..`/oooooooo: GPU: AMD ATI Radeon Vega Series / Radeon :oooooooo/`..`/oooooooo: Memory: 1858MiB / 13837MiB -+ooooooo/.`sMMs`./ooooooo+- .+ooooooo+-`oNMMMMNo`-+ooooooo+. ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` `sMMMMMMMm: :dMMMMMMMs` `hMMMMMMd/ /dMMMMMMh` `://:` `://:` root@pve:~# アップグレード後 メモリ64GB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@pve:~# neofetch .://:` `://:. root@pve `hMMMMMMd/ /dMMMMMMh` -------- `sMMMMMMMd: :mMMMMMMMs` OS: Proxmox VE 8.2.4 x86_64 `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` Kernel: 6.8.8-1-pve `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` Uptime: 1 min `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` Packages: 852 (dpkg) ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. Shell: bash 5.2.15 .+ooooooo+-`oNMMMMNo`-+ooooooo+. Terminal: /dev/pts/0 -+ooooooo/.`sMMs`./ooooooo+- CPU: AMD Ryzen 7 5700G with Radeon Graphics (16) @ 4.673GHz :oooooooo/`..`/oooooooo: GPU: AMD ATI Radeon Vega Series / Radeon Vega Mobile Series :oooooooo/`..`/oooooooo: Memory: 1508MiB / 60133MiB -+ooooooo/.`sMMs`./ooooooo+- .+ooooooo+-`oNMMMMNo`-+ooooooo+. ./ooooooo+- +NMMMMMMMMN+ -+ooooooo/. `/oooooooo:`:mMMMMMMMMMMMMm:`:oooooooo/` `:oooooooo/`-hMMMMMMMyyMMMMMMMh-`/oooooooo:` `-/+oo+/:`.yMMMMMMMh- -hMMMMMMMy.`:/+oo+/-` `sMMMMMMMm: :dMMMMMMMs` `hMMMMMMd/ /dMMMMMMh` `://:` `://:` root@pve:~# ","date":"2024-06-22T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/deskminix300-memory-upgrade/","title":"ProxmoxがインストールされているDeskminix300のメモリを増設する"},{"content":"環境 Ubuntu 24.04 Zbbix 7.0 Ubuntu MySQL Nginx 手順 このページの通りにインストールをしていく(\u0026ldquo;https://www.zabbix.com/jp/download?zabbix=7.0\u0026os_distribution=ubuntu\u0026os_version=24.04\u0026components=server_frontend_agent\u0026db=mysql\u0026ws=nginx\") ただし、MySQLは別でインストールをする必要がある 以下に公式手順をコピペしたものを記載しています\nZabbixリポジトリをインストールする 1 2 3 wget https://repo.zabbix.com/zabbix/7.0/ubuntu/pool/main/z/zabbix-release/zabbix-release_7.0-1+ubuntu24.04_all.deb dpkg -i zabbix-release_7.0-1+ubuntu24.04_all.deb apt update Zabbixサーバー、フロントエンド、エージェントをインストールする 1 apt install zabbix-server-mysql zabbix-frontend-php zabbix-nginx-conf zabbix-sql-scripts zabbix-agent MySQLをインストールする 1 sudo apt install mysql-server 初期データベースを作成する 1 2 mysql -uroot -p password 1 2 3 4 5 mysql\u0026gt; create database zabbix character set utf8mb4 collate utf8mb4_bin; mysql\u0026gt; create user zabbix@localhost identified by \u0026#39;password\u0026#39;; mysql\u0026gt; grant all privileges on zabbix.* to zabbix@localhost; mysql\u0026gt; set global log_bin_trust_function_creators = 1; mysql\u0026gt; quit; Zabbix サーバー ホストで初期スキーマとデータをインポートします。新しく作成したパスワードを入力するよう求められます。 1 zcat /usr/share/zabbix-sql-scripts/mysql/server.sql.gz | mysql --default-character-set=utf8mb4 -uzabbix -p zabbix データベース スキーマをインポートした後、log_bin_trust_function_creators オプションを無効にします。\n1 2 mysql -uroot -p password 1 2 mysql\u0026gt; set global log_bin_trust_function_creators = 0; mysql\u0026gt; quit; Zabbixサーバーのデータベースを構成する ファイル /etc/zabbix/zabbix_server.conf を編集します。\n1 DBPassword=password Zabbixフロントエンド用にPHPを構成する ファイル /etc/zabbix/nginx.conf を編集し、コメントアウトを解除して \u0026rsquo;listen\u0026rsquo; および \u0026lsquo;server_name\u0026rsquo; ディレクティブを設定します。\n1 2 listen 8080; server_name example.com; Zabbixサーバーとエージェントのプロセスを起動する Zabbix サーバーおよびエージェント プロセスを起動し、システムの起動時に起動するようにします。\n1 2 systemctl restart zabbix-server zabbix-agent nginx php8.3-fpm systemctl enable zabbix-server zabbix-agent nginx php8.3-fpm Zabbix UI Webページを開く 1 IPアドレス:8080 初期設定をします\nブラウザの画面 ログインする際の初期ID・パスワードは以下の通りです\nUsername：Admin Password：zabbix 参考URL https://www.zabbix.com/jp/download?zabbix=7.0\u0026os_distribution=ubuntu\u0026os_version=24.04\u0026components=server_frontend_agent\u0026db=mysql\u0026ws=nginx https://www.site24x7.jp/blog/zabbix-6-construction/ 備考：MySQLをアンインストールして再インストールする MySQLをインストールしてZbbixのデータベースを作成する際にエラーになってしまったので、再度インストールをした際の手順です\nアンインストール 1 2 3 4 5 6 sudo apt update sudo apt upgrade sudo apt purge mysql* sudo rm -rf /etx/mysql /var/lib/mysql sudo apt autoremove sudo apt autoclean 再インストール 1 sudo apt install mysql-server ","date":"2024-06-22T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/zabbiz-install/","title":"Zbbix7.0LTSをUbuntuにインストールする"},{"content":"環境 Windows 11 Home バージョン 23H2 hugo v0.121.1-extended windows/amd64 hugoのテーマ：stack(\u0026ldquo;https://github.com/CaiJimmy/hugo-theme-stack\") 更新日時の表示 記事のMarkdownファイルに\u0026quot;lastmod:\u0026ldquo;を追加し、更新日時を入れます\n1 2 3 4 title: xxx date: 2024-06-01 lastmod: 2024-06-14 slug: そうすると記事の一番下に更新日時が表示されます\nただ、一番下なので記事を見たとき更新日時をすぐに確認できないので、作成日時の横に更新日時を表示させられるようにします\n作成日時の横に更新日時を表示 下記のパスにある\u0026quot;footer.html\u0026quot;を開きます\n1 ./layouts/partials/article/components/footer.html 上記ファイルの中にある下記の部分をコピーします 下記が更新日時を表示させているコードです\n1 2 3 4 5 6 7 8 {{- if ne .Lastmod .Date -}} \u0026lt;section class=\u0026#34;article-lastmod\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;span\u0026gt; {{ T \u0026#34;article.lastUpdatedOn\u0026#34; }} {{ .Lastmod.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/span\u0026gt; \u0026lt;/section\u0026gt; {{- end -}} 下記のパスにある\u0026quot;details.html\u0026quot;を開きます\n1 ./layouts/partials/article/components/details.html 下記のコメント（20行目から29行目）を付けた部分に先ほどコピーしたコードを追加します\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026lt;footer class=\u0026#34;article-time\u0026#34;\u0026gt; {{ if $showDate }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;date\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--published\u0026#34;\u0026gt; {{- .Date.Format (or .Site.Params.dateFormat.published \u0026#34;Jan 02, 2006\u0026#34;) -}} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ if $showReadingTime }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--reading\u0026#34;\u0026gt; {{ T \u0026#34;article.readingTime\u0026#34; .ReadingTime }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;!--ここから--\u0026gt; {{- if ne .Lastmod .Date -}} \u0026lt;div class=\u0026#34;article-time--lastUpdated\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time\u0026gt; {{ T \u0026#34;article.lastUpdatedOn\u0026#34; }} {{ .Lastmod.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{- end -}} \u0026lt;!--ここを追加--\u0026gt; \u0026lt;/footer\u0026gt; 確認 下記のコマンドで起動して確認してみます\n1 hugo server -D 無事作成日時の横に更新日時が表示されました\n","date":"2024-06-14T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/hugo-stack-custom-lastupdated/","title":"Hugoで作成日時の横に更新日時を表示できるようにする"},{"content":"ファイル・フォルダ構成 以下のような構成になっています\n1 2 3 4 5 6 dev ┣━ db-data ┣━ log ┣━ mysql ┃　┗━ my.cnf ┗━ docker-compose.yaml docker-compose.yaml のファイル 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 services: mysql: image: mysql:8.4.0 ports: - \u0026#34;3306:3306\u0026#34; environment: MYSQL_ROOT_PASSWORD: mysql MYSQL_DATABASE: db MYSQL_USER: user MYSQL_PASSWORD: password TZ: \u0026#39;Asia/Tokyo\u0026#39; volumes: - ./db-data:/var/lib/mysql - ./mysql:/etc/mysql/conf.d - ./log:/var/log/mysql phpmyadmin: image: phpmyadmin:5.2.1 depends_on: - mysql environment: - PMA_ARBITRARY=1 - PMA_HOSTS=mysql - PMA_USER=root - PMA_PASSWORD=mysql ports: - \u0026#34;3001:80\u0026#34; volumes: db-data: docker composeで構築したMySQLのログをローカルに保存したい my.cnfファイルに以下の内容を追記する\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [mysqld] log_output=FILE # General Log general_log=1 general_log_file=/var/log/mysql/mysql-query.log # Slow Query Log slow_query_log=1 slow_query_log_file=/var/log/mysql/mysql-slow.log # slow_query_time = 1.0s long_query_time=1.0 log_queries_not_using_indexes=0 # Error Log log_error=/var/log/mysql/mysql-error.log log_error_verbosity=3 docker-compose.yamlに以下の内容を追記する\n1 2 volumes: - ./log:/var/log/mysql ログが書き込まれない パーミッションがありすぎるとログファイルが生成されないので権限を必要最小限にする\n1 sudo chmod -R 775 . 1 2 3 4 5 mao@mao:~/dev$ sudo ls -l ./log total 32 -rw-r----- 1 999 systemd-journal 16239 6月 1 23:33 mysql-error.log -rw-r----- 1 999 systemd-journal 10468 6月 1 23:29 mysql-query.log -rw-r----- 1 999 systemd-journal 180 6月 1 23:29 mysql-slow.log docker-composeのスタートとストップ スタート\n1 sudo docker compose up -d ストップ\n1 sudo docker compose down -v 以下のURLからphpmyadminにアクセスし、少し作業をします\nするとログファイルを作成されます\nhttp://localhost:3001/ ","date":"2024-06-02T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/docker-compose-mysql-log/","title":"docker-composeで構築したMySQLのログをローカルに保存する"},{"content":"発生現象 Deskmini x300(CPU:Ryzen7 5700G) に元々SATA SSDを差していたがデータをM.2 SSDにクローンをして差し替えると、有線LANが繋がらなくなる\n結果、ProxmoxのWebGUIにアクセスできなくなる\n対処方法 WebGUIではなく本体からコマンドで、ブリッジネットワークにリンクしている物理LANを「enp1s0」から「enp2s0」へ変更する\n手順 SATA SSDのとき 元々は500GBのSATA SSDを使用していた ネットワークの設定 M.2 SSDのとき M.2 SSDに差し替えた 元々「enp1s0」だったが「enp2s0」にすると通信可能になった ","date":"2024-05-26T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/proxmox-ssd-lan/","title":"Deskmini x300でSATA SSDからM.2 SSDに差し替えるとProxmoxのWebGUIにアクセスできなくなる"},{"content":"環境 Ubuntu 23.10 Intel Core i5 13500 メモリ64GB 1：setup ソースからビルドするのに必要なソフトを確認する\n1 2 make --version gcc --version インストールされていない場合は以下のコマンドでインストールする\n1 2 sudo apt install make sudo apt install gcc ビルドするのに必要なバージョン\nThe minimum version of Go required depends on the target version of Go:\nGo \u0026lt;= 1.4: a C toolchain. 1.5 \u0026lt;= Go \u0026lt;= 1.19: a Go 1.4 compiler. 1.20 \u0026lt;= Go \u0026lt;= 1.21: a Go 1.17 compiler. 1.22 \u0026lt;= Go \u0026lt;= 1.23: a Go 1.20 compiler. Going forward, Go version 1.N will require a Go 1.M compiler, where M is N-2 rounded down to an even number. Example: Go 1.24 and 1.25 require Go 1.22. 2：build go1.4 go1.4-bootstrapをビルドする\n1 2 3 wget https://dl.google.com/go/go1.4-bootstrap-20171003.tar.gz mkdir go1.4-bootstrap \u0026amp;\u0026amp; tar xzvf go1.4-bootstrap-20171003.tar.gz -C go1.4-bootstrap --strip-components 1 cd ./go1.4-bootstrap/src 1 CGO_ENABLED=0 bash ./make.bash 1 2 Installed Go for linux/amd64 in /home/mao/Desktop/go1.4-bootstrap Installed commands in /home/mao/Desktop/go1.4-bootstrap/bin 3：build go1.17 go1.17をビルドする\n1 2 3 wget https://dl.google.com/go/go1.17.src.tar.gz mkdir go1.17 \u0026amp;\u0026amp; tar xzvf go1.17.src.tar.gz -C go1.17 --strip-components 1 cd ./go1.17/src 1 2 GOROOT_BOOTSTRAP=${PWD}/go1.4-bootstrap bash ./all.bash GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.4-bootstrap bash ./all.bash 1 2 3 4 5 6 7 Go version is \u0026#34;go1.17\u0026#34;, ignoring -next /home/mao/Desktop/go1.17/api/next.txt ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.17 Installed commands in /home/mao/Desktop/go1.17/bin *** You need to add /home/mao/Desktop/go1.17/bin to your PATH. 4：build go1.20 go1.20をビルドする\n1 2 3 wget https://dl.google.com/go/go1.20.src.tar.gz mkdir go1.20 \u0026amp;\u0026amp; tar xzvf go1.20.src.tar.gz -C go1.20 --strip-components 1 cd ./go1.20/src 1 2 /home/mao/Desktop/go1.17/bin GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.17 bash ./all.bash 1 2 3 4 5 ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.20 Installed commands in /home/mao/Desktop/go1.20/bin *** You need to add /home/mao/Desktop/go1.20/bin to your PATH. 5：build go1.22.2 latest go1.22.2をビルドする\n1 2 3 wget https://dl.google.com/go/go1.22.2.src.tar.gz mkdir go1.22.2 \u0026amp;\u0026amp; tar xzvf go1.22.2.src.tar.gz -C go1.22.2 --strip-components 1 cd ./go1.22.2/src 1 2 /home/mao/Desktop/go1.20/bin GOROOT_BOOTSTRAP=/home/mao/Desktop/go1.20 bash ./all.bash 1 2 3 4 5 ALL TESTS PASSED --- Installed Go for linux/amd64 in /home/mao/Desktop/go1.22.2 Installed commands in /home/mao/Desktop/go1.22.2/bin *** You need to add /home/mao/Desktop/go1.22.2/bin to your PATH. 6：Pathを通す パスを通してバージョンを確認する\n1 sudo cp -rp ./go1.22.2 /usr/local/ .bashrc\n1 2 export PATH=$PATH:/usr/local/go/bin export PATH=$PATH:/usr/local/go1.22.2/bin 1 source ~/.bashrc 1 go version 1 go version go1.22.2 linux/amd64 参考URL https://go.dev/doc/install/source https://qiita.com/myoshimi/items/5d1f6a2ee8a849bac7eb https://qiita.com/soarflat/items/d5015bec37f8a8254380 ","date":"2024-05-25T00:00:00Z","permalink":"https://tiisanamaou.github.io/post/golang-source-build/","title":"Go言語をソースコードからビルドする"}]